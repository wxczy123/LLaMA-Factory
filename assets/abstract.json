[
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Automated visual inspection of CMS HGCAL silicon sensor surface using an ensemble of a deep convolutional autoencoder and classifier. More than a thousand 8\" silicon sensors will be visually inspected to look for anomalies on their surface during the quality control preceding assembly into the High-Granularity Calorimeter for the CMS experiment at CERN. A deep learning-based algorithm that pre-selects potentially anomalous images of the sensor surface in real time has been developed to automate the visual inspection. The anomaly detection is done by an ensemble of independent deep convolutional neural networks: an autoencoder and a classifier. The performance is evaluated on images acquired in production. The pre-selection reduces the number of images requiring human inspection by 85%, with recall of 97%. Data gathered in production can be used for continuous learning to improve the accuracy incrementally.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<yes>{Performance studies}<yes>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Ps and Qs: Quantization-aware pruning for efficient low latency neural network inference. Efficient machine learning implementations optimized for inference in hardware have wide-ranging benefits, depending on the application, from lower inference latency to higher data throughput and reduced energy consumption. Two popular techniques for reducing computation in neural networks are pruning, removing insignificant synapses, and quantization, reducing the precision of the calculations. In this work, we explore the interplay between pruning and quantization during the training of neural networks for ultra low latency applications targeting high energy physics use cases. Techniques developed for this study have potential applications across many other domains. We study various configurations of pruning during quantization-aware training, which we term quantization-aware pruning, and the effect of techniques like regularization, batch normalization, and different pruning schemes on performance, computational complexity, and information content metrics. We find that quantization-aware pruning yields more computationally efficient models than either pruning or quantization alone for our task. Further, quantization-aware pruning typically performs similar to or better in terms of computational efficiency compared to other neural architecture search techniques like Bayesian optimization. Surprisingly, while networks with different training configurations can have similar performance for the benchmark application, the information content in the network can vary significantly, affecting its generalizability.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<yes>{Software}<no>{Hardware/firmware}<yes>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  }
]