[
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Toward the end-to-end optimization of particle physics instruments with differentiable programming. The full optimization of the design and operation of instruments whose functioning relies on the interaction of radiation with matter is a super-human task, given the large dimensionality of the space of possible choices for geometry, detection technology, materials, data-acquisition, and information-extraction techniques, and the interdependence of the related parameters. On the other hand, massive potential gains in performance over standard, \"experience-driven\" layouts are in principle within our reach if an objective function fully aligned with the final goals of the instrument is maximized by means of a systematic search of the configuration space. The stochastic nature of the involved quantum processes make the modeling of these systems an intractable problem from a classical statistics point of view, yet the construction of a fully differentiable pipeline and the use of deep learning techniques may allow the simultaneous optimization of all design parameters. In this document we lay down our plans for the design of a modular and versatile modeling tool for the end-to-end optimization of complex instruments for particle physics experiments as well as industrial and medical applications that share the detection of radiation as their basic ingredient. We consider a selected set of use cases to highlight the specific needs of different applications.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<yes>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<yes>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Modelling parametric uncertainty in PDEs models via Physics-Informed Neural Networks. We provide an approach enabling one to employ physics-informed neural networks (PINNs) for uncertainty quantification. Our approach is applicable to systems where observations are scarce (or even lacking), these being typical situations associated with subsurface water bodies. Our novel physics-informed neural network under uncertainty (PINN-UU) integrates the space-time domain across which processes take place and uncertain parameter spaces within a unique computational domain. PINN-UU is then trained to satisfy the relevant physical principles (e.g., mass conservation) in the defined input domain. We employ a stage training approach via transfer learning to accommodate high-dimensional solution spaces. We demonstrate the effectiveness of PINN-UU in a scenario associated with reactive transport in porous media, showcasing its reliability, efficiency, and applicability to sensitivity analysis. PINN-UU emerges as a promising tool for robust uncertainty quantification, with broad applicability to groundwater systems. As such, it can be considered as a valuable alternative to traditional methods such as multi-realization Monte Carlo simulations based on direct solvers or black-box surrogate models.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<yes>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<yes>{Interpretability}<no>{Estimation}<yes>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Gluon helicity distribution in the nucleon from lattice QCD and machine learning. We present the first lattice QCD determination of the light cone gluon helicity correlation parton distribution function (PDF) with numerical evidence toward disfavoring negative gluon polarization in the nucleon. We present a solution for eliminating an inevitable contamination term that dominates the Euclidean correlations and makes determining gluon helicity PDF unfeasible. The proposed synergy between lattice QCD and artificial intelligence offers a superior platform to alleviate the defining challenge of extracting quark and gluon PDFs from the lattice data that are available in a limited domain due to a finite range of accessible hadron momenta. We suggest a systematically improvable method to extract PDFs from the lattice data, independent of inadequate parametrizations. The result of the gluon helicity will improve our understanding of the role of spin in the strong interaction and the nucleon spin structure.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<yes>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<yes>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Learning geometries beyond asymptotic AdS. We present a data-driven method for holographic bulk reconstruction that works even when the spacetime is not asymptotically AdS. Given the data of boundary Green functions within a finite frequency window, we iteratively adjust a bulk metric with a finite radial cutoff until its holographic Green functions reproduce the boundary data. Based on the holographic Wilsonian renormalization group for the Klein-Gordon equation in an undetermined curve space, we construct a radial flow equation and transform it into a Neural ODE, which is an infinite-depth neural network for modeling continuous dynamics. Assuming the double-trace coupling $h$ in the Wilsonian action is real, we demonstrate that the Neural ODE can effectively learn the metrics with AdS, Lifshitz, and hyperscaling violated asymptotics. In particular, we apply the algorithm to the Sachdev-Ye-Kitaev (SYK) model which slightly deviates from the conformal limit. In the hyperparameter space spanned by the rescaled temperature $\\bar{T}$ and the radial cutoff $\\epsilon$, we identify a critical curve along which the learned metric is close to AdS$_2$ black hole with finite cutoff. We derive an approximate analytical expression for this curve, from which an effective bulk dual of the SYK coupling $v$ is established. Our work provides a promising way for using machine learning to depict the novel bulk geometry dual to the non-conformal boundary systems in the real world.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<yes>{Theory and physics for ML}<no>{ML for theory}<yes>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "A new graph-neural-network flavor tagger for Belle II and measurement of $\\sin 2\\phi_1$ in $B^0 \\to J/\\psi K^0_\\text{S}$ decays. We present GFlaT, a new algorithm that uses a graph-neural-network to determine the flavor of neutral $B$ mesons produced in $\\Upsilon(4S)$ decays. It improves previous algorithms by using the information from all charged final-state particles and the relations between them. We evaluate its performance using $B$ decays to flavor-specific hadronic final states reconstructed in a 362 $\\text{fb}^{-1}$ sample of electron-positron collisions collected at the $\\Upsilon(4S)$ resonance with the Belle II detector at the SuperKEKB collider. We achieve an effective tagging efficiency of $(37.40 \\pm 0.43 \\pm 0.36) \\%$, where the first uncertainty is statistical and the second systematic, which is $18\\%$ better than the previous Belle II algorithm. Demonstrating the algorithm, we use $B^{0}\\to J/\\psi K^0_\\text{S}$ decays to measure the mixing-induced and direct $CP$ violation parameters, $S = (0.724 \\pm 0.035 \\pm 0.009)$ and $C = (-0.035 \\pm 0.026 \\pm 0.029)$.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<yes>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<yes>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Exact CHY Integrand Construction Using Combinatorial Neural Networks and Discrete Optimization. We present a novel approach to the inverse CHY problem-constructing integrands from desired pole structures-by establishing a correspondence between CHY combinatorics and Combinatorial Neural Networks (CNNs). Our key innovation is the generalized pole degree $K(s_A)$, which is additive under integrand multiplication and satisfies hierarchical recursion relations. These recursions naturally map to CNN message-passing architectures, enabling discrete automatic differentiation that preserves exact integer constraints throughout the computation. Unlike conventional machine learning approaches that yield numerical approximations, our method leverages the intrinsic combinatorial structure of the CHY formalism to guarantee exact algebraic solutions. This work demonstrates how specialized neural architectures can solve fundamental physics problems requiring strict mathematical constraints, providing a modest step toward bridging combinatorics, machine learning, and scattering amplitude theory.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<yes>{Theory and physics for ML}<no>{ML for theory}<yes>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Exploration at the high-energy frontier: ATLAS Run 2 searches investigating the exotic jungle beyond the Standard Model. This report presents a comprehensive collection of searches for new physics performed by the ATLAS Collaboration during the Run 2 period of data taking at the Large Hadron Collider, from 2015 to 2018, corresponding to about 140 fb$^{-1}$ of $\\sqrt{s}=13$ TeV proton-proton collision data. These searches cover a variety of beyond-the-standard model topics such as dark matter candidates, new vector bosons, hidden-sector particles, leptoquarks, or vector-like quarks, among others. Searches for supersymmetric particles or extended Higgs sectors are explicitly excluded as these are the subject of separate reports by the Collaboration. For each topic, the most relevant searches are described, focusing on their importance and sensitivity and, when appropriate, highlighting the experimental techniques employed. In addition to the description of each analysis, complementary searches are compared, and the overall sensitivity of the ATLAS experiment to each type of new physics is discussed. Summary plots and statistical combinations of multiple searches are included whenever possible.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<yes>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<yes>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Track Reconstruction using Geometric Deep Learning in the Straw Tube Tracker (STT) at the PANDA Experiment. The PANDA (anti-Proton ANnihilation at DArmstadt) experiment at the Facility for Anti-proton and Ion Research is going to study strong interactions at the scale at which quarks are confined to form hadrons. A continuous beam of antiproton, provided by the High Energy Storage Ring (HESR), will impinge on a fixed hydrogen target. The antiproton beam momentum spans from 1.5 GeV {Natural units, c=1} to 15 GeV \\cite{physics2009report}, will create optimal conditions for studying many different aspects of hadron physics, including hyperon physics. Precision physics studies require a highly efficient particle track reconstruction. The Straw Tube Tracker in PANDA is the main component for that purpose. It has a hexagonal geometry, consisting of 4224 gas-filled tubes arranged in 26 layers and six sectors. However, the challenge is reconstructing low momentum charged particles given the complex detector geometry and the strongly curved particle trajectory. This paper presents the first application of a geometric deep learning pipeline to track reconstruction in the PANDA experiment. The pipeline reconstructs more than 95\\% of particle tracks and creates less than 0.3\\% fake tracks. The promising results make the pipeline a strong candidate algorithm for the experiment.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<yes>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Towards an integrated determination of proton, deuteron and nuclear PDFs. We present progress towards a unified framework enabling the simultaneous determination of the parton distribution functions (PDFs) of the proton, deuteron, and nuclei up to lead $(^{208}\\rm{Pb})$. Our approach is based on the integration of the fitting framework underlying the nNNPDF3.0 determination of nuclear PDFs into that adopted for the NNPDF4.0 global analysis of proton PDFs. Our work paves the way toward a full integrated global analysis of non-perturbative QCD -- a key ingredient for the exploitation of the scientific potential of present and future nuclear and particle physics facilities such as the Electron-Ion Collider (EIC).",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<yes>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<yes>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Deep Neural Networks for Cross-Energy Particle Identification at RHIC and LHC. This study demonstrates a proof-of-concept application of a deep neural network for particle identification in simulated high transverse momentum proton-proton collisions, with a focus on evaluating model performance under controlled conditions. A model trained on simulated Large Hadron Collider (LHC) proton-proton collisions at $\\sqrt{s} = 13\\,\\mathrm{TeV}$ is used to classify nine particle species based on seven kinematic-level features. The model is then tested on simulated high transverse momentum Relativistic Heavy Ion Collider (RHIC) data at $\\sqrt{s} = 200\\,\\mathrm{GeV}$ without any transfer learning, fine-tuning, or weight adjustment. It maintains accuracy above 91% for both LHC and RHIC sets, while achieving above 96% accuracy for all RHIC sets, including the $p_T > 7\\,\\mathrm{GeV}/c$ set, despite never being trained on any RHIC data. Analysis of per-class accuracy reveals how quantum chromodynamics (QCD) effects, such as leading particle effect and kinematic overlap at high $p_T$, shape the model's performance across particle types. These results suggest that the model captures physically meaningful features of high-energy collisions, rather than simply overfitting to kinematics of the training data. This study demonstrates the potential of simulation-trained deep neural networks to remain effective across lower energy regimes within a controlled environment, and motivates further investigation in realistic settings using detector-level features and more advanced network architectures.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<yes>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<yes>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<yes>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Deep Generative Models for Ultra-High Granularity Particle Physics Detector Simulation: A Voyage From Emulation to Extrapolation. Simulating ultra-high-granularity detector responses in Particle Physics represents a critical yet computationally demanding task. This thesis aims to overcome this challenge for the Pixel Vertex Detector (PXD) at the Belle II experiment, which features over 7.5M pixel channels-the highest spatial resolution detector simulation dataset ever analysed with generative models. This thesis starts off by a comprehensive and taxonomic review on generative models for simulating detector signatures. Then, it presents the Intra-Event Aware Generative Adversarial Network (IEA-GAN), a new geometry-aware generative model that introduces a relational attentive reasoning and Self-Supervised Learning to approximate an \"event\" in the detector. This study underscores the importance of intra-event correlation for downstream physics analyses. Building upon this, the work drifts towards a more generic approach and presents YonedaVAE, a Category Theory-inspired generative model that tackles the open problem of Out-of-Distribution (OOD) simulation. YonedaVAE introduces a learnable Yoneda embedding to capture the entirety of an event based on its sensor relationships, formulating a Category theoretical language for intra-event relational reasoning. This is complemented by introducing a Self-Supervised learnable prior for VAEs and an Adaptive Top-q sampling mechanism, enabling the model to sample point clouds with variable intra-category cardinality in a zero-shot manner. Variable Intra-event cardinality has not been approached before and is vital for simulating irregular detector geometries. Trained on an early experiment data, YonedaVAE can reach a reasonable OOD simulation precision of a later experiment with almost double luminosity. This study introduces, for the first time, the results of using deep generative models for ultra-high granularity detector simulation in Particle Physics.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<no>{(Variational) Autoencoders}<yes>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "The R-mAtrIx Net. We provide a novel Neural Network architecture that can: i) output R-matrix for a given quantum integrable spin chain, ii) search for an integrable Hamiltonian and the corresponding R-matrix under assumptions of certain symmetries or other restrictions, iii) explore the space of Hamiltonians around already learned models and reconstruct the family of integrable spin chains which they belong to. The neural network training is done by minimizing loss functions encoding Yang-Baxter equation, regularity and other model-specific restrictions such as hermiticity. Holomorphy is implemented via the choice of activation functions. We demonstrate the work of our Neural Network on the two-dimensional spin chains of difference form. In particular, we reconstruct the R-matrices for all 14 classes. We also demonstrate its utility as an \\textit{Explorer}, scanning a certain subspace of Hamiltonians and identifying integrable classes after clusterisation. The last strategy can be used in future to carve out the map of integrable spin chains in higher dimensions and in more general settings where no analytical methods are available.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<yes>{Theory and physics for ML}<no>{ML for theory}<yes>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Machine Learning in High Energy Physics Community White Paper. Machine learning has been applied to several problems in particle physics research, beginning with applications to high-level physics analysis in the 1990s and 2000s, followed by an explosion of applications in particle and event identification and reconstruction in the 2010s. In this document we discuss promising future research and development areas for machine learning in particle physics. We detail a roadmap for their implementation, software and hardware resource requirements, collaborative initiatives with the data science community, academia and industry, and training the particle physics community in data science. The main objective of the document is to connect and motivate these areas of research and development with the physics drivers of the High-Luminosity Large Hadron Collider and future neutrino experiments and identify the resource needs for their implementation. Additionally we identify areas where collaboration with external communities will be of great benefit.",
    "output": "{Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "NNPDFpol2.0: unbiased global determination of polarized PDFs and their uncertainties at next-to-next-to-leading order. We present NNPDFpol2.0, a new set of collinear helicity parton distribution functions (PDFs) of the proton based on legacy measurements of structure functions in inclusive neutral-current longitudinally polarised deep-inelastic scattering (DIS), and of W -boson, single-inclusive, and di-jet production asymmetries in longitudinally polarised proton-proton collisions. The determination is accurate to next-to-next-to-leading order in the strong coupling, and includes heavy quark mass corrections in the analysis of DIS data. Uncertainties due to missing higher-order corrections are systematically incorporated by means of a covariance matrix determined by scale variations. NNPDFpol2.0 is based on a machine learning methodology, that makes use of Monte Carlo sampling for the representation of uncertainties into PDFs, of a neural network for the parametrisation of PDFs, of stochastic gradient descent for the optimisation of PDF parameters, and of hyperoptimisation for the selection of the best fitting model. We study the impact on PDFs of higher-order corrections, of the positivity constraint, and of the data. We demonstrate two phenomenological applications of NNPDFpol2.0, specifically the determination of the proton spin fraction carried by gluons and quarks, and of theoretical predictions for single-hadron production in longitudinally polarised DIS and proton-proton collisions.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<yes>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<yes>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Phenomenology at the Large Hadron Collider with Deep Learning: the case of vector-like quarks decaying to light jets. In this work, we continue our exploration of TeV-scale vector-like fermion signatures inspired by a Grand Unification scenario based on the trinification gauge group. A particular focus is given to pair-production topologies of vector-like quarks (VLQs) at the LHC, in a multi-jet plus a charged lepton and a missing energy signature. We employ Deep Learning methods and techniques based in evolutive algorithms that optimize hyper-parameters in the neural network construction, whose objective is to maximise the Asimov estimate for distinct VLQ masses. In this article, we consider the implications of an innovative approach by simultaneously combining detector images (also known as jet images) and tabular data containing kinematic information from the final states. With this technique we are able to exclude VLQs, that are specific for the considered model, to up a mass of 800 GeV in both the high-luminosity the Run-III phases of the LHC programme.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<yes>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Transformers for Charged Particle Track Reconstruction in High Energy Physics. Reconstructing charged particle tracks is a fundamental task in modern collider experiments. The unprecedented particle multiplicities expected at the High-Luminosity Large Hadron Collider (HL-LHC) pose significant challenges for track reconstruction, where traditional algorithms become computationally infeasible. To address this challenge, we present a novel learned approach to track reconstruction that adapts recent advances in computer vision and object detection. Our architecture combines a Transformer hit filtering network with a MaskFormer reconstruction model that jointly optimises hit assignments and the estimation of the charged particles' properties. Evaluated on the TrackML dataset, our best performing model achieves state-of-the-art tracking performance with 97% efficiency for a fake rate of 0.6%, and inference times of 100ms. Our tunable approach enables specialisation for specific applications like triggering systems, while its underlying principles can be extended to other reconstruction challenges in high energy physics. This work demonstrates the potential of modern deep learning architectures to address emerging computational challenges in particle physics while maintaining the precision required for groundbreaking physics analysis.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<yes>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Accelerating Machine Learning Inference with GPUs in ProtoDUNE Data Processing. We study the performance of a cloud-based GPU-accelerated inference server to speed up event reconstruction in neutrino data batch jobs. Using detector data from the ProtoDUNE experiment and employing the standard DUNE grid job submission tools, we attempt to reprocess the data by running several thousand concurrent grid jobs, a rate we expect to be typical of current and future neutrino physics experiments. We process most of the dataset with the GPU version of our processing algorithm and the remainder with the CPU version for timing comparisons. We find that a 100-GPU cloud-based server is able to easily meet the processing demand, and that using the GPU version of the event processing algorithm is two times faster than processing these data with the CPU version when comparing to the newest CPUs in our sample. The amount of data transferred to the inference server during the GPU runs can overwhelm even the highest-bandwidth network switches, however, unless care is taken to observe network facility limits or otherwise distribute the jobs to multiple sites. We discuss the lessons learned from this processing campaign and several avenues for future improvements.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<yes>{Software}<no>{Hardware/firmware}<yes>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "NCoder -- A Quantum Field Theory approach to encoding data. In this paper we present a novel approach to interpretable AI inspired by Quantum Field Theory (QFT) which we call the NCoder. The NCoder is a modified autoencoder neural network whose latent layer is prescribed to be a subset of $n$-point correlation functions. Regarding images as draws from a lattice field theory, this architecture mimics the task of perturbatively constructing the effective action of the theory order by order in an expansion using Feynman diagrams. Alternatively, the NCoder may be regarded as simulating the procedure of statistical inference whereby high dimensional data is first summarized in terms of several lower dimensional summary statistics (here the $n$-point correlation functions), and subsequent out-of-sample data is generated by inferring the data generating distribution from these statistics. In this way the NCoder suggests a fascinating correspondence between perturbative renormalizability and the sufficiency of models. We demonstrate the efficacy of the NCoder by applying it to the generation of MNIST images, and find that generated images can be correctly classified using only information from the first three $n$-point functions of the image distribution.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<yes>{Theory and physics for ML}<no>{ML for theory}<yes>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "How Much Information is in a Jet?. Machine learning techniques are increasingly being applied toward data analyses at the Large Hadron Collider, especially with applications for discrimination of jets with different originating particles. Previous studies of the power of machine learning to jet physics has typically employed image recognition, natural language processing, or other algorithms that have been extensively developed in computer science. While these studies have demonstrated impressive discrimination power, often exceeding that of widely-used observables, they have been formulated in a non-constructive manner and it is not clear what additional information the machines are learning. In this paper, we study machine learning for jet physics constructively, expressing all of the information in a jet onto sets of observables that completely and minimally span N-body phase space. For concreteness, we study the application of machine learning for discrimination of boosted, hadronic decays of Z bosons from jets initiated by QCD processes. Our results demonstrate that the information in a jet that is useful for discrimination power of QCD jets from Z bosons is saturated by only considering observables that are sensitive to 4-body (8 dimensional) phase space.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<yes>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<yes>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Choose Your Diffusion: Efficient and flexible ways to accelerate the diffusion model in fast high energy physics simulation. The diffusion model has demonstrated promising results in image generation, recently becoming mainstream and representing a notable advancement for many generative modeling tasks. Prior applications of the diffusion model for both fast event and detector simulation in high energy physics have shown exceptional performance, providing a viable solution to generate sufficient statistics within a constrained computational budget in preparation for the High Luminosity LHC. However, many of these applications suffer from slow generation with large sampling steps and face challenges in finding the optimal balance between sample quality and speed. The study focuses on the latest benchmark developments in efficient ODE/SDE-based samplers, schedulers, and fast convergence training techniques. We test on the public CaloChallenge and JetNet datasets with the designs implemented on the existing architecture, the performance of the generated classes surpass previous models, achieving significant speedup via various evaluation metrics.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<yes>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Machine Learning on data with sPlot background subtraction. Data analysis in high energy physics often deals with data samples consisting of a mixture of signal and background events. The sPlot technique is a common method to subtract the contribution of the background by assigning weights to events. Part of the weights are by design negative. Negative weights lead to the divergence of some machine learning algorithms training due to absence of the lower bound in the loss function. In this paper we propose a mathematically rigorous way to train machine learning algorithms on data samples with background described by sPlot to obtain signal probabilities conditioned on observables, without encountering negative event weight at all. This allows usage of any out-of-the-box machine learning methods on such data.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<yes>{Hyperparameters}<no>{Weak/Semi supervision}<yes>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Feynman Diagrams as Computational Graphs. Quantum field theory (QFT) for interacting many-electron systems is fundamental to condensed matter physics, yet achieving accurate solutions confronts computational challenges in managing the combinatorial complexity of Feynman diagrams, implementing systematic renormalization, and evaluating high-dimensional integrals. We present a unifying framework that integrates QFT computational workflows with an AI-powered technology stack. A cornerstone of this framework is representing Feynman diagrams as computational graphs, which structures the inherent mathematical complexity and facilitates the application of optimized algorithms developed for machine learning and high-performance computing. Consequently, automatic differentiation, native to these graph representations, delivers efficient, fully automated, high-order field-theoretic renormalization procedures. This graph-centric approach also enables sophisticated numerical integration; our neural-network-enhanced Monte Carlo method, accelerated via massively parallel GPU implementation, efficiently evaluates challenging high-dimensional diagrammatic integrals. Applying this framework to the uniform electron gas, we determine the quasiparticle effective mass to a precision significantly surpassing current state-of-the-art simulations. Our work demonstrates the transformative potential of integrating AI-driven computational advances with QFT, opening systematic pathways for solving complex quantum many-body problems across disciplines.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<yes>{Theory and physics for ML}<no>{ML for theory}<yes>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Implementation of machine learning techniques to predict impact parameter and transverse spherocity in heavy-ion collisions at the LHC. Machine learning techniques have been quite popular recently in the high-energy physics community and have led to numerous developments in this field. In heavy-ion collisions, one of the crucial observables, the impact parameter, plays an important role in the final-state particle production. This being extremely small (i.e. of the order of a few fermi), it is almost impossible to measure impact parameter in experiments. In this work, we implement the ML-based regression technique via Gradient Boosting Decision Trees (GBDT) to obtain a prediction of impact parameter in Pb-Pb collisions at $\\sqrt{s_{NN}}$ = 5.02 TeV using A Multi-Phase Transport (AMPT) model. After its successful implementation in small collision systems, transverse spherocity, an event shape observable, holds an opportunity to reveal more about the particle production in heavy-ion collisions as well. In the absence of any experimental exploration in this direction at the LHC yet, we suggest an ML-based regression method to estimate centrality-wise transverse spherocity distributions in Pb-Pb collisions at $\\sqrt{s_{NN}}$ = 5.02 TeV by training the model with minimum bias collision data. Throughout this work, we have used a few final state observables as the input to the ML-model, which could be easily made available from collision data. Our method seems to work quite well as we see a good agreement between the simulated true values and the predicted values from the ML-model.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<yes>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Neural Networks for Full Phase-space Reweighting and Parameter Tuning. Precise scientific analysis in collider-based particle physics is possible because of complex simulations that connect fundamental theories to observable quantities. The significant computational cost of these programs limits the scope, precision, and accuracy of Standard Model measurements and searches for new phenomena. We therefore introduce Deep neural networks using Classification for Tuning and Reweighting (DCTR), a neural network-based approach to reweight and fit simulations using all kinematic and flavor information -- the full phase space. DCTR can perform tasks that are currently not possible with existing methods, such as estimating non-perturbative fragmentation uncertainties. The core idea behind the new approach is to exploit powerful high-dimensional classifiers to reweight phase space as well as to identify the best parameters for describing data. Numerical examples from $e^+e^-\\rightarrow\\text{jets}$ demonstrate the fidelity of these methods for simulation parameters that have a big and broad impact on phase space as well as those that have a minimal and/or localized impact. The high fidelity of the full phase-space reweighting enables a new paradigm for simulations, parameter tuning, and model systematic uncertainties across particle physics and possibly beyond.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<yes>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<yes>{Unfolding}<no>{Domain adaptation}<yes>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Tagging fully hadronic exotic decays of the vectorlike $\\mathbf{B}$ quark using a graph neural network. Following up on our earlier study in [J. Bardhan et al., Machine learning-enhanced search for a vectorlike singlet B quark decaying to a singlet scalar or pseudoscalar, Phys. Rev. D 107 (2023) 115001; arXiv:2212.02442], we investigate the LHC prospects of pair-produced vectorlike $B$ quarks decaying exotically to a new gauge-singlet (pseudo)scalar field $\\Phi$ and a $b$ quark. After the electroweak symmetry breaking, the $\\Phi$ decays predominantly to $gg/bb$ final states, leading to a fully hadronic $2b+4j$ or $6b$ signature. Because of the large Standard Model background and the lack of leptonic handles, it is a difficult channel to probe. To overcome the challenge, we employ a hybrid deep learning model containing a graph neural network followed by a deep neural network. We estimate that such a state-of-the-art deep learning analysis pipeline can lead to a performance comparable to that in the semi-leptonic mode, taking the discovery (exclusion) reach up to about $M_B=1.8\\:(2.4)$ TeV at HL-LHC when $B$ decays fully exotically, i.e., BR$(B \\to b\\Phi) = 100\\%$.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<yes>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Helicity-dependent parton distribution functions at next-to-next-to-leading order accuracy from inclusive and semi-inclusive deep-inelastic scattering data. We present MAPPDFpol1.0, a new determination of the helicity-dependent parton distribution functions (PDFs) of the proton from a set of longitudinally polarised inclusive and semi-inclusive deep-inelastic scattering data. The determination includes, for the first time, next-to-next-to-leading order QCD corrections to both processes, and is carried out in a framework that combines a neural-network parametrisation of PDFs with a Monte Carlo representation of their uncertainties. We discuss the quality of the determination, in particular its dependence on higher-order corrections, on the choice of data set, and on theoretical constraints.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<yes>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<yes>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Deep Learning for the Matrix Element Method. Extracting scientific results from high-energy collider data involves the comparison of data collected from the experiments with synthetic data produced from computationally-intensive simulations. Comparisons of experimental data and predictions from simulations increasingly utilize machine learning (ML) methods to try to overcome these computational challenges and enhance the data analysis. There is increasing awareness about challenges surrounding interpretability of ML models applied to data to explain these models and validate scientific conclusions based upon them. The matrix element (ME) method is a powerful technique for analysis of particle collider data that utilizes an \\textit{ab initio} calculation of the approximate probability density function for a collision event to be due to a physics process of interest. The ME method has several unique and desirable features, including (1) not requiring training data since it is an \\textit{ab initio} calculation of event probabilities, (2) incorporating all available kinematic information of a hypothesized process, including correlations, without the need for feature engineering and (3) a clear physical interpretation in terms of transition probabilities within the framework of quantum field theory. These proceedings briefly describe an application of deep learning that dramatically speeds-up ME method calculations and novel cyberinfrastructure developed to execute ME-based analyses on heterogeneous computing platforms.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<yes>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<yes>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Solving the nuclear pairing model with neural network quantum states. We present a variational Monte Carlo method that solves the nuclear many-body problem in the occupation number formalism exploiting an artificial neural network representation of the ground-state wave function. A memory-efficient version of the stochastic reconfiguration algorithm is developed to train the network by minimizing the expectation value of the Hamiltonian. We benchmark this approach against widely used nuclear many-body methods by solving a model used to describe pairing in nuclei for different types of interaction and different values of the interaction strength. Despite its polynomial computational cost, our method outperforms coupled-cluster and provides energies that are in excellent agreement with the numerically-exact full configuration interaction values.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<yes>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Inductive CaloFlow. Simulating particle detector response is the single most expensive step in the Large Hadron Collider computational pipeline. Recently it was shown that normalizing flows can accelerate this process while achieving unprecedented levels of accuracy, but scaling this approach up to higher resolutions relevant for future detector upgrades leads to prohibitive memory constraints. To overcome this problem, we introduce Inductive CaloFlow (iCaloFlow), a framework for fast detector simulation based on an inductive series of normalizing flows trained on the pattern of energy depositions in pairs of consecutive calorimeter layers. We further use a teacher-student distillation to increase sampling speed without loss of expressivity. As we demonstrate with Datasets 2 and 3 of the CaloChallenge2022, iCaloFlow can realize the potential of normalizing flows in performing fast, high-fidelity simulation on detector geometries that are ~ 10 - 100 times higher granularity than previously considered.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<yes>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "An updated hybrid deep learning algorithm for identifying and locating primary vertices. We present an improved hybrid algorithm for vertexing, that combines deep learning with conventional methods. Even though the algorithm is a generic approach to vertex finding, we focus here on it's application as an alternative Primary Vertex (PV) finding tool for the LHCb experiment. In the transition to Run 3 in 2021, LHCb will undergo a major luminosity upgrade, going from 1.1 to 5.6 expected visible PVs per event, and it will adopt a purely software trigger. We use a custom kernel to transform the sparse 3D space of hits and tracks into a dense 1D dataset, and then apply Deep Learning techniques to find PV locations using proxy distributions to encode the truth in training data. Last year we reported that training networks on our kernels using several Convolutional Neural Network layers yielded better than 90 % efficiency with no more than 0.2 False Positives (FPs) per event. Modifying several elements of the algorithm, we now achieve better than 94 % efficiency with a significantly lower FP rate. Where our studies to date have been made using toy Monte Carlo (MC), we began to study KDEs produced from complete LHCb Run 3 MC data, including full tracking in the vertex locator rather than proto-tracking.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<yes>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Interpretable Uncertainty Quantification in AI for HEP. Estimating uncertainty is at the core of performing scientific measurements in HEP: a measurement is not useful without an estimate of its uncertainty. The goal of uncertainty quantification (UQ) is inextricably linked to the question, \"how do we physically and statistically interpret these uncertainties?\" The answer to this question depends not only on the computational task we aim to undertake, but also on the methods we use for that task. For artificial intelligence (AI) applications in HEP, there are several areas where interpretable methods for UQ are essential, including inference, simulation, and control/decision-making. There exist some methods for each of these areas, but they have not yet been demonstrated to be as trustworthy as more traditional approaches currently employed in physics (e.g., non-AI frequentist and Bayesian methods). Shedding light on the questions above requires additional understanding of the interplay of AI systems and uncertainty quantification. We briefly discuss the existing methods in each area and relate them to tasks across HEP. We then discuss recommendations for avenues to pursue to develop the necessary techniques for reliable widespread usage of AI with UQ over the next decade.",
    "output": "{Reviews}<yes>{Modern reviews}<no>{Specialized reviews}<yes>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Machine Learning for Prediction of Unitarity and Bounded from Below Constraints. The machine learning (ML) techniques to predict unitarity (UNI) and bounded from below (BFB) constraints in multi-scalar models is employed. The effectiveness of this approach is demonstrated by applying it to the two and three Higgs doublet models, as well as the left-right model. By employing suitable neural network architectures, learning algorithms, and carefully curated training datasets, a significantly high level of predictivity is achieved. Machine learning offers a distinct advantage by enabling faster calculations compared to alternative numerical methods, such as scalar potential minimization. This research investigates the feasibility of utilizing machine learning techniques as an alternative for predicting these constraints, offering potential improvements over traditional numerical calculations.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<yes>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Unsupervised clustering for collider physics. We propose a new method for Unsupervised clustering in particle physics named UCluster, where information in the embedding space created by a neural network is used to categorise collision events into different clusters that share similar properties. We show how this method can be applied to an unsupervised multiclass classification as well as for anomaly detection, which can be used for new physics searches.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<yes>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Flavour tagging with graph neural networks with the ATLAS detector. The identification of jets containing a $b$-hadron, referred to as $b$-tagging, plays an important role for various physics measurements and searches carried out by the ATLAS experiment at the CERN Large Hadron Collider (LHC). The most recent $b$-tagging algorithm developments based on graph neural network architectures are presented. Preliminary performance on Run 3 data in $pp$ collisions at $\\sqrt s = 13.6$ TeV is shown and expected performance at the High-Luminosity LHC (HL-LHC) discussed.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<yes>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<yes>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Extraction of Information from Polarized Deep Exclusive Scattering with Machine Learning. A framework defining benchmarks for the analysis of polarized exclusive scattering cross sections is proposed that uses physics symmetry constraints as well as lattice QCD predictions. These constraints are built into machine learning (ML) algorithms. Both physics driven and ML based benchmarks are applied to a wide range of deeply virtual exclusive processes through explainable ML techniques with controllable uncertainties. The observables, namely the Compton Form Factors (CFFs) which are convolutions of Generalized Parton Distributions (GPDs), are extracted using methods such as the random targets method to evaluate the separate contribution of the aleatoric and epistemic uncertainties in exclusive scattering analyses.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<yes>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<yes>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Learning Particle Physics by Example: Location-Aware Generative Adversarial Networks for Physics Synthesis. We provide a bridge between generative modeling in the Machine Learning community and simulated physical processes in High Energy Particle Physics by applying a novel Generative Adversarial Network (GAN) architecture to the production of jet images -- 2D representations of energy depositions from particles interacting with a calorimeter. We propose a simple architecture, the Location-Aware Generative Adversarial Network, that learns to produce realistic radiation patterns from simulated high energy particle collisions. The pixel intensities of GAN-generated images faithfully span over many orders of magnitude and exhibit the desired low-dimensional physical properties (i.e., jet mass, n-subjettiness, etc.). We shed light on limitations, and provide a novel empirical validation of image quality and validity of GAN-produced simulations of the natural world. This work provides a base for further explorations of GANs for use in faster simulation in High Energy Particle Physics.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<yes>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Mining for Dark Matter Substructure: Inferring subhalo population properties from strong lenses with machine learning. The subtle and unique imprint of dark matter substructure on extended arcs in strong lensing systems contains a wealth of information about the properties and distribution of dark matter on small scales and, consequently, about the underlying particle physics. However, teasing out this effect poses a significant challenge since the likelihood function for realistic simulations of population-level parameters is intractable. We apply recently-developed simulation-based inference techniques to the problem of substructure inference in galaxy-galaxy strong lenses. By leveraging additional information extracted from the simulator, neural networks are efficiently trained to estimate likelihood ratios associated with population-level parameters characterizing substructure. Through proof-of-principle application to simulated data, we show that these methods can provide an efficient and principled way to simultaneously analyze an ensemble of strong lenses, and can be used to mine the large sample of lensing images deliverable by near-future surveys for signatures of dark matter substructure.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<yes>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Taming modeling uncertainties with Mass Unspecific Supervised Tagging. We address the modeling dependence of jet taggers built using the method of Mass Unspecific Supervised Tagging, by using two different parton showering and hadronisation schemes. We find that the modeling dependence of the results - estimated by using different schemes in the design of the taggers and applying them to the same type of data - is rather small, even if the jet substructure varies significantly between the two schemes. These results add great value to the use of generic supervised taggers for new physics searches.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<yes>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Cluster Reconstruction in Electromagnetic Calorimeters Using Machine Learning Methods. Machine-learning-based methods can be developed for the reconstruction of clusters in segmented detectors for high energy physics experiments. Convolutional neural networks with autoencoder architecture trained on labeled data from a simulated dataset reconstruct events by providing information about the hit point and energy of each particle that has entered the detector. The correct reconstruction of the positionand the energy of the incident particles is crucial for the accurate events reconstruction. The presented method shows the ability to reconstruct the impact point within the same segment as the true position and determines the particle energy with good precision. It can be applied in a wide range of cases of event reconstruction where the good separation of overlapping signals plays a key role in the data analysis.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<yes>{Pileup}<yes>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<yes>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Searching for exotic Higgs bosons from top quark decays at the HL-LHC. Exotic spin-$0$ states with unusual couplings with the gauge and matter fields of the Standard Model are worth exploring at the CERN LHC. Though our approach is largely model independent, we take inspiration from flavor models based on some discrete symmetries which predict a set of a scalar and a pseudoscalar having purely off-diagonal Yukawa interactions with quarks and leptons. In a previous paper, some of us explored how to decipher such exotic scalar and pseudoscalar states whose off-diagonal Yukawa couplings involve light quarks. In this work we follow a complementary path and focus on the Yukawa couplings that necessarily involve a top quark. If one such spin-$0$ state is lighter than the top quark, then the rare decay of the latter, on account of the high yield of the $t\\bar t$ events, could provide a potential hunting ground of those exotic states particularly during the high luminosity phase of the LHC run. We carry out an exhaustive collider analysis of some promising signatures of those exotic states using sophisticated Machine Learning techniques and obtain considerable signal significance.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<yes>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Emergent field theories from neural networks. We establish a duality relation between Hamiltonian systems and neural network-based learning systems. We show that the Hamilton-Jacobi equations for position and momentum variables correspond to the equations governing the activation dynamics of non-trainable variables and the learning dynamics of trainable variables. The duality is then applied to model various field theories using the activation and learning dynamics of neural networks. For Klein-Gordon fields, the corresponding weight tensor is symmetric, while for Dirac fields, the weight tensor must contain an anti-symmetric tensor factor. The dynamical components of the weight and bias tensors correspond, respectively, to the temporal and spatial components of the gauge field.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<yes>{Theory and physics for ML}<no>{ML for theory}<yes>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Ariadne: PyTorch Library for Particle Track Reconstruction Using Deep Learning. Particle tracking is a fundamental part of the event analysis in high energy and nuclear physics. Events multiplicity increases each year along with the drastic growth of the experimental data which modern HENP detectors produce, so the classical tracking algorithms such as the well-known Kalman filter cannot satisfy speed and scaling requirements. At the same time, breakthroughs in the study of deep learning open an opportunity for the application of high-performance deep neural networks for solving tracking problems in a dense environment of experiments with heavy ions. However, there are no well-documented software libraries for deep learning track reconstruction yet. We introduce Ariadne, the first open-source library for particle tracking based on the PyTorch deep learning framework. The goal of our library is to provide a simple interface that allows one to prepare train and test datasets and to train and evaluate one of the deep tracking models implemented in the library on the data from your specific experiment. The user experience is greatly facilitated because of the system of gin-configurations. The modular structure of the library and abstract classes let the user develop his data processing pipeline and deep tracking model easily. The proposed library is open-source to facilitate academic research in the field of particle tracking based on deep learning.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<yes>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<yes>{Software}<yes>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Mass Unspecific Supervised Tagging (MUST) for boosted jets. Jet identification tools are crucial for new physics searches at the LHC and at future colliders. We introduce the concept of Mass Unspecific Supervised Tagging (MUST) which relies on considering both jet mass and transverse momentum varying over wide ranges as input variables - together with jet substructure observables - of a multivariate tool. This approach not only provides a single efficient tagger for arbitrary ranges of jet mass and transverse momentum, but also an optimal solution for the mass correlation problem inherent to current taggers. By training neural networks, we build MUST-inspired generic and multi-pronged jet taggers which, when tested with various new physics signals, clearly outperform the variables commonly used by experiments to discriminate signal from background. These taggers are also efficient to spot signals for which they have not been trained. Taggers can also be built to determine, with a high degree of confidence, the prongness of a jet, which would be of utmost importance in case a new physics signal is discovered.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<yes>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Development of the Topological Trigger for LHCb Run 3. The data-taking conditions expected in Run 3 of the LHCb experiment at CERN are unprecedented and challenging for the software and computing systems. Despite that, the LHCb collaboration pioneers the use of a software-only trigger system to cope with the increased event rate efficiently. The beauty physics programme of LHCb is heavily reliant on topological triggers. These are devoted to selecting beauty-hadron candidates inclusively, based on the characteristic decay topology and kinematic properties expected from beauty decays. The following proceeding describes the current progress of the Run 3 implementation of the topological triggers using Lipschitz monotonic neural networks. This architecture offers robustness under varying detector conditions and sensitivity to long-lived candidates, improving the possibility of discovering New Physics at LHCb.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<yes>{Software}<no>{Hardware/firmware}<yes>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Data Science and Machine Learning in Education. The growing role of data science (DS) and machine learning (ML) in high-energy physics (HEP) is well established and pertinent given the complex detectors, large data, sets and sophisticated analyses at the heart of HEP research. Moreover, exploiting symmetries inherent in physics data have inspired physics-informed ML as a vibrant sub-field of computer science research. HEP researchers benefit greatly from materials widely available materials for use in education, training and workforce development. They are also contributing to these materials and providing software to DS/ML-related fields. Increasingly, physics departments are offering courses at the intersection of DS, ML and physics, often using curricula developed by HEP researchers and involving open software and data used in HEP. In this white paper, we explore synergies between HEP research and DS/ML education, discuss opportunities and challenges at this intersection, and propose community activities that will be mutually beneficial.",
    "output": "{Reviews}<yes>{Modern reviews}<no>{Specialized reviews}<yes>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Quantum Convolutional Neural Networks for High Energy Physics Data Analysis. This work presents a quantum convolutional neural network (QCNN) for the classification of high energy physics events. The proposed model is tested using a simulated dataset from the Deep Underground Neutrino Experiment. The proposed architecture demonstrates the quantum advantage of learning faster than the classical convolutional neural networks (CNNs) under a similar number of parameters. In addition to faster convergence, the QCNN achieves greater test accuracy compared to CNNs. Based on experimental results, it is a promising direction to study the application of QCNN and other quantum machine learning models in high energy physics and additional scientific fields.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<yes>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<yes>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<yes>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Hierarchical clustering in particle physics through reinforcement learning. Particle physics experiments often require the reconstruction of decay patterns through a hierarchical clustering of the observed final-state particles. We show that this task can be phrased as a Markov Decision Process and adapt reinforcement learning algorithms to solve it. In particular, we show that Monte-Carlo Tree Search guided by a neural policy can construct high-quality hierarchical clusterings and outperform established greedy and beam search baselines.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<yes>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<yes>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Overview: Jet quenching with machine learning. Jets are suppressed and modified in heavy ion collisions, which serve as powerful probes to the properties of the quark-gluon plasma (QGP). Attributed to the abundant information carried by the jet constituents and reconstructed substructures, plenty of interesting applications of machine learning techniques have been made on a jet-by-jet basis to study the jet quenching phenomena. Here we review recent proceedings on this topic including the tasks of reconstructing jet momentum in heavy ion collisions, classifying quenched jets and unquenched jets, identifying jet energy loss, locating the jet creation points as well as distinguishing between quark- and gluon-initiated jets in the QGP. Such jet-by-jet analyses will allow us to have a better handle on the jet reconstruction and selections to investigate the effects of jet modifications and push forward the long-standing goal of jet tomographic probes of the QGP.",
    "output": "{Reviews}<yes>{Modern reviews}<no>{Specialized reviews}<yes>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Constraining CP-violation in the Higgs-top-quark interaction using machine-learning-based inference. While CP violation in the Higgs interactions with massive vector boson is already tightly constrained, the CP nature of the Higgs interactions with fermions is far less constrained. In this work, we assess the potential of machine-learning-based inference methods to constrain CP violation in the Higgs top-Yukawa coupling. This approach enables the use of the full available kinematic information. Concentrating on top-associated Higgs production with the Higgs decaying to two photons, we derive expected exclusion bounds for the LHC and the high-luminosity phase of the LHC. We also study the dependence of these bounds on the Higgs interaction with massive vector bosons and their robustness against theoretical uncertainties. In addition to deriving expected exclusion bounds, we discuss at which level a non-zero CP-violating top-Yukawa coupling can be distinguished from the SM. Moreover, we analyze which kinematic distributions are most sensitive to a CP-violating top-Yukawa coupling.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<yes>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<yes>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Auto-tuning capabilities of the ACTS track reconstruction suite. The reconstruction of charged particle trajectories is a crucial challenge of particle physics experiments as it directly impacts particle reconstruction and physics performances. To reconstruct these trajectories, different reconstruction algorithms are used sequentially. Each of these algorithms uses many configuration parameters that must be fine-tuned to properly account for the detector/experimental setup, the available CPU budget and the desired physics performance. Examples of such parameters are cut values limiting the algorithm's search space, approximations accounting for complex phenomenons, or parameters controlling algorithm performance. Until now, these parameters had to be optimised by human experts, which is inefficient and raises issues for the long-term maintainability of such algorithms. Previous experience using machine learning for particle reconstruction (such as the TrackML challenge) has shown that they can be easily adapted to different experiments by learning directly from the data. We propose to bring the same approach to the classic track reconstruction algorithms by connecting them to an agent-driven optimiser, allowing us to find the best input parameters using an iterative tuning approach. We have so far demonstrated this method on different track reconstruction algorithms within A Common Tracking Software (ACTS) framework using the Open Data Detector (ODD). These algorithms include the trajectory seed reconstruction and selection, the particle vertex reconstruction and the generation of simplified material maps used for trajectory reconstruction.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<yes>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<yes>{Hyperparameters}<yes>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Combining Resonant and Tail-based Anomaly Detection. In many well-motivated models of the electroweak scale, cascade decays of new particles can result in highly boosted hadronic resonances (e.g. $Z/W/h$). This can make these models rich and promising targets for recently developed resonant anomaly detection methods powered by modern machine learning. We demonstrate this using the state-of-the-art CATHODE method applied to supersymmetry scenarios with gluino pair production. We show that CATHODE, despite being model-agnostic, is nevertheless competitive with dedicated cut-based searches, while simultaneously covering a much wider region of parameter space. The gluino events also populate the tails of the missing energy and $H_T$ distributions, making this a novel combination of resonant and tail-based anomaly detection.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<yes>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<yes>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Efficiently Moving Instead of Reweighting Collider Events with Machine Learning. There are many cases in collider physics and elsewhere where a calibration dataset is used to predict the known physics and / or noise of a target region of phase space. This calibration dataset usually cannot be used out-of-the-box but must be tweaked, often with conditional importance weights, to be maximally realistic. Using resonant anomaly detection as an example, we compare a number of alternative approaches based on transporting events with normalizing flows instead of reweighting them. We find that the accuracy of the morphed calibration dataset depends on the degree to which the transport task is set up to carry out optimal transport, which motivates future research into this area.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<yes>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Neural Network Solutions of Bosonic Quantum Systems in One Dimension. Neural networks have been proposed as efficient numerical wavefunction ansatze which can be used to variationally search a wide range of functional forms for ground state solutions. These neural network methods are also advantageous in that more variational parameters and system degrees of freedom can be easily added. We benchmark the methodology by using neural networks to study several different integrable bosonic quantum systems in one dimension and compare our results to the exact solutions. While testing the scalability of the procedure to systems with many particles, we also introduce using symmetric function inputs to the neural network to enforce exchange symmetries of indistinguishable particles.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<yes>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Advanced Multi-Variate Analysis Methods for New Physics Searches at the Large Hadron Collider. Between the years 2015 and 2019, members of the Horizon 2020-funded Innovative Training Network named \"AMVA4NewPhysics\" studied the customization and application of advanced multivariate analysis methods and statistical learning tools to high-energy physics problems, as well as developed entirely new ones. Many of those methods were successfully used to improve the sensitivity of data analyses performed by the ATLAS and CMS experiments at the CERN Large Hadron Collider; several others, still in the testing phase, promise to further improve the precision of measurements of fundamental physics parameters and the reach of searches for new phenomena. In this paper, the most relevant new tools, among those studied and developed, are presented along with the evaluation of their performances.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<yes>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Probing Electroweak Phase Transition in Singlet scalar extension of Standard Model at HL-LHC through $bbZZ$ channel using parameterized machine learning. In this paper, a collider signature of a heavy Higgs boson at $14$ TeV HL-LHC is studied, where the heavy Higgs boson decays into a pair of standard model Higgs boson, which further decays to $bbZZ$ state and subsequently to $bb\\ell^{+} \\ell^{-}\\nu_{\\ell} \\nu_{\\ell}$ final state. To study this, we consider singlet scalar extension of the standard model and select the parameter space and mass of the heavy Higgs boson such that it prefers a strong first-order electroweak phase transition. The study is done following the $bbZZ$ analysis of CMS Collaboration and further using parameterized machine learning for final discrimination which simplifies the training process along with an improved discrimination between signal and background over the range of benchmark points. Despite the lower branching fraction, this channel can be a potential probe of the electroweak phase transition with the data sets collected by the CMS and ATLAS experiments at the $14$ TeV HL-LHC with $3$ $\\rm{ab}^{-1}$ of integrated luminosity and a production of resonant di-Higgs signal can be potentially discovered up to 490 GeV of resonance mass.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<yes>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Learning new physics efficiently with nonparametric methods. We present a machine learning approach for model-independent new physics searches. The corresponding algorithm is powered by recent large-scale implementations of kernel methods, nonparametric learning algorithms that can approximate any continuous function given enough data. Based on the original proposal by D'Agnolo and Wulzer (arXiv:1806.02350), the model evaluates the compatibility between experimental data and a reference model, by implementing a hypothesis testing procedure based on the likelihood ratio. Model-independence is enforced by avoiding any prior assumption about the presence or shape of new physics components in the measurements. We show that our approach has dramatic advantages compared to neural network implementations in terms of training times and computational resources, while maintaining comparable performances. In particular, we conduct our tests on higher dimensional datasets, a step forward with respect to previous studies.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<yes>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Predicting Feynman periods in $\\phi^4$-theory. We present efficient data-driven approaches to predict Feynman periods in $\\phi^4$-theory from properties of the underlying Feynman graphs. We find that the numbers of cuts and cycles determines the period to approximately 2% accuracy. Hepp bound and Martin invariant allow to predict the period with accuracy much better than 1%. In most cases, the period is a multi-linear function of the parameters in question. Besides classical correlation analysis, we also investigate the usefulness of machine-learning algorithms to predict the period. When sufficiently many properties of the graph are used, the period can be predicted with better than 0.05% relative accuracy. We use one of the constructed prediction models for weighted Monte-Carlo sampling of Feynman graphs, and compute the primitive contribution to the beta function of $\\phi^4$-theory at $L\\in \\left \\lbrace 13, 14, 15, 16 \\right \\rbrace $ loops. Our results confirm the previously known numerical estimates of the primitive beta function and improve their accuracy. Compared to uniform random sampling of graphs, our new algorithm reaches 35-fold higher accuracy in fixed runtime, or requires 1000-fold less runtime to reach a given accuracy. The data set of all periods computed for this work, combined with a previous data set, is made publicly available. Besides the physical application, it could serve as a benchmark for graph-based machine learning algorithms.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<yes>{Theory and physics for ML}<no>{ML for theory}<yes>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Progress in Nuclear Astrophysics: a multi-disciplinary field with still many open questions. Nuclear astrophysics is a multi-disciplinary field with a huge demand for nuclear data. Among its various fields, stellar evolution and nucleosynthesis are clearly the most closely related to nuclear physics. The need for nuclear data for astrophysics applications challenges experimental techniques as well as the robustness and predictive power of present nuclear models. Despite impressive progress for the last years, major problems and puzzles remain. In the present contribution, only a few nuclear astrophysics specific aspects are discussed. These concern some experimental progress related to the measurement of key reactions of relevance for the so-called s-and p-processes of nucleosynthesis, the theoretical effort in predicting nuclear properties of exotic neutron-rich nuclei of interest for the r-process nucleosynthesis, and the recent introduction of machine learning techniques in nuclear astrophysics applications.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<yes>{Tracking}<no>{Heavy Ions / Nuclear Physics}<yes>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Deep Learning Exotic Hadrons. We perform the first model independent analysis of experimental data using Deep Neural Networks to determine the nature of an exotic hadron. Specifically, we study the line shape of the $P_c(4312)$ signal reported by the LHCb collaboration and we find that its most likely interpretation is that of a virtual state. This method can be applied to other near-threshold resonance candidates.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<yes>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Using AI for Efficient Statistical Inference of Lattice Correlators Across Mass Parameters. Lattice QCD is notorious for its computational expense. Modern lattice simulations require large-scale computational resources to handle the large number of Dirac operator inversions used to construct correlation functions. Machine learning (ML) techniques that can increase, at the analysis level, the information inferred from the correlation functions would therefore be beneficial. We apply supervised learning to infer two-point lattice correlation functions at different target masses. Our work proposes a new method for separating data into training and bias correction subsets for efficient uncertainty estimation. We also benchmark our ML models against a simple ratio method.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<yes>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<yes>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "QCD or What?. Autoencoder networks, trained only on QCD jets, can be used to search for anomalies in jet-substructure. We show how, based either on images or on 4-vectors, they identify jets from decays of arbitrary heavy resonances. To control the backgrounds and the underlying systematics we can de-correlate the jet mass using an adversarial network. Such an adversarial autoencoder allows for a general and at the same time easily controllable search for new physics. Ideally, it can be trained and applied to data in the same phase space region, allowing us to efficiently search for new physics using un-supervised learning.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<yes>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Parameterized neural networks for high-energy physics. We investigate a new structure for machine learning classifiers applied to problems in high-energy physics by expanding the inputs to include not only measured features but also physics parameters. The physics parameters represent a smoothly varying learning task, and the resulting parameterized classifier can smoothly interpolate between them and replace sets of classifiers trained at individual values. This simplifies the training process and gives improved performance at intermediate values, even for complex problems requiring deep learning. Applications include tools parameterized in terms of theoretical model parameters, such as the mass of a particle, which allow for a single network to provide improved discrimination across a range of masses. This concept is simple to implement and allows for optimized interpolatable results.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<yes>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Fine-tuning machine-learned particle-flow reconstruction for new detector geometries in future colliders. We demonstrate transfer learning capabilities in a machine-learned algorithm trained for particle-flow reconstruction in high energy particle colliders. This paper presents a cross-detector fine-tuning study, where we initially pretrain the model on a large full simulation dataset from one detector design, and subsequently fine-tune the model on a sample with a different collider and detector design. Specifically, we use the Compact Linear Collider detector (CLICdet) model for the initial training set and demonstrate successful knowledge transfer to the CLIC-like detector (CLD) proposed for the Future Circular Collider in electron-positron mode. We show that with an order of magnitude less samples from the second dataset, we can achieve the same performance as a costly training from scratch, across particle-level and event-level performance metrics, including jet and missing transverse momentum resolution. Furthermore, we find that the fine-tuned model achieves comparable performance to the traditional rule-based particle-flow approach on event-level metrics after training on 100,000 CLD events, whereas a model trained from scratch requires at least 1 million CLD events to achieve similar reconstruction performance. To our knowledge, this represents the first full-simulation cross-detector transfer learning study for particle-flow reconstruction. These findings offer valuable insights towards building large foundation models that can be fine-tuned across different detector designs and geometries, helping to accelerate the development cycle for new detectors and opening the door to rapid detector design and optimization using machine learning.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<yes>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<yes>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Machine learning based event reconstruction for the MUonE experiment. A proof-of-concept solution based on the machine learning techniques has been implemented and tested within the MUonE experiment designed to search for New Physics in the sector of anomalous magnetic moment of a muon. The results of the DNN based algorithm are comparable to the classical reconstruction, reducing enormously the execution time for the pattern recognition phase. The present implementation meets the conditions of classical reconstruction, providing an advantageous basis for further studies.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<yes>{Pileup}<no>{Calibration}<yes>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Neutrino type identification for atmospheric neutrinos in a large homogeneous liquid scintillation detector. Atmospheric neutrino oscillations are important to the study of neutrino properties, including the neutrino mass ordering problem. A good capability to identify neutrinos' flavor and neutrinos against antineutrinos is crucial in such measurements. In this paper, we present a machine-learning-based approach for identifying atmospheric neutrino events in a large homogeneous liquid scintillator detector. This method identifies features of PMT waveforms that reflect event topologies and uses them as input to machine learning models. In addition, neutron-capture information is utilized to achieve neutrino versus antineutrino discrimination. Preliminary performances based on Monte Carlo simulations are presented, which demonstrate such a detector's potential in future measurements of atmospheric neutrinos such as the one planned for the JUNO experiment.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<yes>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<yes>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Multi-scale cross-attention transformer encoder for event classification. We deploy an advanced Machine Learning (ML) environment, leveraging a multi-scale cross-attention encoder for event classification, towards the identification of the $gg\\to H\\to hh\\to b\\bar b b\\bar b$ process at the High Luminosity Large Hadron Collider (HL-LHC), where $h$ is the discovered Standard Model (SM)-like Higgs boson and $H$ a heavier version of it (with $m_H>2m_h$). In the ensuing boosted Higgs regime, the final state consists of two fat jets. Our multi-modal network can extract information from the jet substructure and the kinematics of the final state particles through self-attention transformer layers. The diverse learned information is subsequently integrated to improve classification performance using an additional transformer encoder with cross-attention heads. We ultimately prove that our approach surpasses in performance current alternative methods used to establish sensitivity to this process, whether solely based on kinematic analysis or else on a combination of this with mainstream ML approaches. Then, we employ various interpretive methods to evaluate the network results, including attention map analysis and visual representation of Gradient-weighted Class Activation Mapping (Grad-CAM). Finally, we note that the proposed network is generic and can be applied to analyse any process carrying information at different scales. Our code is publicly available for generic use.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<yes>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<yes>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<yes>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "The Full Event Interpretation}: {An Exclusive Tagging Algorithm for the Belle II Experiment. The Full Event Interpretation is presented: a new exclusive tagging algorithm used by the high-energy physics experiment Belle II. The experimental setup of Belle II allows the precise measurement of otherwise inaccessible $B$ meson decay-modes. The Full Event Interpretation algorithm enables many of these measurements. The algorithm relies on machine learning to automatically identify plausible $B$ meson decay chains based on the data recorded by the detector. Compared to similar algorithms employed by previous experiments, the Full Event Interpretation provides a greater efficiency, yielding a larger effective sample size usable in the measurement.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<yes>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<yes>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<yes>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<yes>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Unlocking Multi-Dimensional Integration with Quantum Adaptive Importance Sampling. We introduce a quantum algorithm that performs Quantum Adaptive Importance Sampling (QAIS) for Monte Carlo integration of multidimensional functions, targeting in particular the computational challenges of high-energy physics. In this domain, the fundamental ingredients for theoretical predictions such as multiloop Feynman diagrams and the phase-space require evaluating high-dimensional integrals that are computationally demanding due to divergences and complex mathematical structures. The established method of Adaptive Importance Sampling, as implemented in tools like VEGAS, uses a grid-based approach that is iteratively refined in a separable way, per dimension. This separable approach efficiently suppresses the exponentially growing grid-handling computational cost, but also introduces performance drawbacks whenever strong inter-variable correlations are present. To utilize sampling resources more efficiently, QAIS exploits the exponentially large Hilbert space of a Parameterised Quantum Circuit (PQC) to manipulate a non-separable Probability Density Function (PDF) defined on a multidimensional grid. In this setting, entanglement within the PQC captures the correlations and intricacies of the target integrand's structure. Performing measurements on the PQC determines the sample allocation across the multidimensional grid. This focuses samples in the small subspace where the important structures of the target integrand lie, and thus generates very precise integral estimations. As an application, we look at a very sharply peaked loop Feynman integral and at multi-modal benchmark integrals.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<yes>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<yes>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<yes>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Measurements of multijet event isotropies using optimal transport with the ATLAS detector. A measurement of novel event shapes quantifying the isotropy of collider events is performed in 140 fb$^{-1}$ of proton-proton collisions with $\\sqrt s=13$ TeV centre-of-mass energy recorded with the ATLAS detector at CERN's Large Hadron Collider. These event shapes are defined as the Wasserstein distance between collider events and isotropic reference geometries. This distance is evaluated by solving optimal transport problems, using the 'Energy-Mover's Distance'. Isotropic references with cylindrical and circular symmetries are studied, to probe the symmetries of interest at hadron colliders. The novel event-shape observables defined in this way are infrared- and collinear-safe, have improved dynamic range and have greater sensitivity to isotropic radiation patterns than other event shapes. The measured event-shape variables are corrected for detector effects, and presented in inclusive bins of jet multiplicity and the scalar sum of the two leading jets' transverse momenta. The measured distributions are provided as inputs to future Monte Carlo tuning campaigns and other studies probing fundamental properties of QCD and the production of hadronic final states up to the TeV-scale.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<yes>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<yes>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Reconstructing particles in jets using set transformer and hypergraph prediction networks. The task of reconstructing particles from low-level detector response data to predict the set of final state particles in collision events represents a set-to-set prediction task requiring the use of multiple features and their correlations in the input data. We deploy three separate set-to-set neural network architectures to reconstruct particles in events containing a single jet in a fully-simulated calorimeter. Performance is evaluated in terms of particle reconstruction quality, properties regression, and jet-level metrics. The results demonstrate that such a high dimensional end-to-end approach succeeds in surpassing basic parametric approaches in disentangling individual neutral particles inside of jets and optimizing the use of complementary detector information. In particular, the performance comparison favors a novel architecture based on learning hypergraph structure, HGPflow, which benefits from a physically-interpretable approach to particle reconstruction.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<yes>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<yes>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Application of Quantum Machine Learning using the Quantum Kernel Algorithm on High Energy Physics Analysis at the LHC. Quantum machine learning could possibly become a valuable alternative to classical machine learning for applications in High Energy Physics by offering computational speed-ups. In this study, we employ a support vector machine with a quantum kernel estimator (QSVM-Kernel method) to a recent LHC flagship physics analysis: $t\\bar{t}H$ (Higgs boson production in association with a top quark pair). In our quantum simulation study using up to 20 qubits and up to 50000 events, the QSVM-Kernel method performs as well as its classical counterparts in three different platforms from Google Tensorflow Quantum, IBM Quantum and Amazon Braket. Additionally, using 15 qubits and 100 events, the application of the QSVM-Kernel method on the IBM superconducting quantum hardware approaches the performance of a noiseless quantum simulator. Our study confirms that the QSVM-Kernel method can use the large dimensionality of the quantum Hilbert space to replace the classical feature space in realistic physics datasets.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<yes>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<yes>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Diffusion-model approach to flavor models: A case study for $S_4^\\prime$ modular flavor model. We propose a numerical method of searching for parameters with experimental constraints in generic flavor models by utilizing diffusion models, which are classified as a type of generative artificial intelligence (generative AI). As a specific example, we consider the $S_4^\\prime$ modular flavor model and construct a neural network that reproduces quark masses, the CKM matrix, and the Jarlskog invariant by treating free parameters in the flavor model as generating targets. By generating new parameters with the trained network, we find various phenomenologically interesting parameter regions where an analytical evaluation of the $S_4^\\prime$ model is challenging. Additionally, we confirm that the spontaneous CP violation occurs in the $S_4^\\prime$ model. The diffusion model enables an inverse problem approach, allowing the machine to provide a series of plausible model parameters from given experimental data. Moreover, it can serve as a versatile analytical tool for extracting new physical predictions from flavor models.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<yes>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<yes>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<yes>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Learning Symmetry-Independent Jet Representations via Jet-Based Joint Embedding Predictive Architecture. In high energy physics, self-supervised learning (SSL) methods have the potential to aid in the creation of machine learning models without the need for labeled datasets for a variety of tasks, including those related to jets -- narrow sprays of particles produced by quarks and gluons in high energy particle collisions. This study introduces an approach to learning jet representations without hand-crafted augmentations using a jet-based joint embedding predictive architecture (J-JEPA), which aims to predict various physical targets from an informative context. As our method does not require hand-crafted augmentation like other common SSL techniques, J-JEPA avoids introducing biases that could harm downstream tasks. Since different tasks generally require invariance under different augmentations, this training without hand-crafted augmentation enables versatile applications, offering a pathway toward a cross-task foundation model. We finetune the representations learned by J-JEPA for jet tagging and benchmark them against task-specific representations.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<yes>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<yes>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Searching for anomalous quartic gauge couplings at muon colliders using principle component analysis. Searching for new physics~(NP) is one of the areas of high-energy physics that requires the most processing of large amounts of data. At the same time, quantum computing has huge potential advantages when dealing with large amounts of data. The principal component analysis~(PCA) algorithm may be one of the bridges connecting these two aspects. On the one hand, it can be used for anomaly detection, and on the other hand, there are corresponding quantum algorithms for PCA. In this paper, we investigate how to use PCA to search for NP. Taking the example of anomalous quartic gauge couplings in the tri-photon process at muon colliders, we find that PCA can be used to search for NP. Compared with the traditional event selection strategy, the expected constraints on the operator coefficients obtained by PCA based event selection strategy are even better.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<yes>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "From strange-quark tagging to fragmentation tagging with machine learning. We apply advanced machine learning techniques to two challenging jet classification problems at the LHC. The first is strange-quark tagging, in particular distinguishing strange-quark jets from down-quark jets. The second, which we term fragmentation tagging, involves identifying the fragmentation channel of a quark. We exemplify the latter by training neural networks to differentiate between bottom jets containing a bottom baryon and those containing a bottom meson. The common challenge in these two problems is that neither quark lifetimes and masses nor parton showering provide discriminating tools, making it necessary to rely on differences in the distributions of the hadron types contained in each type of jet and their kinematics. For these classification tasks, we employ variations of Graph Attention Networks and the Particle Transformer, which receive jet and all constituent properties as inputs. We compare their performance to a simple Multilayer Perceptron that uses simple variables. We find that the more sophisticated architectures do not improve $s$-quark versus $d$-quark jet differentiation by a significant amount, but they do lead to a significant gain in $b$-baryon versus $b$-meson jet differentiation.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<yes>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Pulling the Higgs and Top needles from the jet stack with Feature Extended Supervised Tagging. Jet tagging has become an essential tool for new physics searches at the high-energy frontier. For jets that contain energetic charged leptons we introduce Feature Extended Supervised Tagging (FEST) which, in addition to jet substructure, considers the features of the charged lepton within the jet. With this method we build dedicated taggers to discriminate among boosted $H \\to \\ell \\nu q \\bar q$, $t \\to \\ell \\nu b$, and QCD jets (with $\\ell$ an electron or muon). The taggers have an impressive performance, allowing for overall light jet rejection factors of $10^4-10^5$, for top quark / Higgs boson efficiencies of $0.5$. The taggers are also excellent in the discrimination of Higgs bosons from top quarks and vice versa, for example rejecting top quarks by factors of $100-300$ for Higgs boson efficiencies of $0.5$. We demonstrate the potential of these taggers to improve the sensitivity to new physics by using as example a search for a new $Z'$ boson decaying into $Z H$, in the fully-hadronic final state.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<yes>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Comparing machine learning models for tau triggers. This paper introduces supervised learning techniques for real-time selection (triggering) of hadronically decaying tau leptons in proton-proton colliders. By implementing classic machine learning decision trees and advanced deep learning models, such as Multi-Layer Perceptron or residual neural networks, visible improvements in performance compared to standard threshold tau triggers are observed. We show how such an implementation may lower selection energy thresholds, thus contributing to increasing the sensitivity of searches for new phenomena in proton-proton collisions classified by low-energy tau leptons. Moreover, we analyze when it is better to use neural networks versus decision trees for tau triggers with conclusions relevant to other problems in physics.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<yes>{Software}<no>{Hardware/firmware}<yes>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Fast Simulation of a High Granularity Calorimeter by Generative Adversarial Networks. We present the 3DGAN for the simulation of a future high granularity calorimeter output as three-dimensional images. We prove the efficacy of Generative Adversarial Networks (GANs) for generating scientific data while retaining a high level of accuracy for diverse metrics across a large range of input variables. We demonstrate a successful application of the transfer learning concept: we train the network to simulate showers for electrons from a reduced range of primary energies, we then train further for a five times larger range (the model could not train for the larger range directly). The same concept is extended to generate showers for other particles (photons and neutral pions) depositing most of their energies in electromagnetic interactions. In addition, the generation of charged pion showers is also explored, a more accurate effort would require additional data from other detectors not included in the scope of the current work. Our further contribution is a demonstration of using GAN-generated data for a practical application. We train a third-party network using GAN-generated data and prove that the response is similar to a network trained with data from the Monte Carlo simulation. The showers generated by GAN present accuracy within $10\\%$ of Monte Carlo for a diverse range of physics features, with three orders of magnitude speedup. The speedup for both the training and inference can be further enhanced by distributed training.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<yes>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Deep learning for flow observables in high energy heavy-ion collisions. We demonstrate how deep convolutional neural networks can be trained to predict 2+1 D hydrodynamic simulation results for flow coefficients, mean-transverse-momentum and charged particle multiplicity from the initial energy density profile. We show that this method provides results that are accurate enough, so that one can use neural networks to reliably estimate multi-particle flow correlators. Additionally, we train networks that can take any model parameter as an additional input and demonstrate with a few examples that the accuracy remains good. The usage of neural networks can reduce the computation time needed in performing Bayesian analyses with multi-particle flow correlators by many orders of magnitude.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<yes>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Advancing Physics Data Analysis through Machine Learning and Physics-Informed Neural Networks. In an era increasingly focused on green computing and explainable AI, revisiting traditional approaches in theoretical and phenomenological particle physics is paramount. This project evaluates various machine learning (ML) algorithms-including Nearest Neighbors, Decision Trees, Random Forest, AdaBoost, Naive Bayes, Quadratic Discriminant Analysis (QDA), and XGBoost-alongside standard neural networks and a novel Physics-Informed Neural Network (PINN) for physics data analysis. We apply these techniques to a binary classification task that distinguishes the experimental viability of simulated scenarios based on Higgs observables and essential parameters. Through this comprehensive analysis, we aim to showcase the capabilities and computational efficiency of each model in binary classification tasks, thereby contributing to the ongoing discourse on integrating ML and Deep Neural Networks (DNNs) into physics research. In this study, XGBoost emerged as the preferred choice among the evaluated machine learning algorithms for its speed and effectiveness, especially in the initial stages of computation with limited datasets. However, while standard Neural Networks and Physics-Informed Neural Networks (PINNs) demonstrated superior performance in terms of accuracy and adherence to physical laws, they require more computational time. These findings underscore the trade-offs between computational efficiency and model sophistication.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<yes>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<yes>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<yes>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Binary JUNIPR: an interpretable probabilistic model for discrimination. JUNIPR is an approach to unsupervised learning in particle physics that scaffolds a probabilistic model for jets around their representation as binary trees. Separate JUNIPR models can be learned for different event or jet types, then compared and explored for physical insight. The relative probabilities can also be used for discrimination. In this paper, we show how the training of the separate models can be refined in the context of classification to optimize discrimination power. We refer to this refined approach as Binary JUNIPR. Binary JUNIPR achieves state-of-the-art performance for quark/gluon discrimination and top-tagging. The trained models can then be analyzed to provide physical insight into how the classification is achieved. As examples, we explore differences between quark and gluon jets and between gluon jets generated with two different simulations.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<yes>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "The Machine Learning Landscape of Top Taggers. Based on the established task of identifying boosted, hadronically decaying top quarks, we compare a wide range of modern machine learning approaches. Unlike most established methods they rely on low-level input, for instance calorimeter output. While their network architectures are vastly different, their performance is comparatively similar. In general, we find that these new approaches are extremely powerful and great fun.",
    "output": "{Reviews}<yes>{Modern reviews}<no>{Specialized reviews}<yes>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<yes>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Reconstruction of muon bundles in KM3NeT detectors using machine learning methods. The KM3NeT Collaboration is installing the ARCA and ORCA neutrino detectors at the bottom of the Mediterranean Sea. The focus of ARCA is neutrino astronomy, while ORCA is optimised for neutrino oscillation studies. Both detectors are already operational in their intermediate states and collect valuable data, including the measurements of the muons produced by cosmic ray interactions in the atmosphere. This work explores the potential of machine learning models for the reconstruction of muon bundles, which are multi-muon events. For this, data collected with intermediate detector configurations of ARCA and ORCA was used in addition to simulated data from the envisaged final configurations of those detectors. Prediction of the total number of muons in a bundle as well as their total energy and even the energy of the primary cosmic ray is presented.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<yes>{Neutrino Detectors}<yes>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "FlexCAST: Enabling Flexible Scientific Data Analyses. The development of scientific data analyses is a resource-intensive process that often yields results with untapped potential for reuse and reinterpretation. In many cases, a developed analysis can be used to measure more than it was designed for, by changing its input data or parametrization. Existing reinterpretation frameworks, such as RECAST, enable analysis reinterpretation by preserving the analysis implementation to allow for changes of particular parts of the input data. We introduce FlexCAST, which generalizes this concept by preserving the analysis design itself, supporting changes to the entire input data and analysis parametrization. FlexCAST is based on three core principles: modularity, validity, and robustness. Modularity enables a change of the input data and parametrization, while validity ensures that the obtained results remain meaningful, and robustness ensures that as many configurations as possible yield meaningful results. While not being limited to data-driven machine learning techniques, FlexCAST is particularly valuable for the reinterpretation of analyses in this context, where changes in the input data can significantly impact the parametrization of the analysis. Using a state-of-the-art anomaly detection analysis on LHC-like data, we showcase FlexCAST's core principles and demonstrate how it can expand the reach of scientific data analysis through flexible reuse and reinterpretation.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<yes>{Pileup}<no>{Calibration}<no>{Recasting}<yes>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "PIPPIN: Generating variable length full events from partons. This paper presents a novel approach for directly generating full events at detector-level from parton-level information, leveraging cutting-edge machine learning techniques. To address the challenge of multiplicity variations between parton and reconstructed object spaces, we employ transformers, score-based models and normalizing flows. Our method tackles the inherent complexities of the stochastic transition between these two spaces and achieves remarkably accurate results. The combination of innovative techniques and the achieved accuracy demonstrates the potential of our approach in advancing the field and opens avenues for further exploration. This research contributes to the ongoing efforts in high-energy physics and generative modelling, providing a promising direction for enhanced precision in fast detector simulation.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<yes>{Diffusion Models}<yes>{Transformer Models}<yes>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Electron-nucleus cross sections from transfer learning. Transfer learning (TL) allows a deep neural network (DNN) trained on one type of data to be adapted for new problems with limited information. We propose to use the TL technique in physics. The DNN learns the details of one process, and after fine-tuning, it makes predictions for related processes. We consider the DNNs, trained on inclusive electron-carbon scattering data, and show that after fine-tuning, they accurately predict cross sections for electron interactions with nuclear targets ranging from helium-3 to iron.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<yes>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Clustering of electromagnetic showers and particle interactions with graph neural networks in liquid argon time projection chambers. Liquid Argon Time Projection Chambers (LArTPCs) are a class of detectors that produce high resolution images of charged particles within their sensitive volume. In these images, the clustering of distinct particles into superstructures is of central importance to the current and future neutrino physics program. Electromagnetic (EM) activity typically exhibits spatially detached fragments of varying morphology and orientation that are challenging to efficiently assemble using traditional algorithms. Similarly, particles that are spatially removed from each other in the detector may originate from a common interaction. Graph Neural Networks (GNNs) were developed in recent years to find correlations between objects embedded in an arbitrary space. The Graph Particle Aggregator (GrapPA) first leverages GNNs to predict the adjacency matrix of EM shower fragments and to identify the origin of showers, i.e. primary fragments. On the PILArNet public LArTPC simulation dataset, the algorithm achieves achieves a shower clustering accuracy characterized by a mean adjusted Rand index (ARI) of 97.8 % and a primary identification accuracy of 99.8 %. It yields a relative shower energy resolution of $(4.1+1.4/\\sqrt{E (\\text{GeV})})\\,\\%$ and a shower direction resolution of $(2.1/\\sqrt{E(\\text{GeV})})^{\\circ}$. The optimized algorithm is then applied to the related task of clustering particle instances into interactions and yields a mean ARI of 99.2 % for an interaction density of $\\sim\\mathcal{O}(1)\\,m^{-3}$.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<yes>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Towards a Computer Vision Particle Flow. In High Energy Physics experiments Particle Flow (PFlow) algorithms are designed to provide an optimal reconstruction of the nature and kinematic properties of the particles produced within the detector acceptance during collisions. At the heart of PFlow algorithms is the ability to distinguish the calorimeter energy deposits of neutral particles from those of charged particles, using the complementary measurements of charged particle tracking devices, to provide a superior measurement of the particle content and kinematics. In this paper, a computer vision approach to this fundamental aspect of PFlow algorithms, based on calorimeter images, is proposed. A comparative study of the state of the art deep learning techniques is performed. A significantly improved reconstruction of the neutral particle calorimeter energy deposits is obtained in a context of large overlaps with the deposits from charged particles. Calorimeter images with augmented finer granularity are also obtained using super-resolution techniques.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<yes>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<yes>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Phase Space Sampling and Inference from Weighted Events with Autoregressive Flows. We explore the use of autoregressive flows, a type of generative model with tractable likelihood, as a means of efficient generation of physical particle collider events. The usual maximum likelihood loss function is supplemented by an event weight, allowing for inference from event samples with variable, and even negative event weights. To illustrate the efficacy of the model, we perform experiments with leading-order top pair production events at an electron collider with importance sampling weights, and with next-to-leading-order top pair production events at the LHC that involve negative weights.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<yes>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Generative Diffusion Models for Fast Simulations of Particle Collisions at CERN. In High Energy Physics simulations play a crucial role in unraveling the complexities of particle collision experiments within CERN's Large Hadron Collider. Machine learning simulation methods have garnered attention as promising alternatives to traditional approaches. While existing methods mainly employ Variational Autoencoders (VAEs) or Generative Adversarial Networks (GANs), recent advancements highlight the efficacy of diffusion models as state-of-the-art generative machine learning methods. We present the first simulation for Zero Degree Calorimeter (ZDC) at the ALICE experiment based on diffusion models, achieving the highest fidelity compared to existing baselines. We perform an analysis of trade-offs between generation times and the simulation quality. The results indicate a significant potential of latent diffusion model due to its rapid generation time.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<yes>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Fitting a Deep Generative Hadronization Model. Hadronization is a critical step in the simulation of high-energy particle and nuclear physics experiments. As there is no first principles understanding of this process, physically-inspired hadronization models have a large number of parameters that are fit to data. Deep generative models are a natural replacement for classical techniques, since they are more flexible and may be able to improve the overall precision. Proof of principle studies have shown how to use neural networks to emulate specific hadronization when trained using the inputs and outputs of classical methods. However, these approaches will not work with data, where we do not have a matching between observed hadrons and partons. In this paper, we develop a protocol for fitting a deep generative hadronization model in a realistic setting, where we only have access to a set of hadrons in data. Our approach uses a variation of a Generative Adversarial Network with a permutation invariant discriminator. We find that this setup is able to match the hadronization model in Herwig with multiple sets of parameters. This work represents a significant step forward in a longer term program to develop, train, and integrate machine learning-based hadronization models into parton shower Monte Carlo programs.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<yes>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Per-Object Systematics using Deep-Learned Calibration. We show how to treat systematic uncertainties using Bayesian deep networks for regression. First, we analyze how these networks separately trace statistical and systematic uncertainties on the momenta of boosted top quarks forming fat jets. Next, we propose a novel calibration procedure by training on labels and their error bars. Again, the network cleanly separates the different uncertainties. As a technical side effect, we show how Bayesian networks can be extended to describe non-Gaussian features.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<yes>{Pileup}<no>{Calibration}<yes>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "A continuous calibration of the ATLAS flavour-tagging classifiers via optimal transportation maps. A calibration of the ATLAS flavour-tagging algorithms using a new calibration procedure based on optimal transportation maps is presented. Simultaneous, continuous corrections to the $b$-jet, $c$-jet, and light-flavour jet classification probabilities from jet-tagging algorithms in simulation are derived for $b$-jets using $t\\bar t \\to e\\mu\\nu\\nu bb$ data. After application of the derived calibration maps, closure between simulation and observation is achieved for jet flavour observables used in ATLAS analyses of Large Hadron Collider (LHC) Run 2 proton-proton collision data. This continuous calibration opens up new possibilities for the future use of jet flavour information in LHC analyses and also serves as a guide for deriving high-dimensional corrections to simulation via transportation maps, an important development for a broad range of inference tasks.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<yes>{top quark tagging}<no>{strange jets}<yes>{$b$-tagging}<yes>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<yes>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<yes>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Unbinned Deep Learning Jet Substructure Measurement in High $Q^2$ ep collisions at HERA. The radiation pattern within high energy quark- and gluon-initiated jets (jet substructure) is used extensively as a precision probe of the strong force as well as an environment for optimizing event generators with numerous applications in high energy particle and nuclear physics. Looking at electron-proton collisions is of particular interest as many of the complications present at hadron colliders are absent. A detailed study of modern jet substructure observables, jet angularities, in electron-proton collisions is presented using data recorded using the H1 detector at HERA. The measurement is unbinned and multi-dimensional, using machine learning to correct for detector effects. All of the available reconstructed object information of the respective jets is interpreted by a graph neural network, achieving superior precision on a selected set of jet angularities. Training these networks was enabled by the use of a large number of GPUs in the Perlmutter supercomputer at Berkeley Lab. The particle jets are reconstructed in the laboratory frame, using the $k_{\\mathrm{T}}$ jet clustering algorithm. Results are reported at high transverse momentum transfer $Q^2>150$ GeV${}^2$, and inelasticity $0.2 < y < 0.7$. The analysis is also performed in sub-regions of $Q^2$, thus probing scale dependencies of the substructure variables. The data are compared with a variety of predictions and point towards possible improvements of such models.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<yes>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<yes>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Machine Learning in the Search for New Fundamental Physics. Machine learning plays a crucial role in enhancing and accelerating the search for new fundamental physics. We review the state of machine learning methods and applications for new physics searches in the context of terrestrial high energy physics experiments, including the Large Hadron Collider, rare event searches, and neutrino experiments. While machine learning has a long history in these fields, the deep learning revolution (early 2010s) has yielded a qualitative shift in terms of the scope and ambition of research. These modern machine learning developments are the focus of the present review.",
    "output": "{Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Conditional Generative Modelling of Reconstructed Particles at Collider Experiments. The simulation of particle physics data is a fundamental but computationally intensive ingredient for physics analysis at the Large Hadron Collider, where observational set-valued data is generated conditional on a set of incoming particles. To accelerate this task, we present a novel generative model based on a graph neural network and slot-attention components, which exceeds the performance of pre-existing baselines.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<yes>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Fuzzy Jets. Collimated streams of particles produced in high energy physics experiments are organized using clustering algorithms to form jets. To construct jets, the experimental collaborations based at the Large Hadron Collider (LHC) primarily use agglomerative hierarchical clustering schemes known as sequential recombination. We propose a new class of algorithms for clustering jets that use infrared and collinear safe mixture models. These new algorithms, known as fuzzy jets, are clustered using maximum likelihood techniques and can dynamically determine various properties of jets like their size. We show that the fuzzy jet size adds additional information to conventional jet tagging variables. Furthermore, we study the impact of pileup and show that with some slight modifications to the algorithm, fuzzy jets can be stable up to high pileup interaction multiplicities.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<yes>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<yes>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Deep learning techniques for energy clustering in the CMS ECAL. The reconstruction of electrons and photons in CMS depends on topological clustering of the energy deposited by an incident particle in different crystals of the electromagnetic calorimeter (ECAL). These clusters are formed by aggregating neighbouring crystals according to the expected topology of an electromagnetic shower in the ECAL. The presence of upstream material (beampipe, tracker and support structures) causes electrons and photons to start showering before reaching the calorimeter. This effect, combined with the 3.8T CMS magnetic field, leads to energy being spread in several clusters around the primary one. It is essential to recover the energy contained in these satellite clusters in order to achieve the best possible energy resolution for physics analyses. Historically satellite clusters have been associated to the primary cluster using a purely topological algorithm which does not attempt to remove spurious energy deposits from additional pileup interactions (PU). The performance of this algorithm is expected to degrade during LHC Run 3 (2022+) because of the larger average PU levels and the increasing levels of noise due to the ageing of the ECAL detector. New methods are being investigated that exploit state-of-the-art deep learning architectures like Graph Neural Networks (GNN) and self-attention algorithms. These more sophisticated models improve the energy collection and are more resilient to PU and noise, helping to preserve the electron and photon energy resolution achieved during LHC Runs 1 and 2. This work will cover the challenges of training the models as well the opportunity that this new approach offers to unify the ECAL energy measurement with the particle identification steps used in the global CMS photon and electron reconstruction.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<yes>{Pileup}<no>{Calibration}<yes>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Flow-based sampling for lattice field theories. Critical slowing down and topological freezing severely hinder Monte Carlo sampling of lattice field theories as the continuum limit is approached. Recently, significant progress has been made in applying a class of generative machine learning models, known as \"flow-based\" samplers, to combat these issues. These generative samplers also enable promising practical improvements in Monte Carlo sampling, such as fully parallelized configuration generation. These proceedings review the progress towards this goal and future prospects of the method.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<yes>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<yes>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<yes>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Jet Flavour Classification Using DeepJet. Jet flavour classification is of paramount importance for a broad range of applications in modern-day high-energy-physics experiments, particularly at the LHC. In this paper we propose a novel architecture for this task that exploits modern deep learning techniques. This new model, called DeepJet, overcomes the limitations in input size that affected previous approaches. As a result, the heavy flavour classification performance improves, and the model is extended to also perform quark-gluon tagging.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<yes>{Jet images}<no>{Event images}<no>{Sequences}<yes>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<yes>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Contrastive Learning for Robust Representations of Neutrino Data. In neutrino physics, analyses often depend on large simulated datasets, making it essential for models to generalise effectively to real-world detector data. Contrastive learning, a well-established technique in deep learning, offers a promising solution to this challenge. By applying controlled data augmentations to simulated data, contrastive learning enables the extraction of robust and transferable features. This improves the ability of models trained on simulations to adapt to real experimental data distributions. In this paper, we investigate the application of contrastive learning methods in the context of neutrino physics. Through a combination of empirical evaluations and theoretical insights, we demonstrate how contrastive learning enhances model performance and adaptability. Additionally, we compare it to other domain adaptation techniques, highlighting the unique advantages of contrastive learning for this field.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<yes>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Machine learning-based jet and event classification at the Electron-Ion Collider with applications to hadron structure and spin physics. We explore machine learning-based jet and event identification at the future Electron-Ion Collider (EIC). We study the effectiveness of machine learning-based classifiers at relatively low EIC energies, focusing on (i) identifying the flavor of the jet and (ii) identifying the underlying hard process of the event. We propose applications of our machine learning-based jet identification in the key research areas at the future EIC and current Relativistic Heavy Ion Collider program, including enhancing constraints on (transverse momentum dependent) parton distribution functions, improving experimental access to transverse spin asymmetries, studying photon structure, and quantifying the modification of hadrons and jets in the cold nuclear matter environment in electron-nucleus collisions. We establish first benchmarks and contrast the estimated performance of flavor tagging at the EIC with that at the Large Hadron Collider. We perform studies relevant to aspects of detector design including particle identification, charge information, and minimum transverse momentum capabilities. Additionally, we study the impact of using full event information instead of using only information associated with the identified jet. These methods can be deployed either on suitably accurate Monte Carlo event generators, or, for several applications, directly on experimental data. We provide an outlook for ultimately connecting these machine learning-based methods with first principles calculations in quantum chromodynamics.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<yes>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "The information content of jet quenching and machine learning assisted observable design. Jets produced in high-energy heavy-ion collisions are modified compared to those in proton-proton collisions due to their interaction with the deconfined, strongly-coupled quark-gluon plasma (QGP). In this work, we employ machine learning techniques to identify important features that distinguish jets produced in heavy-ion collisions from jets produced in proton-proton collisions. We formulate the problem using binary classification and focus on leveraging machine learning in ways that inform theoretical calculations of jet modification: (i) we quantify the information content in terms of Infrared Collinear (IRC)-safety and in terms of hard vs. soft emissions, (ii) we identify optimally discriminating observables that are in principle calculable in perturbative QCD, and (iii) we assess the information loss due to the heavy-ion underlying event and background subtraction algorithms. We illustrate our methodology using Monte Carlo event generators, where we find that important information about jet quenching is contained not only in hard splittings but also in soft emissions and IRC-unsafe physics inside the jet. This information appears to be significantly reduced by the presence of the underlying event. We discuss the implications of this for the prospect of using jet quenching to extract properties of the QGP. Since the training labels are exactly known, this methodology can be used directly on experimental data without reliance on modeling. We outline a proposal for how such an experimental analysis can be carried out, and how it can guide future measurements.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<yes>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "$\\Sigma$ Resonances from a Neural Network-based Partial Wave Analysis on $K^-p$ Scattering. We implement a convolutional neural network to study the $\\Sigma$ hyperons using experimental data of the $K^-p\\to\\pi^0\\Lambda$ reaction. The averaged accuracy of the NN models in resolving resonances on the test data sets is ${\\rm 98.5\\%}$, ${\\rm 94.8\\%}$ and ${\\rm 82.5\\%}$ for one-, two- and three-additional-resonance case. We find that the three most significant resonances are $1/2^+$, $3/2^+$ and $3/2^-$ states with mass being ${\\rm 1.62(11)~GeV}$, ${\\rm 1.72(6)~GeV}$ and ${\\rm 1.61(9)~GeV}$, and probability being $\\rm 100(3)\\%$, $\\rm 72(24)\\%$ and $\\rm 98(52)\\%$, respectively, where the errors mostly come from the uncertainties of the experimental data. Our results support the three-star $\\Sigma(1660)1/2^+$, the one-star $\\Sigma(1780)3/2^+$ and the one-star $\\Sigma(1580)3/2^-$ in PDG. The ability of giving quantitative probabilities in resonance resolving and numerical stability make NN potentially a life-changing tool in baryon partial wave analysis, and this approach can be easily extended to accommodate other theoretical models and/or to include more experimental data.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<yes>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<yes>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<yes>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "EPiC-GAN: Equivariant Point Cloud Generation for Particle Jets. With the vast data-collecting capabilities of current and future high-energy collider experiments, there is an increasing demand for computationally efficient simulations. Generative machine learning models enable fast event generation, yet so far these approaches are largely constrained to fixed data structures and rigid detector geometries. In this paper, we introduce EPiC-GAN - equivariant point cloud generative adversarial network - which can produce point clouds of variable multiplicity. This flexible framework is based on deep sets and is well suited for simulating sprays of particles called jets. The generator and discriminator utilize multiple EPiC layers with an interpretable global latent vector. Crucially, the EPiC layers do not rely on pairwise information sharing between particles, which leads to a significant speed-up over graph- and transformer-based approaches with more complex relation diagrams. We demonstrate that EPiC-GAN scales well to large particle multiplicities and achieves high generation fidelity on benchmark jet generation tasks.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<yes>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<yes>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Associated production of Higgs and single top at the LHC in presence of the SMEFT operators. We analyse the single top production in association with the Higgs at the Large Hadron Collider (LHC) using Standard Model (SM) effective operators upto dimension six. We show that the presence of effective operators can significantly alter the existing bound on the top-Higgs Yukawa coupling. We analyse events at the LHC with 35.9 and 137(140) fb$^{-1}$ integrated luminosities using both cut-based and machine learning techniques to probe new physics (NP) scale and operator coefficients addressing relevant SM background reduction. The four fermi effective operator(s) that contributes to the signal, turns out to be crucial and a bound on the operator coefficient is obtained from the present data and for future sensitivities.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<yes>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Anomaly-aware summary statistic from data batches. Signal-agnostic data exploration based on machine learning could unveil very subtle statistical deviations of collider data from the expected Standard Model of particle physics. The beneficial impact of a large training sample on machine learning solutions motivates the exploration of increasingly large and inclusive samples of acquired data with resource efficient computational methods. In this work we consider the New Physics Learning Machine (NPLM), a multivariate goodness-of-fit test built on the Neyman-Pearson maximum-likelihood-ratio construction, and we address the problem of testing large size samples under computational and storage resource constraints. We propose to perform parallel NPLM routines over batches of the data, and to combine them by locally aggregating over the data-to-reference density ratios learnt by each batch. The resulting data hypothesis defining the likelihood-ratio test is thus shared over the batches, and complies with the assumption that the expected rate of new physical processes is time invariant. We show that this method outperforms the simple sum of the independent tests run over the batches, and can recover, or even surpass, the sensitivity of the single test run over the full data. Beside the significant advantage for the offline application of NPLM to large size samples, the proposed approach offers new prospects toward the use of NPLM to construct anomaly-aware summary statistics in quasi-online data streaming scenarios.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<yes>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Accelerating Deep Neural Networks for Real-time Data Selection for High-resolution Imaging Particle Detectors. This paper presents the custom implementation, optimization, and performance evaluation of convolutional neural networks on field programmable gate arrays, for the purposes of accelerating deep neural network inference on large, two-dimensional image inputs. The targeted application is that of data selection for high-resolution particle imaging detectors, and in particular liquid argon time projection chamber detectors, such as that employed by the future Deep Underground Neutrino Experiment. We motivate this particular application based on the excellent performance of deep neural networks on classifying simulated raw data from the DUNE LArTPC, combined with the need for power-efficient data processing in the case of remote, long-term, and limited-access operating detector conditions.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<yes>{Software}<no>{Hardware/firmware}<yes>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Transforming Calabi-Yau Constructions: Generating New Calabi-Yau Manifolds with Transformers. Fine, regular, and star triangulations (FRSTs) of four-dimensional reflexive polytopes give rise to toric varieties, within which generic anticanonical hypersurfaces yield smooth Calabi-Yau threefolds. We employ transformers -- deep learning models originally developed for language modeling -- to generate FRSTs across a range of polytope sizes. Our models exhibit efficient and unbiased sampling, and can self-improve through retraining on their own output. These results lay the foundation for AICY: a community-driven platform that combines self-improving machine learning models with a continuously expanding FRST database to explore and catalog the Calabi-Yau landscape.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<yes>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<yes>{Theory and physics for ML}<no>{ML for theory}<yes>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Particle Graph Autoencoders and Differentiable, Learned Energy Mover's Distance. Autoencoders have useful applications in high energy physics in anomaly detection, particularly for jets - collimated showers of particles produced in collisions such as those at the CERN Large Hadron Collider. We explore the use of graph-based autoencoders, which operate on jets in their \"particle cloud\" representations and can leverage the interdependencies among the particles within a jet, for such tasks. Additionally, we develop a differentiable approximation to the energy mover's distance via a graph neural network, which may subsequently be used as a reconstruction loss function for autoencoders.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<yes>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<yes>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<no>{(Variational) Autoencoders}<yes>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Explainable AI for ML jet taggers using expert variables and layerwise relevance propagation. A framework is presented to extract and understand decision-making information from a deep neural network (DNN) classifier of jet substructure tagging techniques. The general method studied is to provide expert variables that augment inputs (\"eXpert AUGmented\" variables, or XAUG variables), then apply layerwise relevance propagation (LRP) to networks both with and without XAUG variables. The XAUG variables are concatenated with the intermediate layers after network-specific operations (such as convolution or recurrence), and used in the final layers of the network. The results of comparing networks with and without the addition of XAUG variables show that XAUG variables can be used to interpret classifier behavior, increase discrimination ability when combined with low-level features, and in some cases capture the behavior of the classifier completely. The LRP technique can be used to find relevant information the network is using, and when combined with the XAUG variables, can be used to rank features, allowing one to find a reduced set of features that capture part of the network performance. In the studies presented, adding XAUG variables to low-level DNNs increased the efficiency of classifiers by as much as 30-40\\%. In addition to performance improvements, an approach to quantify numerical uncertainties in the training of these DNNs is presented.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<yes>{Interpretability}<yes>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "A multicategory jet image classification framework using deep neural network. Jet point cloud images are high dimensional data structures that needs to be transformed to a separable feature space for machine learning algorithms to distinguish them with simple decision boundaries. In this article, the authors focus on jet category separability by particle and jet feature extraction, resulting in more efficient training of a simple deep neural network, resulting in a computational efficient interpretable model for jet classification. The methodology is tested with three to five categories of jets from the JetNet benchmark jet tagging dataset, resulting in comparable performance to particle flow network. This work demonstrates that high dimensional datasets represented in separable latent spaces lead to simpler architectures for jet classification.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<yes>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "TASI Lectures on Physics for Machine Learning. These notes are based on lectures I gave at TASI 2024 on Physics for Machine Learning. The focus is on neural network theory, organized according to network expressivity, statistics, and dynamics. I present classic results such as the universal approximation theorem and neural network / Gaussian process correspondence, and also more recent results such as the neural tangent kernel, feature learning with the maximal update parameterization, and Kolmogorov-Arnold networks. The exposition on neural network theory emphasizes a field theoretic perspective familiar to theoretical physicists. I elaborate on connections between the two, including a neural network approach to field theory.",
    "output": "{Reviews}<yes>{Modern reviews}<no>{Specialized reviews}<yes>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Random Matrix Theory for Stochastic Gradient Descent. Investigating the dynamics of learning in machine learning algorithms is of paramount importance for understanding how and why an approach may be successful. The tools of physics and statistics provide a robust setting for such investigations. Here we apply concepts from random matrix theory to describe stochastic weight matrix dynamics, using the framework of Dyson Brownian motion. We derive the linear scaling rule between the learning rate (step size) and the batch size, and identify universal and non-universal aspects of weight matrix dynamics. We test our findings in the (near-)solvable case of the Gaussian Restricted Boltzmann Machine and in a linear one-hidden-layer neural network.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<yes>{Theory and physics for ML}<yes>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "GNN for Deep Full Event Interpretation and hierarchical reconstruction of heavy-hadron decays in proton-proton collisions. The LHCb experiment at the Large Hadron Collider (LHC) is designed to perform high-precision measurements of heavy-hadron decays, which requires the collection of large data samples and a good understanding and suppression of multiple background sources. Both factors are challenged by a five-fold increase in the average number of proton-proton collisions per bunch crossing, corresponding to a change in the detector operation conditions for the LHCb Upgrade I phase, recently started. A further ten-fold increase is expected in the Upgrade II phase, planed for the next decade. The limits in the storage capacity of the trigger will bring an inverse relation between the amount of particles selected to be stored per event and the number of events that can be recorded, and the background levels will raise due to the enlarged combinatorics. To tackle both challenges, we propose a novel approach, never attempted before in a hadronic collider: a Deep-learning based Full Event Interpretation (DFEI), to perform the simultaneous identification, isolation and hierarchical reconstruction of all the heavy-hadron decay chains per event. This approach radically contrasts with the standard selection procedure used in LHCb to identify heavy-hadron decays, that looks individually at sub-sets of particles compatible with being products of specific decay types, disregarding the contextual information from the rest of the event. We present the first prototype for the DFEI algorithm, that leverages the power of Graph Neural Networks (GNN). This paper describes the design and development of the algorithm, and its performance in Upgrade I simulated conditions.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<yes>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<yes>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Deep machine learning for the PANDA software trigger. Deep machine learning methods have been studied for the software trigger of the future PANDA experiment at FAIR, using Monte Carlo simulated data from the GEANT-based detector simulation framework PandaRoot. Ten physics channels that cover the main physics topics, including electromagnetic, exotic, charmonium, open charm, and baryonic reaction channels, have been investigated at four different anti-proton beam momenta. Binary and multi-class classification together with seven different network architectures have been studied. Finally a residual convolutional neural network with four residual blocks in a binary classification scheme has been chosen due to its extendability, performance and stability. The presented study represents a feasibility study of a completely software-based trigger system. Compared to a conventional selection method, the deep machine learning approach achieved a significant efficiency gain of up to 200\\%, while keeping the background reduction factor at the required level of 1/1000. Furthermore, it is shown that the use of additional input variables can improve the data quality for subsequent analysis. This study shows that the PANDA software trigger can benefit greatly from the deep machine learning methods.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<yes>{Software}<yes>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<yes>{Performance studies}<yes>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Ring-based ML calibration with in situ pileup correction for real-time jet triggers. We present a machine learning (ML) method to calibrate hadronic jet energy in real-time trigger systems of the High-Luminosity Large Hadron Collider (HL-LHC) using an efficient implementation on field programmable gate arrays (FPGA). Regression is done to estimate the transverse energy of jet candidates, using concentric rings of electromagnetic and hadronic contributions in 0.1 x 0.1 towers around fixed-radius cone jet seeds, that accounts for in situ pileup correction. Classification separates hard-scatter jets from those due to pileup using the same inputs; its output provides a correction for the regression estimate. The algorithm is tested on simulated samples using an ATLAS-inspired detector in the dense environment of 200 simultaneous proton-proton collisions per bunch crossing. Our method improves the signal efficiency of saving Higgs pair production in HH -> bbbb by a factor of two over the traditional cone jet algorithm in real-time trigger systems.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<yes>{Software}<no>{Hardware/firmware}<yes>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Anomaly Detection with Density Estimation. We leverage recent breakthroughs in neural density estimation to propose a new unsupervised anomaly detection technique (ANODE). By estimating the probability density of the data in a signal region and in sidebands, and interpolating the latter into the signal region, a likelihood ratio of data vs. background can be constructed. This likelihood ratio is broadly sensitive to overdensities in the data that could be due to localized anomalies. In addition, a unique potential benefit of the ANODE method is that the background can be directly estimated using the learned densities. Finally, ANODE is robust against systematic differences between signal region and sidebands, giving it broader applicability than other methods. We demonstrate the power of this new approach using the LHC Olympics 2020 R\\&D Dataset. We show how ANODE can enhance the significance of a dijet bump hunt by up to a factor of 7 with a 10\\% accuracy on the background prediction. While the LHC is used as the recurring example, the methods developed here have a much broader applicability to anomaly detection in physics and beyond.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<yes>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<yes>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Mass Agnostic Jet Taggers. Searching for new physics in large data sets needs a balance between two competing effects---signal identification vs background distortion. In this work, we perform a systematic study of both single variable and multivariate jet tagging methods that aim for this balance. The methods preserve the shape of the background distribution by either augmenting the training procedure or the data itself. Multiple quantitative metrics to compare the methods are considered, for tagging 2-, 3-, or 4-prong jets from the QCD background. This is the first study to show that the data augmentation techniques of Planing and PCA based scaling deliver similar performance as the augmented training techniques of Adversarial NN and uBoost, but are both easier to implement and computationally cheaper.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<yes>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Aspen Open Jets: Unlocking LHC Data for Foundation Models in Particle Physics. Foundation models are deep learning models pre-trained on large amounts of data which are capable of generalizing to multiple datasets and/or downstream tasks. This work demonstrates how data collected by the CMS experiment at the Large Hadron Collider can be useful in pre-training foundation models for HEP. Specifically, we introduce the AspenOpenJets dataset, consisting of approximately 180M high $p_T$ jets derived from CMS 2016 Open Data. We show how pre-training the OmniJet-$\\alpha$ foundation model on AspenOpenJets improves performance on generative tasks with significant domain shift: generating boosted top and QCD jets from the simulated JetClass dataset. In addition to demonstrating the power of pre-training of a jet-based foundation model on actual proton-proton collision data, we provide the ML-ready derived AspenOpenJets dataset for further public use.",
    "output": "{Reviews}<yes>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<yes>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<yes>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Identification of light leptons and pions in the electromagnetic calorimeter of Belle II. The paper discusses new method for electron/pion and muon/pion separation in the Belle II detector at transverse momenta below 0.7 $\\mathrm{GeV}/c$, which is essential for efficient measurements of semi-leptonic decays of $B$ mesons with tau lepton in the final state. The method is based on the analysis of patterns in the electromagnetic calorimeter by using a Convolutional Neural Network (CNN).",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<yes>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Introduction and analysis of a method for the investigation of QCD-like tree data. The properties of decays that take place during jet formation cannot be easily deduced from the final distribution of particles in a detector. In this work, we first simulate a system of particles with well defined masses, decay channels, and decay probabilities. This presents the \"true system\" for which we want to reproduce the decay probability distributions. Assuming we only have the data that this system produces in the detector, we decided to employ an iterative method which uses a neural network as a classifier between events produced in the detector by the \"true system\" and some arbitrary \"test system\". In the end, we compare the distributions obtained with the iterative method to the \"true\" distributions.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<yes>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<yes>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Accelerating Monte Carlo event generation -- rejection sampling using neural network event-weight estimates. The generation of unit-weight events for complex scattering processes presents a severe challenge to modern Monte Carlo event generators. Even when using sophisticated phase-space sampling techniques adapted to the underlying transition matrix elements, the efficiency for generating unit-weight events from weighted samples can become a limiting factor in practical applications. Here we present a novel two-staged unweighting procedure that makes use of a neural-network surrogate for the full event weight. The algorithm can significantly accelerate the unweighting process, while it still guarantees unbiased sampling from the correct target distribution. We apply, validate and benchmark the new approach in high-multiplicity LHC production processes, including $Z/W$+4 jets and $t\\bar{t}$+3 jets, where we find speed-up factors up to ten.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<yes>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<yes>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<yes>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Extract the energy scale of anomalous $\\gamma\\gamma \\to W^+W^-$ scattering in the vector boson scattering process using artificial neural networks. As a model independent approach to search for the signals of new physics~(NP) beyond the Standard Model~(SM), the SM effective field theory~(SMEFT) draws a lot of attention recently. The energy scale of a process is an important parameter in the study of an EFT such as the SMEFT. However, for the processes at a hadron collider with neutrinos in the final states, the energy scales are difficult to reconstruct. In this paper, we study the energy scale of anomalous $\\gamma\\gamma \\to W^+W^-$ scattering in the vector boson scattering~(VBS) process $pp\\to j j \\ell^+\\ell^-\\nu\\bar{\\nu}$ at the large hadron collider~(LHC) using artificial neural networks~(ANNs). We find that the ANN is a powerful tool to reconstruct the energy scale of $\\gamma\\gamma \\to W^+W^-$ scattering. The factors affecting the effects of ANNs are also studied. In addition, we make an attempt to interpret the ANN and arrive at an approximate formula which has only five fitting parameters and works much better than the approximation derived from kinematic analysis. With the help of ANN approach, the unitarity bound is applied as a cut on the energy scale of $\\gamma\\gamma \\to W^+W^-$ scattering, which is found to has a significant suppressive effect on signal events. The sensitivity of the process $pp\\to j j \\ell^+\\ell^-\\nu\\bar{\\nu}$ to anomalous $\\gamma\\gamma WW$ couplings and the expected constraints on the coefficients at current and possible future LHC are also studied.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<yes>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "FM4NPP: A Scaling Foundation Model for Nuclear and Particle Physics. Large language models have revolutionized artificial intelligence by enabling large, generalizable models trained through self-supervision. This paradigm has inspired the development of scientific foundation models (FMs). However, applying this capability to experimental particle physics is challenging due to the sparse, spatially distributed nature of detector data, which differs dramatically from natural language. This work addresses if an FM for particle physics can scale and generalize across diverse tasks. We introduce a new dataset with more than 11 million particle collision events and a suite of downstream tasks and labeled data for evaluation. We propose a novel self-supervised training method for detector data and demonstrate its neural scalability with models that feature up to 188 million parameters. With frozen weights and task-specific adapters, this FM consistently outperforms baseline models across all downstream tasks. The performance also exhibits robust data-efficient adaptation. Further analysis reveals that the representations extracted by the FM are task-agnostic but can be specialized via a single linear mapping for different downstream tasks.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<yes>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Exploring Parameter Spaces with Artificial Intelligence and Machine Learning Black-Box Optimisation Algorithms. Constraining Beyond the Standard Model theories usually involves scanning highly multi-dimensional parameter spaces and check observable predictions against experimental bounds and theoretical constraints. Such task is often timely and computationally expensive, especially when the model is severely constrained and thus leading to very low random sampling efficiency. In this work we tackled this challenge using Artificial Intelligence and Machine Learning search algorithms used for Black-Box optimisation problems. Using the cMSSM and the pMSSM parameter spaces, we consider both the Higgs mass and the Dark Matter Relic Density constraints to study their sampling efficiency and parameter space coverage. We find our methodology to produce orders of magnitude improvement of sampling efficiency whilst reasonably covering the parameter space.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<yes>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<yes>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Search for the radiative leptonic decay $D^+\\to\\gamma e^+\\nu_e$ with Deep Learning. Using 20.3$~\\rm fb^{-1}$ of $e^+e^-$ annihilation data collected at a center-of-mass energy of 3.773$~\\rm GeV$ with the BESIII detector, we report an improved search for the radiative leptonic decay $D^+\\to\\gamma e^+\\nu_e$. An upper limit on its partial branching fraction for photon energies $E_\\gamma>10~\\rm MeV$ is determined to be $1.2\\times10^{-5}$ at 90\\% confidence level, which excludes most current theoretical predictions. A sophisticated deep learning approach with thorough validation, based on the Transformer architecture, is implemented to efficiently distinguish the signal from massive backgrounds.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<yes>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<yes>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Accelerating multijet-merged event generation with neural network matrix element surrogates. The efficient simulation of multijet final states presents a serious computational task for analyses of LHC data and will be even more so at the HL-LHC. We here discuss means to accelerate the generation of unweighted events based on a two-stage rejection-sampling algorithm that employs neural-network surrogates for unweighting the hard-process matrix elements. To this end, we generalise the previously proposed algorithm based on factorisation-aware neural networks to the case of multijet merging at tree-level accuracy. We thereby account for several non-trivial aspects of realistic event-simulation setups, including biased phase-space sampling, partial unweighting, and the mapping of partonic subprocesses. We apply our methods to the production of Z+jets final states at the HL-LHC using the Sherpa event generator, including matrix elements with up to six final-state partons. When using neural-network surrogates for the dominant Z+5 jets and Z+6 jets partonic processes, we find a reduction in the total event-generation time by more than a factor of 10 compared to baseline Sherpa.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<yes>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<yes>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Seeking Truth and Beauty in Flavor Physics with Machine Learning. The discovery process of building new theoretical physics models involves the dual aspect of both fitting to the existing experimental data and satisfying abstract theorists' criteria like beauty, naturalness, etc. We design loss functions for performing both of those tasks with machine learning techniques. We use the Yukawa quark sector as a toy example to demonstrate that the optimization of these loss functions results in true and beautiful models.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<yes>{Theory and physics for ML}<no>{ML for theory}<yes>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Generalized Parton Distributions from Symbolic Regression. AI/ML informed Symbolic Regression is the next stage of scientific modeling. We utilize a highly customizable symbolic regression package ``PySR\" to model the $x$ and $t$ dependence of the flavor isovector combination $H_{u-d}(x,t,\\xi)$ at $\\xi=0$. These PySR models were trained on GPD results provided by both Lattice QCD and phenomenological sources GGL, GK, and VGG. We demonstrate, for the first time, the consistency and systematic convergence of Symbolic Regression by quantifying the disparate models through their Taylor expansion coefficients. In addition to PySR penalizing models with higher complexity and mean-squared error, we implement schemes that test specific physics hypotheses, including force-factorized $x$ and $t$ dependence and Regge behavior in PySR GPDs. We show that PySR can identify factorizing GPD sources based on their response to the Force-Factorized model. Knowing the precise behavior of the GPDs, and their uncertainties in a wide range in $x$ and $t$, crucially impacts our ability to concretely and quantitatively predict hadronic spatial distributions and their derived quantities.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<yes>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<yes>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<yes>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Physics and Computing Performance of the Exa.TrkX TrackML Pipeline. The Exa.TrkX project has applied geometric learning concepts such as metric learning and graph neural networks to HEP particle tracking. Exa.TrkX's tracking pipeline groups detector measurements to form track candidates and filters them. The pipeline, originally developed using the TrackML dataset (a simulation of an LHC-inspired tracking detector), has been demonstrated on other detectors, including DUNE Liquid Argon TPC and CMS High-Granularity Calorimeter. This paper documents new developments needed to study the physics and computing performance of the Exa.TrkX pipeline on the full TrackML dataset, a first step towards validating the pipeline using ATLAS and CMS data. The pipeline achieves tracking efficiency and purity similar to production tracking algorithms. Crucially for future HEP applications, the pipeline benefits significantly from GPU acceleration, and its computational requirements scale close to linearly with the number of particles in the event.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<yes>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Guided Graph Compression for Quantum Graph Neural Networks. Graph Neural Networks (GNNs) are effective for processing graph-structured data but face challenges with large graphs due to high memory requirements and inefficient sparse matrix operations on GPUs. Quantum Computing (QC) offers a promising avenue to address these issues and inspires new algorithmic approaches. In particular, Quantum Graph Neural Networks (QGNNs) have been explored in recent literature. However, current quantum hardware limits the dimension of the data that can be effectively encoded. Existing approaches either simplify datasets manually or use artificial graph datasets. This work introduces the Guided Graph Compression (GGC) framework, which uses a graph autoencoder to reduce both the number of nodes and the dimensionality of node features. The compression is guided to enhance the performance of a downstream classification task, which can be applied either with a quantum or a classical classifier. The framework is evaluated on the Jet Tagging task, a classification problem of fundamental importance in high energy physics that involves distinguishing particle jets initiated by quarks from those by gluons. The GGC is compared against using the autoencoder as a standalone preprocessing step and against a baseline classical GNN classifier. Our numerical results demonstrate that GGC outperforms both alternatives, while also facilitating the testing of novel QGNN ansatzes on realistic datasets.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<yes>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<yes>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<yes>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<yes>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Equivariant Energy Flow Networks for Jet Tagging. Jet tagging techniques that make use of deep learning show great potential for improving physics analyses at colliders. One such method is the Energy Flow Network (EFN) - a recently introduced neural network architecture that represents jets as permutation-invariant sets of particle momenta while maintaining infrared and collinear safety. We develop a variant of the Energy Flow Network architecture based on the Deep Sets formalism, incorporating permutation-equivariant layers. We derive conditions under which infrared and collinear safety can be maintained, and study the performance of these networks on the canonical example of W-boson tagging. We find that equivariant Energy Flow Networks have similar performance to Particle Flow Networks, which are superior to standard EFNs. However, equivariant Particle Flow Networks suffer from convergence and overfitting issues. Finally, we study how equivariant networks sculpt the jet mass and provide some initial results on decorrelation using planing.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<yes>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<yes>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<yes>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Variational Autoencoders for Jet Simulation. We introduce a novel variational autoencoder (VAE) architecture that can generate realistic and diverse high energy physics events. The model we propose utilizes several techniques from VAE literature in order to simulate high fidelity jet images. In addition to demonstrating the model's ability to produce high fidelity jet images through various assessments, we also demonstrate its ability to control the events it generates from the latent space. This can be potentially useful for other tasks such as jet tagging, where we can test how well jet taggers can classify signal from background for events generated by the VAE. We test this idea by seeing the signal efficiency vs background rejection for different types of jet images produced by our model. We compare our VAE with generative adversarial networks (GAN) in several ways, most notably in speed. The architecture we propose is ultimately a fast, stable, and easy-to-train deep generative model that demonstrates the potential of VAEs in simulating high energy physics events.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<no>{(Variational) Autoencoders}<yes>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Inclusive search for highly boosted Higgs bosons decaying to bottom quark-antiquark pairs in proton-proton collisions at $\\sqrt{s}. A search for standard model Higgs bosons (H) produced with transverse momentum ($p_\\mathrm{T}$) greater than 450 GeV and decaying to bottom quark-antiquark pairs ($\\mathrm{b\\bar{b}}$) is performed using proton-proton collision data collected by the CMS experiment at the LHC at $\\sqrt{s} =$ 13 TeV. The data sample corresponds to an integrated luminosity of 137 fb$^{-1}$. The search is inclusive in the Higgs boson production mode. Highly Lorentz-boosted Higgs bosons decaying to $\\mathrm{b\\bar{b}}$ are reconstructed as single large-radius jets, and are identified using jet substructure and a dedicated b tagging technique based on a deep neural network. The method is validated with Z $\\to$ $\\mathrm{b\\bar{b}}$ decays. For a Higgs boson mass of 125 GeV, an excess of events above the background assuming no Higgs boson production is observed with a local significance of 2.5 standard deviations ($\\sigma$), while the expectation is 0.7. The corresponding signal strength and local significance with respect to the standard model expectation are $\\mu_\\mathrm{H}$ $=$ 3.7 $\\pm$ 1.2 (stat) $^{+0.8}_{-0.7}$ (syst) $^{+0.8}_{-0.5}$ (theo) and 1.9 $\\sigma$. Additionally, an unfolded differential cross section as a function of Higgs boson $p_\\mathrm{T}$ for the gluon fusion production mode is presented, assuming the other production modes occur at the expected rates.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<yes>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<yes>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Inclusive, prompt and non-prompt $\\rm{J}/\\psi$ identification in proton-proton collisions at the Large Hadron Collider using machine learning. Studies related to $\\rm{J}/\\psi$ meson, a bound state of charm and anti-charm quarks ($c\\bar{c}$), in heavy-ion collisions, provide genuine testing grounds for the theory of strong interaction, quantum chromodynamics (QCD). To better understand the underlying production mechanism, cold nuclear matter effects, and influence from the quark-gluon plasma, baseline measurements are also performed in proton-proton ($pp$) and proton-nucleus ($p$--A) collisions. The inclusive $\\rm{J}/\\psi$ measurement has contributions from both prompt and non-prompt productions. The prompt $\\rm{J}/\\psi$ is produced directly from the hadronic interactions or via feed-down from directly produced higher charmonium states, whereas non-prompt $\\rm{J}/\\psi$ comes from the decay of beauty hadrons. In experiments, $\\rm{J}/\\psi$ is reconstructed through its electromagnetic decays to lepton pairs, in either $e^{+}+e^{-}$ or $\\mu^{+}+\\mu^{-}$ decay channels. In this work, for the first time, machine learning techniques are implemented to separate the prompt and non-prompt dimuon pairs from the background to obtain a better identification of the $\\rm{J}/\\psi$ signal for different production modes. The study has been performed in $pp$ collisions at $\\sqrt{s} = 7$ and 13 TeV simulated using PYTHIA8. Machine learning models such as XGBoost and LightGBM are explored. The models could achieve up to 99\\% prediction accuracy. The transverse momentum ($p_{\\rm T}$) and rapidity ($y$) differential measurements of inclusive, prompt, and non-prompt $\\rm{J}/\\psi$, its multiplicity dependence, and the $p_{\\rm T}$ dependence of fraction of non-prompt $\\rm{J}/\\psi$ ($f_{\\rm B}$) are shown. These results are compared to experimental findings wherever possible.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<yes>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Machine Learning in High Energy Physics: A review of heavy-flavor jet tagging at the LHC. The application of machine learning (ML) in high energy physics (HEP), specifically in heavy-flavor jet tagging at Large Hadron Collider (LHC) experiments, has experienced remarkable growth and innovation in the past decade. This review provides a detailed examination of current and past ML techniques in this domain. It starts by exploring various data representation methods and ML architectures, encompassing traditional ML algorithms and advanced deep learning techniques. Subsequent sections discuss specific instances of successful ML applications in jet flavor tagging in the ATLAS and CMS experiments at the LHC, ranging from basic fully-connected layers to graph neural networks employing attention mechanisms. To systematically categorize the advancements over the LHC's three runs, the paper classifies jet tagging algorithms into three generations, each characterized by specific data representation techniques and ML architectures. This classification aims to provide an overview of the chronological evolution in this field. Finally, a brief discussion about anticipated future developments and potential research directions in the field is presented.",
    "output": "{Reviews}<yes>{Modern reviews}<no>{Specialized reviews}<yes>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "OmniJet-$\\alpha$: The first cross-task foundation model for particle physics. Foundation models are multi-dataset and multi-task machine learning methods that once pre-trained can be fine-tuned for a large variety of downstream applications. The successful development of such general-purpose models for physics data would be a major breakthrough as they could improve the achievable physics performance while at the same time drastically reduce the required amount of training time and data. We report significant progress on this challenge on several fronts. First, a comprehensive set of evaluation methods is introduced to judge the quality of an encoding from physics data into a representation suitable for the autoregressive generation of particle jets with transformer architectures (the common backbone of foundation models). These measures motivate the choice of a higher-fidelity tokenization compared to previous works. Finally, we demonstrate transfer learning between an unsupervised problem (jet generation) and a classic supervised task (jet tagging) with our new OmniJet-$\\alpha$ model. This is the first successful transfer between two different and actively studied classes of tasks and constitutes a major step in the building of foundation models for particle physics.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<yes>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Flow-based generative models for Markov chain Monte Carlo in lattice field theory. A Markov chain update scheme using a machine-learned flow-based generative model is proposed for Monte Carlo sampling in lattice field theories. The generative model may be optimized (trained) to produce samples from a distribution approximating the desired Boltzmann distribution determined by the lattice action of the theory being studied. Training the model systematically improves autocorrelation times in the Markov chain, even in regions of parameter space where standard Markov chain Monte Carlo algorithms exhibit critical slowing down in producing decorrelated updates. Moreover, the model may be trained without existing samples from the desired distribution. The algorithm is compared with HMC and local Metropolis sampling for $\\phi^4$ theory in two dimensions.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<yes>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Testing a 95 GeV Scalar at the CEPC with Machine Learning. Several possible excesses around $95\\,$GeV hint at an additional light scalar beyond the Standard Model. We examine the capability of the CEPC to test this hypothesis in the Higgsstrahlung channel $e^{+}e^{-}\\!\\to ZS$ with $Z\\!\\to\\mu^{+}\\mu^{-}$ and $S\\!\\to\\tau^{+}\\tau^{-}$. Full detector simulation shows that the optimal center-of-mass energy to study the 95 GeV light scalar is 210 GeV. A deep neural network classifier reduces the luminosity required for discovery by half. At $L=500$ fb$^{-1}$, the CEPC can probe signal strengths down to $\\mu_{\\tau\\tau}^{ZS} = 0.116\\ (0.150)$ for $\\sqrt{s}=210\\ (240)$ GeV. A complete $5\\sigma$ coverage of all surviving samples in the flipped N2HDM requires $L = 0.9\\ (1.4)$ ab$^{-1}$ at the corresponding energies. These results establish a 210 GeV run, augmented by machine-learning selection, as the most efficient strategy to confirm or refute the 95 GeV excess at future lepton colliders.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<yes>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Generative Unfolding with Distribution Mapping. Machine learning enables unbinned, highly-differential cross section measurements. A recent idea uses generative models to morph a starting simulation into the unfolded data. We show how to extend two morphing techniques, Schr\\\"odinger Bridges and Direct Diffusion, in order to ensure that the models learn the correct conditional probabilities. This brings distribution mapping to a similar level of accuracy as the state-of-the-art conditional generative unfolding methods. Numerical results are presented with a standard benchmark dataset of single jet substructure as well as for a new dataset describing a 22-dimensional phase space of Z + 2-jets.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<yes>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<yes>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "High-dimensional and Permutation Invariant Anomaly Detection. Methods for anomaly detection of new physics processes are often limited to low-dimensional spaces due to the difficulty of learning high-dimensional probability densities. Particularly at the constituent level, incorporating desirable properties such as permutation invariance and variable-length inputs becomes difficult within popular density estimation methods. In this work, we introduce a permutation-invariant density estimator for particle physics data based on diffusion models, specifically designed to handle variable-length inputs. We demonstrate the efficacy of our methodology by utilizing the learned density as a permutation-invariant anomaly detection score, effectively identifying jets with low likelihood under the background-only hypothesis. To validate our density estimation method, we investigate the ratio of learned densities and compare to those obtained by a supervised classification algorithm.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<yes>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<yes>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Leveraging on-shell interference to search for FCNCs of the top quark and the Z boson. Flavour-changing-neutral currents (FCNCs) involving the top quark are highly suppressed within the Standard Model (SM). Hence, any signal in current or planned future collider experiments would constitute a clear manifestation of physics beyond the SM. We propose a novel, interference-based strategy to search for top-quark FCNCs involving the $Z$ boson that has the potential to complement traditional search strategies due to a more favourable luminosity scaling. The strategy leverages on-shell interference between the FCNC and SM decay of the top quark into hadronic final states. We estimate the feasibility of the most promising case of anomalous $tZc$ couplings using Monte Carlo simulations and a simplified detector simulation. We consider the main background processes and discriminate the signal from the background with a deep neural network that is parametrised in the value of the anomalous $tZc$ coupling. We present sensitivity projections for the HL-LHC and the FCC-hh. We find an expected $95\\%$ CL upper limit of $\\mathcal{B}_{\\mathrm{excl}}(t\\rightarrow Zc) = 6.4 \\times 10^{-5}$ for the HL-LHC. In general, we conclude that the interference-based approach has the potential to provide both competitive and complementary constraints to traditional multi-lepton searches and other strategies that have been proposed to search for $tZc$ FCNCs.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<yes>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Unifying Simulation and Inference with Normalizing Flows. There have been many applications of deep neural networks to detector calibrations and a growing number of studies that propose deep generative models as automated fast detector simulators. We show that these two tasks can be unified by using maximum likelihood estimation (MLE) from conditional generative models for energy regression. Unlike direct regression techniques, the MLE approach is prior-independent and non-Gaussian resolutions can be determined from the shape of the likelihood near the maximum. Using an ATLAS-like calorimeter simulation, we demonstrate this concept in the context of calorimeter energy calibration.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<yes>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Flavor dependent Critical endpoint from holographic QCD through machine learning. QCD phase diagram in the $T - \\mu$ plane and the equation of state for pure gluon, 2-flavor, 2+1-flavor systems, and 2+1+1-flavor systems have been investigated using the Einstein-Maxwell-Dilaton (EMD) framework at finite temperature and chemical potential. By inputting lattice QCD data for the equation of state and baryon number susceptibility at zero chemical potential into holographic model, all the parameters can be determined with the aid of machine learning algorithms. Our findings indicate that the deconfinement phase transition is of first order for the pure gluon system with critical temperature $T_c = 0.265$ GeV at vanishing chemical potential. The phase transition for the 2-flavor, 2+1-flavor systems, and 2+1+1-flavor systems are crossover at vanishing chemical potential and first-order at high chemical potential, and the critical endpoint (CEP) in the $T - \\mu$ plane locates at ($\\mu_B^c$=0.46 GeV, $T^c$=0.147 GeV), ($\\mu_B^c$ = 0.74 GeV, $T^c$ = 0.094 GeV), and ($\\mu_B^c$= 0.87 GeV,$T^c$ = 0.108 GeV), respectively. Additionally, the thermodynamic quantities of the system for different flavors at finite chemical potential are presented in this paper. It is observed that the difference between the 2+1-flavor and 2+1+1-flavor systems is invisible at vanishing chemical potential and low temperature. The location of CEP for 2+1+1-flavor system deviates explicitly from that of the 2+1-flavor system with the increase of chemical potential. Both 2+1-flavor and 2+1+1-flavor systems differ significantly from the 2-flavor system. Moreover, at zero temperature, the critical chemical potential is found to be $\\mu_B$ = 1.1 GeV, 1.6 GeV, 1.9 GeV for the 2-flavor, 2+1-flavor and 2+1+1-flavor systems, respectively.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<yes>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<yes>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Anomaly detection from mass unspecific jet tagging. We introduce a novel anomaly search method based on (i) jet tagging to select interesting events, which are less likely to be produced by background processes; (ii) comparison of the untagged and tagged samples to single out features (such as bumps produced by the decay of new particles) in the latter. We demonstrate the usefulness of this method by applying it to a final state with two massive boosted jets: for the new physics benchmarks considered, the signal significance increases an order of magnitude, up to a factor of 40. We compare to other anomaly detection methods in the literature and discuss possible generalisations.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<yes>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Probing Light Scalars and Vector-like Quarks at the High-Luminosity LHC. A model based on a $U(1)_{T^3_R}$ extension of the Standard Model can address the mass hierarchy between generations of fermions, explain thermal dark matter abundance, and the muon $g - 2$, $R_{(D)}$, and $R_{(D^*)}$ anomalies. The model contains a light scalar boson $\\phi'$ and a heavy vector-like quark $\\chi_\\mathrm{u}$ that can be probed at CERN's Large Hadron Collider (LHC). We perform a phenomenology study on the production of $\\phi'$ and ${\\chi}_u$ particles from proton-proton $(\\mathrm{pp})$ collisions at the LHC at $\\sqrt{s}=13.6$ TeV, primarily through $g{-g}$ and $t{-\\chi_\\mathrm{u}}$ fusion. We work under an effective field theory approach, in which the $\\chi_\\mathrm{u}$ and $\\phi'$ masses are free parameters. We perform a phenomenological analysis considering $\\chi_\\mathrm{u}$ final states to b-quarks, muons, and neutrinos, and $\\phi'$ decays to $\\mu^+\\mu^-$. A machine learning algorithm is used to maximize the signal sensitivity, considering an integrated luminosity of $3000$ $\\textrm{fb}^{-1}$. The proposed methodology can be a key mode for discovery over a large mass range, including low masses, traditionally considered difficult due to experimental constraints.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<yes>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<yes>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Deep Learning and Model Independence. The lack of evidence in favor of any new physics models means that the search for new physics beyond the Standard Model (BSM) is wide open, with no direction clearly more promising than any other. This marks a turn towards what can be called `model-independent' methods-strategies that reduce the influence of modelling assumptions by performing minimally-biased precision measurements, using effective field theories, or using Deep Learning methods (DL). In this paper, I present the novel and promising uses of DL as a primary tool in high energy physics research, highlighting the use of autoencoder networks and unsupervised learning methods. I advocate for the importance and usefulness of the concept of model independence and propose a definition that recognizes that independence of models is not absolute, but comes in degrees.",
    "output": "{Reviews}<yes>{Modern reviews}<no>{Specialized reviews}<yes>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Inverting cosmic ray propagation by Convolutional Neural Networks. We propose a machine learning method to investigate the propagation of cosmic rays based on the precisely measured spectra of the primary and secondary cosmic ray nuclei of Li, Be, B, C, and O from AMS-02, ACE, and Voyager-1. We train two convolutional neural networks. One network learns how to infer propagation and source parameters from the energy spectra of cosmic rays, and the other network, which is similar to the former, has the flexibility to learn from the data with added artificial fluctuations. Together with the simulated data generated by GALPROP, we find that both networks can properly invert the propagation process and infer the propagation and source parameters reasonably well. This approach can be much more efficient than the traditional Markov chain Monte Carlo fitting method for deriving the propagation parameters if users choose to update confidence intervals with new experimental data. Both of the trained networks are available at (https://github.com/alan200276/CR_ML).",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<yes>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Machine-learning-based particle identification with missing data. In this work, we introduce a novel method for Particle Identification (PID) within the scope of the ALICE experiment at the Large Hadron Collider at CERN. Identifying products of ultrarelativisitc collisions delivered by the LHC is one of the crucial objectives of ALICE. Typically employed PID methods rely on hand-crafted selections, which compare experimental data to theoretical simulations. To improve the performance of the baseline methods, novel approaches use machine learning models that learn the proper assignment in a classification task. However, because of the various detection techniques used by different subdetectors, as well as the limited detector efficiency and acceptance, produced particles do not always yield signals in all of the ALICE components. This results in data with missing values. Machine learning techniques cannot be trained with such examples, so a significant part of the data is skipped during training. In this work, we propose the first method for PID that can be trained with all of the available data examples, including incomplete ones. Our approach improves the PID purity and efficiency of the selected sample for all investigated particle species.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<yes>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "A Deep Learning Approach to Extracting Nuclear Matter Properties from Neutron Star Observations. Understanding the equation of state of dense QCD matter remains a major challenge in both nuclear physics and astrophysics. Neutron star observations from electromagnetic and gravitational wave spectra provide critical insights into the behavior of dense neutron-rich matter. The next generation of telescopes and gravitational wave observatories will offer even more detailed observations of neutron stars. Utilizing deep learning techniques to map neutron star mass and radius observations to the equation of state allows for its accurate and reliable determination. This work demonstrates the feasibility of using deep learning to extract the equation of state directly from neutron star observational data, and to also obtain related nuclear matter properties such as the slope, curvature, and skewness of the nuclear symmetry energy at saturation density. Most importantly, we show that this deep learning approach is able to reconstruct \\textit{realistic} equations of state, and deduce \\textit{realistic} nuclear matter properties. This highlights the potential of artificial neural networks in providing a reliable and efficient means to extract crucial information about the equation of state and related properties of dense neutron-rich matter in the era of multi-messenger astrophysics.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<yes>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Principles for Initialization and Architecture Selection in Graph Neural Networks with ReLU Activations. This article derives and validates three principles for initialization and architecture selection in finite width graph neural networks (GNNs) with ReLU activations. First, we theoretically derive what is essentially the unique generalization to ReLU GNNs of the well-known He-initialization. Our initialization scheme guarantees that the average scale of network outputs and gradients remains order one at initialization. Second, we prove in finite width vanilla ReLU GNNs that oversmoothing is unavoidable at large depth when using fixed aggregation operator, regardless of initialization. We then prove that using residual aggregation operators, obtained by interpolating a fixed aggregation operator with the identity, provably alleviates oversmoothing at initialization. Finally, we show that the common practice of using residual connections with a fixup-type initialization provably avoids correlation collapse in final layer features at initialization. Through ablation studies we find that using the correct initialization, residual aggregation operators, and residual connections in the forward pass significantly and reliably speeds up early training dynamics in deep ReLU GNNs on a variety of tasks.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<yes>{Hyperparameters}<yes>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Quantum anomaly detection in the latent space of proton collision events at the LHC. The ongoing quest to discover new phenomena at the LHC necessitates the continuous development of algorithms and technologies. Established approaches like machine learning, along with emerging technologies such as quantum computing show promise in the enhancement of experimental capabilities. In this work, we propose a strategy for anomaly detection tasks at the LHC based on unsupervised quantum machine learning, and demonstrate its effectiveness in identifying new phenomena. The designed quantum models, an unsupervised kernel machine and two clustering algorithms, are trained to detect new-physics events using a latent representation of LHC data, generated by an autoencoder designed to accommodate current quantum hardware limitations on problem size. For kernel-based anomaly detection, we implement an instance of the model on a quantum computer, and we identify a regime where it significantly outperforms its classical counterparts. We show that the observed performance enhancement is related to the quantum resources utilised by the model.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<yes>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<yes>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Study of anomalous $W^-W^+\\gamma/Z$ couplings using polarizations and spin correlations in $e^-e^+\\to W^-W^+$ with polarized beams. We study the anomalous $W^-W^+\\gamma/Z$ couplings in $e^-e^+\\to W^-W^+$ followed by semileptonic decay using a complete set of polarization and spin correlation observables of $W$ boson with the longitudinally polarized beam. We consider a complete set of dimension-six operators affecting $W^-W^+\\gamma/Z$ vertex, which are $SU(2)\\times U(1)$ gauge invariant. Some of the polarization and spin correlation asymmetries average out if the daughter of $W^+$ is not tagged. We developed an artificial neural network and boosted decision trees to distinguish down-type jets from up-type jets. We obtain bounds on the anomalous couplings for center of mass energy $\\sqrt{s} = 250$ GeV with integrated luminosities of~$\\mathcal{L}\\in\\{100~\\text{fb}^{-1}, 250~\\text{fb}^{-1}, 1000~\\text{fb}^{-1}, 3000~\\text{fb}^{-1}\\}$. We find that using spin-related observables and cross~section in the presence of initial beam polarization significantly improves the bounds on anomalous couplings compared to previous studies.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<yes>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Validation of Deep Convolutional Generative Adversarial Networks for High Energy Physics Calorimeter Simulations. In particle physics the simulation of particle transport through detectors requires an enormous amount of computational resources, utilizing more than 50% of the resources of the CERN Worldwide Large Hadron Collider Grid. This challenge has motivated the investigation of different, faster approaches for replacing the standard Monte Carlo simulations. Deep Learning Generative Adversarial Networks are among the most promising alternatives. Previous studies showed that they achieve the necessary level of accuracy while decreasing the simulation time by orders of magnitudes. In this paper we present a newly developed neural network architecture which reproduces a three-dimensional problem employing 2D convolutional layers and we compare its performance with an earlier architecture consisting of 3D convolutional layers. The performance evaluation relies on direct comparison to Monte Carlo simulations, in terms of different physics quantities usually employed to quantify the detector response. We prove that our new neural network architecture reaches a higher level of accuracy with respect to the 3D convolutional GAN while reducing the necessary computational resources. Calorimeters are among the most expensive detectors in terms of simulation time. Therefore we focus our study on an electromagnetic calorimeter prototype with a regular highly granular geometry, as an example of future calorimeters.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<yes>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Optimizers for Stabilizing Likelihood-free Inference. A growing number of applications in particle physics and beyond use neural networks as unbinned likelihood ratio estimators applied to real or simulated data. Precision requirements on the inference tasks demand a high-level of stability from these networks, which are affected by the stochastic nature of training. We show how physics concepts can be used to stabilize network training through a physics-inspired optimizer. In particular, the Energy Conserving Descent (ECD) optimization framework uses classical Hamiltonian dynamics on the space of network parameters to reduce the dependence on the initial conditions while also stabilizing the result near the minimum of the loss function. We develop a version of this optimizer known as $ECD_{q=1}$, which has few free hyperparameters with limited ranges guided by physical reasoning. We apply $ECD_{q=1}$ to representative likelihood-ratio estimation tasks in particle physics and find that it out-performs the widely-used Adam optimizer. We expect that ECD will be a useful tool for wide array of data-limited problems, where it is computationally expensive to exhaustively optimize hyperparameters and mitigate fluctuations with ensembling.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<yes>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<yes>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<yes>{Theory and physics for ML}<yes>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Machine Learning Insights into Quark-Antiquark Interactions: Probing Field Distributions and String Tension in QCD. Understanding the interactions between quark-antiquark pairs is essential for elucidating quark confinement within the framework of quantum chromodynamics (QCD). This study investigates the field distribution patterns that arise between these pairs by employing advanced machine learning techniques, namely multilayer perceptrons (MLP) and Kolmogorov-Arnold networks (KAN), to analyze data obtained from lattice QCD simulations. The models developed through this training are then applied to calculate the string tension and width associated with chromo flux tubes, and these results are rigorously compared to those derived from lattice QCD. Moreover, we introduce a preliminary analytical expression that characterizes the field distribution as a function of quark separation, utilizing the KAN methodology. Our comprehensive quantitative analysis underscores the potential of integrating machine learning approaches into conventional QCD research.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<yes>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<yes>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<yes>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "A Convolutional Neural Network for Multiple Particle Identification in the MicroBooNE Liquid Argon Time Projection Chamber. We present the multiple particle identification (MPID) network, a convolutional neural network (CNN) for multiple object classification, developed by MicroBooNE. MPID provides the probabilities of $e^-$, $\\gamma$, $\\mu^-$, $\\pi^\\pm$, and protons in a single liquid argon time projection chamber (LArTPC) readout plane. The network extends the single particle identification network previously developed by MicroBooNE. MPID takes as input an image either cropped around a reconstructed interaction vertex or containing only activity connected to a reconstructed vertex, therefore relieving the tool from inefficiencies in vertex finding and particle clustering. The network serves as an important component in MicroBooNE's deep learning based $\\nu_e$ search analysis. In this paper, we present the network's design, training, and performance on simulation and data from the MicroBooNE detector.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<yes>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Learning Broken Symmetries with Resimulation and Encouraged Invariance. Recognizing symmetries in data allows for significant boosts in neural network training. In many cases, however, the underlying symmetry is present only in an idealized dataset, and is broken in the training data, due to effects such as arbitrary and/or non-uniform detector bin edges. Standard approaches, such as data augmentation or equivariant networks fail to represent the nature of the full, broken symmetry. We introduce a novel data-augmentation scheme that respects the true underlying symmetry and avoids artifacts by augmenting the training set with transformed pre-detector examples whose detector response is then resimulated. In addition, we encourage the network to treat the augmented copies identically, allowing it to learn the broken symmetry. While the technique can be extended to other symmetries, we demonstrate its application on rotational symmetry in particle physics calorimeter images. We find that neural networks trained with pre-detector rotations converge to a solution more quickly than networks trained with standard post-detector augmentation, and that networks modified to encourage similar internal treatment of augmentations of the same input converge even faster.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<yes>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<yes>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Distinguishing $W'$ Signals at Hadron Colliders Using Neural Networks. We investigate a neural network-based hypothesis test to distinguish different $W'$ and charged scalar resonances through the $\\ell+\\require{cancel}\\cancel{E}_T$ channel at hadron colliders. This is traditionally challenging due to a four-fold ambiguity at proton-proton colliders, such as the Large Hadron Collider. Of the neural network approaches we studied, we find a multi-class classifier based on a fully-connected neural network trained upon 2D histograms made from kinematic variables of the final state $\\ell$ to be the most powerful. Furthermore, by considering the 1-jet processes, we demonstrate that one can generalize to multiple $2D$ histograms to represent different variable pairs. Finally, as a comparison to traditional approaches, we compare our method with Bayesian hypothesis testing and discuss the pros and cons of each approach. The neural network scheme presented in this paper is a powerful tool that can help probe the properties of charged resonances.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<yes>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Machine Learning the 6d Supergravity Landscape. In this paper, we apply both supervised and unsupervised machine learning algorithms to the study of the string landscape and swampland in 6-dimensions. Our data are the (almost) anomaly-free 6-dimensional $\\mathcal{N} = (1,0)$ supergravity models, characterised by the Gram matrix of anomaly coefficients. Our work demonstrates the ability of machine learning algorithms to efficiently learn highly complex features of the landscape and swampland. Employing an autoencoder for unsupervised learning, we provide an auto-classification of these models by compressing the Gram matrix data to 2-dimensions. Through compression, similar models cluster together, and we identify prominent features of these clusters. The autoencoder also identifies outlier models which are difficult to reconstruct. One of these outliers proves to be incredibly difficult to combine with other models such that the $\\text{tr}R^{4}$ anomaly vanishes, making its presence in the landscape extremely rare. Further, we utilise supervised learning to build two classifiers predicting (1) model consistency under probe string insertion (precision: 0.78, predicting consistency for 214,837 models with reasonable certainty) and (2) inconsistency under anomaly inflow (precision: 0.91, predicting inconsistency for 1,909,359 models). Notably, projecting these predictions onto the autoencoder's 2-dimensional latent layer shows consistent models clustering together, further indicating that the autoencoder has learnt interesting and complex features of the set of models and potentially offers a novel approach to mapping the landscape and swampland of 6-dimensional supergravity theories.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<yes>{Theory and physics for ML}<no>{ML for theory}<yes>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Simulation-Prior Independent Neural Unfolding Procedure. Machine learning allows unfolding high-dimensional spaces without binning at the LHC. The new SPINUP method extracts the unfolded distribution based on a neural network encoding the forward mapping, making it independent of the prior from the simulated training data. It is made efficient through neural importance sampling, and ensembling can be used to estimate the effect of information loss in the forward process. We showcase SPINUP for unfolding detector effects on jet substructure observables and for unfolding to parton level of associated Higgs and single-top production.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<yes>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<yes>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Searching for Exotic Particles in High-Energy Physics with Deep Learning. Collisions at high-energy particle colliders are a traditionally fruitful source of exotic particle discoveries. Finding these rare particles requires solving difficult signal-versus-background classification problems, hence machine learning approaches are often used. Standard approaches have relied on `shallow' machine learning models that have a limited capacity to learn complex non-linear functions of the inputs, and rely on a pain-staking search through manually constructed non-linear features. Progress on this problem has slowed, as a variety of techniques have shown equivalent performance. Recent advances in the field of deep learning make it possible to learn more complex functions and better discriminate between signal and background classes. Using benchmark datasets, we show that deep learning methods need no manually constructed inputs and yet improve the classification metric by as much as 8\\% over the best current approaches. This demonstrates that deep learning approaches can improve the power of collider searches for exotic particles.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<yes>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "An implementation of neural simulation-based inference for parameter estimation in ATLAS. Neural simulation-based inference is a powerful class of machine-learning-based methods for statistical inference that naturally handles high-dimensional parameter estimation without the need to bin data into low-dimensional summary histograms. Such methods are promising for a range of measurements, including at the Large Hadron Collider, where no single observable may be optimal to scan over the entire theoretical phase space under consideration, or where binning data into histograms could result in a loss of sensitivity. This work develops a neural simulation-based inference framework for statistical inference, using neural networks to estimate probability density ratios, which enables the application to a full-scale analysis. It incorporates a large number of systematic uncertainties, quantifies the uncertainty due to the finite number of events in training samples, develops a method to construct confidence intervals, and demonstrates a series of intermediate diagnostic checks that can be performed to validate the robustness of the method. As an example, the power and feasibility of the method are assessed on simulated data for a simplified version of an off-shell Higgs boson couplings measurement in the four-lepton final states. This approach represents an extension to the standard statistical methodology used by the experiments at the Large Hadron Collider, and can benefit many physics analyses.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<yes>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<yes>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Fast simulation of muons produced at the SHiP experiment using Generative Adversarial Networks. This paper presents a fast approach to simulating muons produced in interactions of the SPS proton beams with the target of the SHiP experiment. The SHiP experiment will be able to search for new long-lived particles produced in a 400~GeV$/c$ SPS proton beam dump and which travel distances between fifty metres and tens of kilometers. The SHiP detector needs to operate under ultra-low background conditions and requires large simulated samples of muon induced background processes. Through the use of Generative Adversarial Networks it is possible to emulate the simulation of the interaction of 400~GeV$/c$ proton beams with the SHiP target, an otherwise computationally intensive process. For the simulation requirements of the SHiP experiment, generative networks are capable of approximating the full simulation of the dense fixed target, offering a speed increase by a factor of $\\mathcal{O}(10^6)$. To evaluate the performance of such an approach, comparisons of the distributions of reconstructed muon momenta in SHiP's spectrometer between samples using the full simulation and samples produced through generative models are presented. The methods discussed in this paper can be generalised and applied to modelling any non-discrete multi-dimensional distribution.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<yes>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Normalizing Flows for High-Dimensional Detector Simulations. Whenever invertible generative networks are needed for LHC physics, normalizing flows show excellent performance. In this work, we investigate their performance for fast calorimeter shower simulations with increasing phase space dimension. We use fast and expressive coupling spline transformations applied to the CaloChallenge datasets. In addition to the base flow architecture we also employ a VAE to compress the dimensionality and train a generative network in the latent space. We evaluate our networks on several metrics, including high-level features, classifiers, and generation timing. Our findings demonstrate that invertible neural networks have competitive performance when compared to autoregressive flows, while being substantially faster during generation.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<yes>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Event Generation with Normalizing Flows. We present a novel integrator based on normalizing flows which can be used to improve the unweighting efficiency of Monte-Carlo event generators for collider physics simulations. In contrast to machine learning approaches based on surrogate models, our method generates the correct result even if the underlying neural networks are not optimally trained. We exemplify the new strategy using the example of Drell-Yan type processes at the LHC, both at leading and partially at next-to-leading order QCD.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<yes>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<yes>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Parnassus: An Automated Approach to Accurate, Precise, and Fast Detector Simulation and Reconstruction. Detector simulation and reconstruction are a significant computational bottleneck in particle physics. We develop Particle-flow Neural Assisted Simulations (Parnassus) to address this challenge. Our deep learning model takes as input a point cloud (particles impinging on a detector) and produces a point cloud (reconstructed particles). By combining detector simulations and reconstruction into one step, we aim to minimize resource utilization and enable fast surrogate models suitable for application both inside and outside large collaborations. We demonstrate this approach using a publicly available dataset of jets passed through the full simulation and reconstruction pipeline of the CMS experiment. We show that Parnassus accurately mimics the CMS particle flow algorithm on the (statistically) same events it was trained on and can generalize to jet momentum and type outside of the training distribution.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<yes>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Exploring QCD matter in extreme conditions with Machine Learning. In recent years, machine learning has emerged as a powerful computational tool and novel problem-solving perspective for physics, offering new avenues for studying strongly interacting QCD matter properties under extreme conditions. This review article aims to provide an overview of the current state of this intersection of fields, focusing on the application of machine learning to theoretical studies in high energy nuclear physics. It covers diverse aspects, including heavy ion collisions, lattice field theory, and neutron stars, and discuss how machine learning can be used to explore and facilitate the physics goals of understanding QCD matter. The review also provides a commonality overview from a methodology perspective, from data-driven perspective to physics-driven perspective. We conclude by discussing the challenges and future prospects of machine learning applications in high energy nuclear physics, also underscoring the importance of incorporating physics priors into the purely data-driven learning toolbox. This review highlights the critical role of machine learning as a valuable computational paradigm for advancing physics exploration in high energy nuclear physics.",
    "output": "{Reviews}<yes>{Modern reviews}<no>{Specialized reviews}<yes>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<yes>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<yes>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<yes>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Autoencoders for unsupervised anomaly detection in high energy physics. Autoencoders are widely used in machine learning applications, in particular for anomaly detection. Hence, they have been introduced in high energy physics as a promising tool for model-independent new physics searches. We scrutinize the usage of autoencoders for unsupervised anomaly detection based on reconstruction loss to show their capabilities, but also their limitations. As a particle physics benchmark scenario, we study the tagging of top jet images in a background of QCD jet images. Although we reproduce the positive results from the literature, we show that the standard autoencoder setup cannot be considered as a model-independent anomaly tagger by inverting the task: due to the sparsity and the specific structure of the jet images, the autoencoder fails to tag QCD jets if it is trained on top jets even in a semi-supervised setup. Since the same autoencoder architecture can be a good tagger for a specific example of an anomaly and a bad tagger for a different example, we suggest improved performance measures for the task of model-independent anomaly detection. We also improve the capability of the autoencoder to learn non-trivial features of the jet images, such that it is able to achieve both top jet tagging and the inverse task of QCD jet tagging with the same setup. However, we want to stress that a truly model-independent and powerful autoencoder-based unsupervised jet tagger still needs to be developed.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<yes>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Exploring the substructure of nucleons and nuclei with machine learning. Perturbative quantum chromodynamics (QCD) ceases to be applicable at low interaction energies due to the rapid increase of the strong coupling. In that limit, the non-perturbative regime determines the properties of quarks and gluons (partons) in terms of parton distribution functions (PDFs) or nuclear PDFs, based on whether they are confined within nucleons or nuclei respectively. Related non-perturbative dynamics describe the hadronisation of partons into hadrons and are encoded by the fragmentation functions (FFs). This thesis focuses on the detailed study of PDFs in protons and nuclei as well as the charged pions FFs by means of a statistical framework based on machine learning algorithms. The key ingredients are the Monte Carlo method for error propagation as well as artificial neural networks that act as universal unbiased interpolators. The main topics addressed are the inference of proton PDFs with theoretical uncertainties and the impact on the gluon PDF from dijet cross-sections; a global determination of nuclear PDFs exploiting the constraints from proton-lead collisions at the LHC and using for the first time NNLO calculations; a new determination of FFs from single-inclusive annihilation and semi-inclusive deep-inelastic scattering data; and a quantitative assessment of the impact of future colliders such as the High-Luminosity LHC and the Electron-Ion Collider on the proton and nuclear PDFs.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<yes>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<yes>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "The Dark Machines Anomaly Score Challenge: Benchmark Data and Model Independent Event Classification for the Large Hadron Collider. We describe the outcome of a data challenge conducted as part of the Dark Machines Initiative and the Les Houches 2019 workshop on Physics at TeV colliders. The challenged aims at detecting signals of new physics at the LHC using unsupervised machine learning algorithms. First, we propose how an anomaly score could be implemented to define model-independent signal regions in LHC searches. We define and describe a large benchmark dataset, consisting of >1 Billion simulated LHC events corresponding to $10~\\rm{fb}^{-1}$ of proton-proton collisions at a center-of-mass energy of 13 TeV. We then review a wide range of anomaly detection and density estimation algorithms, developed in the context of the data challenge, and we measure their performance in a set of realistic analysis environments. We draw a number of useful conclusions that will aid the development of unsupervised new physics searches during the third run of the LHC, and provide our benchmark dataset for future studies at https://www.phenoMLdata.org. Code to reproduce the analysis is provided at https://github.com/bostdiek/DarkMachines-UnsupervisedChallenge.",
    "output": "{Reviews}<yes>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<yes>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<yes>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "On Focusing Statistical Power for Searches and Measurements in Particle Physics. Particle physics experiments rely on the (generalised) likelihood ratio test (LRT) for searches and measurements, which consist of composite hypothesis tests. However, this test is not guaranteed to be optimal, as the Neyman-Pearson lemma pertains only to simple hypothesis tests. Any choice of test statistic thus implicitly determines how statistical power varies across the parameter space. An improvement in the core statistical testing methodology for general settings with composite tests would have widespread ramifications across experiments. We discuss an alternate test statistic that provides the data analyzer an ability to focus the power of the test on physics-motivated regions of the parameter space. We demonstrate the improvement from this technique compared to the LRT on a Higgs $\\rightarrow\\tau\\tau$ dataset simulated by the ATLAS experiment and a dark matter dataset inspired by the LZ experiment. We also employ machine learning to efficiently perform the Neyman construction, which is essential to ensure statistically valid confidence intervals.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<yes>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<yes>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "DeepEfficiency - optimal efficiency inversion in higher dimensions at the LHC. We introduce a new high dimensional algorithm for efficiency corrected, maximally Monte Carlo event generator independent fiducial measurements at the LHC and beyond. The approach is driven probabilistically using a Deep Neural Network on an event-by-event basis, trained using detector simulation and even only pure phase space distributed events. This approach gives also a glimpse into the future of high energy physics, where experiments publish new type of measurements in a radically multidimensional way.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<yes>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<yes>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "A study of topological quantities of lattice QCD by a modified DCGAN frame. A modified deep convolutional generative adversarial network (M-DCGAN) frame is proposed to study the N-dimensional (ND) topological quantities in lattice QCD based on the Monte Carlo (MC) simulations. We construct a new scaling structure including fully connected layers to support the generation of high-quality high-dimensional images for the M-DCGAN. Our results show that the M-DCGAN scheme of the Machine learning should be helpful for us to calculate efficiently the 1D distribution of topological charge and the 4D topological charge density compared with the case by the MC simulation alone.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<yes>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<yes>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Sampling NNLO QCD phase space with normalizing flows. We showcase the application of neural importance sampling for the evaluation of NNLO QCD scattering cross sections. We consider Normalizing Flows in the form of discrete Coupling Layers and time continuous flows for the integration of the various cross-section contributions when using the sector-improved residue subtraction scheme. We thereby consider the stratification of the integrands into their positive and negative contributions, and separately optimize the phase-space sampler. We exemplify the novel methods for the case of gluonic top-quark pair production at the LHC at NNLO QCD accuracy. We find significant gains with respect to the current default methods used in STRIPPER in terms of reduced cross-section variances and increased unweighting efficiencies. In turn, the computational costs for evaluations of the integrand needed to achieve a certain statistical uncertainty for the cross section can be reduced by a factor 8.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<yes>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<yes>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Preheating with deep learning. We apply deep learning techniques to the late-time turbulent regime in a post-inflationary model where a real scalar inflaton field and the standard model Higgs doublet interact with renormalizable couplings between them. After inflation, the inflaton decays into the Higgs through a trilinear coupling and the Higgs field subsequently thermalizes with gauge bosons via its $SU(2)\\times U(1)$ gauge interaction. Depending on the strength of the trilinear interaction and the Higgs self-coupling, the effective mass squared of Higgs can become negative, leading to the tachyonic production of Higgs particles. These produced Higgs particles would then share their energy with gauge bosons, potentially indicating thermalization. Since the model entails different non-perturbative effects, it is necessary to resort to numerical and semi-classical techniques. However, simulations require significant costs in terms of time and computational resources depending on the model used. Particularly, when $SU(2)$ gauge interactions are introduced, this becomes evident as the gauge field redistributes particle energies through rescattering processes, leading to an abundance of UV modes that disrupt simulation stability. This necessitates very small lattice spacings, resulting in exceedingly long simulation runtimes. Furthermore, the late-time behavior of preheating dynamics exhibits a universal form by wave kinetic theory. Therefore, we analyze patterns in the flow of particle numbers and predict future behavior using CNN-LSTM (Convolutional Neural Network combined with Long Short-Term Memory) time series analysis. In this way, we can reduce our dependence on simulations by orders of magnitude in terms of time and computational resources.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<yes>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Large Physics Models: Towards a collaborative approach with Large Language Models and Foundation Models. This paper explores ideas and provides a potential roadmap for the development and evaluation of physics-specific large-scale AI models, which we call Large Physics Models (LPMs). These models, based on foundation models such as Large Language Models (LLMs) - trained on broad data - are tailored to address the demands of physics research. LPMs can function independently or as part of an integrated framework. This framework can incorporate specialized tools, including symbolic reasoning modules for mathematical manipulations, frameworks to analyse specific experimental and simulated data, and mechanisms for synthesizing theories and scientific literature. We begin by examining whether the physics community should actively develop and refine dedicated models, rather than relying solely on commercial LLMs. We then outline how LPMs can be realized through interdisciplinary collaboration among experts in physics, computer science, and philosophy of science. To integrate these models effectively, we identify three key pillars: Development, Evaluation, and Philosophical Reflection. Development focuses on constructing models capable of processing physics texts, mathematical formulations, and diverse physical data. Evaluation assesses accuracy and reliability by testing and benchmarking. Finally, Philosophical Reflection encompasses the analysis of broader implications of LLMs in physics, including their potential to generate new scientific understanding and what novel collaboration dynamics might arise in research. Inspired by the organizational structure of experimental collaborations in particle physics, we propose a similarly interdisciplinary and collaborative approach to building and refining Large Physics Models. This roadmap provides specific objectives, defines pathways to achieve them, and identifies challenges that must be addressed to realise physics-specific large scale AI models.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<yes>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Graph Neural Networks for Particle Tracking and Reconstruction. Machine learning methods have a long history of applications in high energy physics (HEP). Recently, there is a growing interest in exploiting these methods to reconstruct particle signatures from raw detector data. In order to benefit from modern deep learning algorithms that were initially designed for computer vision or natural language processing tasks, it is common practice to transform HEP data into images or sequences. Conversely, graph neural networks (GNNs), which operate on graph data composed of elements with a set of features and their pairwise connections, provide an alternative way of incorporating weight sharing, local connectivity, and specialized domain knowledge. Particle physics data, such as the hits in a tracking detector, can generally be represented as graphs, making the use of GNNs natural. In this chapter, we recapitulate the mathematical formalism of GNNs and highlight aspects to consider when designing these networks for HEP data, including graph construction, model architectures, learning objectives, and graph pooling. We also review promising applications of GNNs for particle tracking and reconstruction in HEP and summarize the outlook for their deployment in current and future experiments.",
    "output": "{Reviews}<yes>{Modern reviews}<no>{Specialized reviews}<yes>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Jet Image Tagging Using Deep Learning: An Ensemble Model. Jet classification in high-energy particle physics is important for understanding fundamental interactions and probing phenomena beyond the Standard Model. Jets originate from the fragmentation and hadronization of quarks and gluons, and pose a challenge for identification due to their complex, multidimensional structure. Traditional classification methods often fall short in capturing these intricacies, necessitating advanced machine learning approaches. In this paper, we employ two neural networks simultaneously as an ensemble to tag various jet types. We convert the jet data to two-dimensional histograms instead of representing them as points in a higher-dimensional space. Specifically, this ensemble approach, hereafter referred to as Ensemble Model, is used to tag jets into classes from the JetNet dataset, corresponding to: Top Quarks, Light Quarks (up or down), and W and Z bosons. For the jet classes mentioned above, we show that the Ensemble Model can be used for both binary and multi-categorical classification. This ensemble approach learns jet features by leveraging the strengths of each constituent network achieving superior performance compared to either individual network.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<yes>{Jet images}<yes>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<yes>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<yes>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Mitigating a discrete sign problem with extreme learning machines. An extreme learning machine is a neural network in which only the weights in the last layer are changed during training; for such networks training can be performed efficiently and deterministically. We use an extreme learning machine to construct a control variate that tames the sign problem in the classical Ising model at imaginary external magnetic field. Using this control variate, we directly compute the partition function at imaginary magnetic field in two and three dimensions, yielding information on the positions of Lee-Yang zeros.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<yes>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<yes>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "New Angles on Fast Calorimeter Shower Simulation. The demands placed on computational resources by the simulation requirements of high energy physics experiments motivate the development of novel simulation tools. Machine learning based generative models offer a solution that is both fast and accurate. In this work we extend the Bounded Information Bottleneck Autoencoder (BIB-AE) architecture, designed for the simulation of particle showers in highly granular calorimeters, in two key directions. First, we generalise the model to a multi-parameter conditioning scenario, while retaining a high degree of physics fidelity. In a second step, we perform a detailed study of the effect of applying a state-of-the-art particle flow-based reconstruction procedure to the generated showers. We demonstrate that the performance of the model remains high after reconstruction. These results are an important step towards creating a more general simulation tool, where maintaining physics performance after reconstruction is the ultimate target.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<yes>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "On the impact of modern deep-learning techniques to the performance and time-requirements of classification models in experimental high-energy physics. Beginning from a basic neural-network architecture, we test the potential benefits offered by a range of advanced techniques for machine learning, in particular deep learning, in the context of a typical classification problem encountered in the domain of high-energy physics, using a well-studied dataset: the 2014 Higgs ML Kaggle dataset. The advantages are evaluated in terms of both performance metrics and the time required to train and apply the resulting models. Techniques examined include domain-specific data-augmentation, learning rate and momentum scheduling, (advanced) ensembling in both model-space and weight-space, and alternative architectures and connection methods. Following the investigation, we arrive at a model which achieves equal performance to the winning solution of the original Kaggle challenge, whilst being significantly quicker to train and apply, and being suitable for use with both GPU and CPU hardware setups. These reductions in timing and hardware requirements potentially allow the use of more powerful algorithms in HEP analyses, where models must be retrained frequently, sometimes at short notice, by small groups of researchers with limited hardware resources. Additionally, a new wrapper library for PyTorch called LUMIN is presented, which incorporates all of the techniques studied.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<yes>{Software}<yes>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Quantum similarity learning for anomaly detection. Anomaly detection is a vital technique for exploring signatures of new physics Beyond the Standard Model (BSM) at the Large Hadron Collider (LHC). The vast number of collisions generated by the LHC demands sophisticated deep learning techniques. Similarity learning, a self-supervised machine learning, detects anomalous signals by estimating their similarity to background events. In this paper, we explore the potential of quantum computers for anomaly detection through similarity learning, leveraging the power of quantum computing to enhance the known similarity learning method. In the realm of noisy intermediate-scale quantum (NISQ) devices, we employ a hybrid classical-quantum network to search for heavy scalar resonances in the di-Higgs production channel. In the absence of quantum noise, the hybrid network demonstrates improvement over the known similarity learning method. Moreover, we employ a clustering algorithm to reduce measurement noise from limited shot counts, resulting in $9\\%$ improvement in the hybrid network performance. Our analysis highlights the applicability of quantum algorithms for LHC data analysis, where improvements are anticipated with the advent of fault-tolerant quantum computers.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<yes>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<yes>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<yes>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Data Augmentation at the LHC through Analysis-specific Fast Simulation with Deep Learning. We present a fast simulation application based on a Deep Neural Network, designed to create large analysis-specific datasets. Taking as an example the generation of W+jet events produced in sqrt(s)= 13 TeV proton-proton collisions, we train a neural network to model detector resolution effects as a transfer function acting on an analysis-specific set of relevant features, computed at generation level, i.e., in absence of detector effects. Based on this model, we propose a novel fast-simulation workflow that starts from a large amount of generator-level events to deliver large analysis-specific samples. The adoption of this approach would result in about an order-of-magnitude reduction in computing and storage requirements for the collision simulation workflow. This strategy could help the high energy physics community to face the computing challenges of the future High-Luminosity LHC.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<yes>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Neural Network-based Top Tagger with Two-Point Energy Correlations and Geometry of Soft Emissions. Deep neural networks trained on jet images have been successful in classifying different kinds of jets. In this paper, we identify the crucial physics features that could reproduce the classification performance of the convolutional neural network in the top jet vs. QCD jet classification. We design a neural network that considers two types of substructural features: two-point energy correlations, and the IRC unsafe counting variables of a morphological analysis of jet images. The new set of IRC unsafe variables can be described by Minkowski functionals from integral geometry. To integrate these features into a single framework, we reintroduce two-point energy correlations in terms of a graph neural network and provide the other features to the network afterward. The network shows a comparable classification performance to the convolutional neural network. Since both networks are using IRC unsafe features at some level, the results based on simulations are often dependent on the event generator choice. We compare the classification results of Pythia 8 and Herwig 7, and a simple reweighting on the distribution of IRC unsafe features reduces the difference between the results from the two simulations.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<yes>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<yes>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<yes>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "A Neural-Network Extraction of Unpolarised Transverse-Momentum-Dependent Distributions. We present the first extraction of transverse-momentum-dependent distributions of unpolarised quarks from experimental Drell-Yan data using neural networks to parametrise their nonperturbative part. We show that neural networks outperform traditional parametrisations providing a more accurate description of data. This work establishes the feasibility of using neural networks to explore the multi-dimensional partonic structure of hadrons and paves the way for more accurate determinations based on machine-learning techniques.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<yes>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<yes>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Improved Neural Network Monte Carlo Simulation. The algorithm for Monte Carlo simulation of parton-level events based on an Artificial Neural Network (ANN) proposed in arXiv:1810.11509 is used to perform a simulation of $H\\to 4\\ell$ decay. Improvements in the training algorithm have been implemented to avoid numerical instabilities. The integrated decay width evaluated by the ANN is within 0.7% of the true value and unweighting efficiency of 26% is reached. While the ANN is not automatically bijective between input and output spaces, which can lead to issues with simulation quality, we argue that the training procedure naturally prefers bijective maps, and demonstrate that the trained ANN is bijective to a very good approximation.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<yes>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "DNN-based identification of additional b jets for a differential $t\\bar{t}b\\bar{b}$ cross section measurement. In differential measurements of the $t\\bar{t}b\\bar{b}$ process, observables related to the b jets not originating from top quark decays are of special interest to probe the multi-scale QCD nature of the $t\\bar{t}b\\bar{b}$ process, and the description of the additional b jet radiation via different simulation tools. In order to access these additional b jets, identification algorithms have to be applied to determine which of the b jets recorded in the detector are the b jets of interest. This is achieved via a DNN-based method which will be highlighted in this article.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<yes>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Analysis of the $gg\\to H\\to hh\\to4\\tau$ process in the 2HDM lepton specific model at the LHC. We analyse the signature of a light Higgs boson pair in the 2-Higgs Doublet Model(2HDM) Type-X (or lepton specific) over the parameter spaces allowed by theoretical self-consistency requirements as well as the latest experimental constraints from the Large Hadron Collider (LHC), precision test data and $B$ physics. Over the viable regions of the latter, wherein the Standard Model (SM)-like Higgs boson discovered at the LHC in 2012 is the heavier CP-even state of the 2HDM, $H$, it is found that the SM-like Higgs boson can decay into a pair of the lighter CP-even Higgs boson, $h$, via the process $H\\to hh$ with a Branching Ratio (BR) of $5\\%-10\\%$ or so, (with $2 m_h < m_H =125$ GeV). Furthermore, in the Type-X scenario, the lighter Higgs bosons $h$ can dominantly decay into two $\\tau$'s due to a large $\\tan\\beta$. Therefore, the pair of lighter Higgs bosons can altogether decay into a 4 $\\tau$ final state. In order to suppress the huge SM background events, we confine ourself to consider the fraction of signal events with two Same-Sign (SS) $\\tau$'s further decaying into same sign leptons while the other two $\\tau$'s decay hadronically. By using Monte Carlo (MC) and Machine Learning (ML) tools, we thus focus on the analysis of the signal process $pp\\to H\\to hh\\to \\tau^{+}\\tau^{-}\\tau^{+}\\tau^{-}\\to \\ell v_\\ell \\ell v_\\ell \\tau_h \\tau_h$ (where $\\ell= e, \\mu$ and $\\tau_h$ means a hadronic decay of the $\\tau$) and explore the feasibility of such a search at the LHC for a collision energy $\\sqrt{s}=~\\text{14 TeV}$ and a luminosity $\\text{300}~\\text{fb}^{-1}$",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<yes>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Top squark signal significance enhancement by different Machine Learning Algorithms. A study of four different machine learning (ML) algorithms is performed to determine the most suitable ML technique to disentangle a hypothetical supersymmetry signal from its corresponding Standard Model (SM) backgrounds and to establish their impact on signal significance. The study focuses on the production of SUSY top squark pairs (stops), in the mass range of $500<m_{\\tilde{t}_1}<800$ GeV, from proton-proton collisions with a center of mass energy of 13 TeV and an integrated luminosity of 150 fb$^{-1}$, emulating the data-taking conditions of the run II LHC accelerator. In particular, the semileptonic channel is analyzed, corresponding to final states with a single isolated lepton (electron or muon), missing transverse energy, and four jets, with at least one tagged as $b$-jet. The challenging compressed spectra region is targeted, where the stop decays mainly into a $W$ boson, a $b$-jet, and a neutralino ($\\tilde{t}_1\\rightarrow W+b+\\tilde{\\chi}_1^0$), with a mass gap between the stop and the neutralino of about 150 GeV. The ML algorithms are chosen to cover different mathematical implementations and features in machine learning. We compare the performance of a logistic regression (LR), a Random Forest (RF), an XGBoost (XG), and a Neural Network (NN) algorithm. Our results indicate that all four algorithms provide an improvement in signal significance calculation when compared to the ones obtained with a standard analysis method based on sequential requirements of different kinematic variables. The highest gain in significance is obtained with the NN and XG classifiers with an average improvement over 20\\%, both having compatible statistical performance for the stop mass range considered, followed by RF(15\\%). The LR has the poorest performance of all ML algorithms studied, but still presents an average improvement of about 4\\%.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<yes>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Stabilizing Neural Likelihood Ratio Estimation. Likelihood ratios are used for a variety of applications in particle physics data analysis, including parameter estimation, unfolding, and anomaly detection. When the data are high-dimensional, neural networks provide an effective tools for approximating these ratios. However, neural network training has an inherent stochasticity that limits their precision. A widely-used approach to reduce these fluctuations is to train many times and average the output (ensembling). We explore different approaches to ensembling and pretraining neural networks for stabilizing likelihood ratio estimation. For numerical studies focus on unbinned unfolding with OmniFold, as it requires many likelihood ratio estimations. We find that ensembling approaches that aggregate the models at step 1, before pushing the weights to step 2 improve both bias and variance of final results. Variance can be further improved by pre-training, however at the cost increasing bias.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<yes>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<yes>{Unfolding}<yes>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "The Tracking Machine Learning challenge : Throughput phase. This paper reports on the second \"Throughput\" phase of the Tracking Machine Learning (TrackML) challenge on the Codalab platform. As in the first \"Accuracy\" phase, the participants had to solve a difficult experimental problem linked to tracking accurately the trajectory of particles as e.g. created at the Large Hadron Collider (LHC): given O($10^5$) points, the participants had to connect them into O($10^4$) individual groups that represent the particle trajectories which are approximated helical. While in the first phase only the accuracy mattered, the goal of this second phase was a compromise between the accuracy and the speed of inference. Both were measured on the Codalab platform where the participants had to upload their software. The best three participants had solutions with good accuracy and speed an order of magnitude faster than the state of the art when the challenge was designed. Although the core algorithms were less diverse than in the first phase, a diversity of techniques have been used and are described in this paper. The performance of the algorithms are analysed in depth and lessons derived.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<yes>{Software}<yes>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Integrating Particle Flavor into Deep Learning Models for Hadronization. Hadronization models used in event generators are physics-inspired functions with many tunable parameters. Since we do not understand hadronization from first principles, there have been multiple proposals to improve the accuracy of hadronization models by utilizing more flexible parameterizations based on neural networks. These recent proposals have focused on the kinematic properties of hadrons, but a full model must also include particle flavor. In this paper, we show how to build a deep learning-based hadronization model that includes both kinematic (continuous) and flavor (discrete) degrees of freedom. Our approach is based on Generative Adversarial Networks and we show the performance within the context of the cluster hadronization model within the Herwig event generator.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<yes>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Deep learning applications for quality control in particle detector construction. The growing complexity of particle detectors makes their construction and quality control a new challenge. We present studies that explore the use of deep learning-based computer vision techniques to perform quality checks of detector components and assembly steps, which will automate procedures and minimize the need for human interventions. This study focuses on the construction steps of a silicon detector, which involve forming a mechanical structure with the sensor and wire bonding individual cells to electronics for reading out signals. Silicon detectors in high energy physics experiments today have millions of channels. Manual quality control of these and other high channel-density detectors requires enormous amounts of labor and can be prone to errors. Here, we explore computer vision applications to either augment or fully replace visual inspections done by humans. We investigated convolutional neural networks for image classification and autoencoders for anomalies detection. Two proof-of-concept studies will be presented.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<yes>{Pileup}<no>{Calibration}<yes>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "GANplifying Event Samples. A critical question concerning generative networks applied to event generation in particle physics is if the generated events add statistical precision beyond the training sample. We show for a simple example with increasing dimensionality how generative networks indeed amplify the training statistics. We quantify their impact through an amplification factor or equivalent numbers of sampled events.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<yes>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "An equation-of-state-meter for CBM using PointNet. A novel method for identifying the nature of QCD transitions in heavy-ion collision experiments is introduced. PointNet based Deep Learning (DL) models are developed to classify the equation of state (EoS) that drives the hydrodynamic evolution of the system created in Au-Au collisions at 10 AGeV. The DL models were trained and evaluated in different hypothetical experimental situations. A decreased performance is observed when more realistic experimental effects (acceptance cuts and decreased resolutions) are taken into account. It is shown that the performance can be improved by combining multiple events to make predictions. The PointNet based models trained on the reconstructed tracks of charged particles from the CBM detector simulation discriminate a crossover transition from a first order phase transition with an accuracy of up to 99.8%. The models were subjected to several tests to evaluate the dependence of its performance on the centrality of the collisions and physical parameters of fluid dynamic simulations. The models are shown to work in a broad range of centralities (b=0-7 fm). However, the performance is found to improve for central collisions (b=0-3 fm). There is a drop in the performance when the model parameters lead to reduced duration of the fluid dynamic evolution or when less fraction of the medium undergoes the transition. These effects are due to the limitations of the underlying physics and the DL models are shown to be superior in its discrimination performance in comparison to conventional mean observables.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<yes>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Reweighting simulated events using machine-learning techniques in the CMS experiment. Data analyses in particle physics rely on an accurate simulation of particle collisions and a detailed simulation of detector effects to extract physics knowledge from the recorded data. Event generators together with a GEANT-based simulation of the detectors are used to produce large samples of simulated events for analysis by the LHC experiments. These simulations come at a high computational cost, where the detector simulation and reconstruction algorithms have the largest CPU demands. This article describes how machine-learning (ML) techniques are used to reweight simulated samples obtained with a given set of model parameters to samples with different parameters or samples obtained from entirely different simulation programs. The ML reweighting method avoids the need for simulating the detector response multiple times by incorporating the relevant information in a single sample through event weights. Results are presented for reweighting to model variations and higher-order calculations in simulated top quark pair production at the LHC. This ML-based reweighting is an important element of the future computing model of the CMS experiment and will facilitate precision measurements at the High-Luminosity LHC.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<yes>{Pileup}<no>{Calibration}<yes>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Probing Light Fermiophobic Higgs Boson via diphoton jets at the HL-LHC. In this study, we explore the phenomenological signatures associated with a light fermiophobic Higgs boson, $h_{\\rm f}$, within the type-I two-Higgs-doublet model at the HL-LHC. Our meticulous parameter scan illuminates an intriguing mass range for $m_{h_{\\rm f}}$, spanning $[1,10]{\\;{\\rm GeV}}$. This mass range owes its viability to substantial parameter points, largely due to the inherent challenges of detecting the soft decay products of $h_{\\rm f}$ at contemporary high-energy colliders. Given that this light $h_{\\rm f}$ ensures $Br(h_{\\rm f}\\to\\gamma\\gamma)\\simeq 1$, $Br(H^\\pm \\to h_{\\rm f} W^\\pm)\\simeq 1$, and $M_{H^\\pm}\\lesssim 330{\\;{\\rm GeV}}$, we propose a golden discovery channel: $pp\\to h_{\\rm f}H^\\pm\\to \\gamma\\gamma\\gamma\\gamma \\,l^\\pm\\nu$, where $l^\\pm$ includes $e^\\pm$ and $\\mu^\\pm$. However, a significant obstacle arises as the two photons from the $h_{\\rm f}$ decay mostly merge into a single jet due to their proximity within $\\Delta R<0.4$. This results in a final state characterized by two jets, rather than four isolated photons, thus intensifying the QCD backgrounds. To tackle this, we devise a strategy within \\textsc{Delphes} to identify jets with two leading subparticles as photons, termed diphoton jets. Our thorough detector-level simulations across 18 benchmark points predominantly show signal significances exceeding the $5\\sigma$ threshold at an integrated luminosity of $3{\\;{\\rm ab}^{-1}}$. Furthermore, our approach facilitates accurate mass reconstructions for both $m_{h_{\\rm f}}$ and $M_{H^\\pm}$. Notably, in the intricate scenarios with heavy charged Higgs bosons, our application of machine learning techniques provides a significant boost in significance.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<yes>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Progress in End-to-End Optimization of Detectors for Fundamental Physics with Differentiable Programming. In this article we examine recent developments in the research area concerning the creation of end-to-end models for the complete optimization of measuring instruments. The models we consider rely on differentiable programming methods and on the specification of a software pipeline including all factors impacting performance -- from the data-generating processes to their reconstruction and the extraction of inference on the parameters of interest of a measuring instrument -- along with the careful specification of a utility function well aligned with the end goals of the experiment. Building on previous studies originated within the MODE Collaboration, we focus specifically on applications involving instruments for particle physics experimentation, as well as industrial and medical applications that share the detection of radiation as their data-generating mechanism.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<yes>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<yes>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "CMS highlights on searches for new physics in final states with jets. Many new physics models, e.g., leptoquarks, extra dimensions, extended Higgs sectors, supersymmetric theories, and dark sector extensions, are expected to manifest themselves in the final states with hadronic jets. Novel experimental techniques, including a dedicated scouting trigger stream and advanced machine learning techniques can be employed to identify such signals. This talk presents searches in CMS for new phenomena in the final states that include jets, focusing on the most recent results obtained using the full Run-II data-set collected at the LHC.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<yes>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<yes>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Preserving New Physics while Simultaneously Unfolding All Observables. Direct searches for new particles at colliders have traditionally been factorized into model proposals by theorists and model testing by experimentalists. With the recent advent of machine learning methods that allow for the simultaneous unfolding of all observables in a given phase space region, there is a new opportunity to blur these traditional boundaries by performing searches on unfolded data. This could facilitate a research program where data are explored in their natural high dimensionality with as little model bias as possible. We study how the information about physics beyond the Standard Model is preserved by full phase space unfolding using an important physics target at the Large Hadron Collider (LHC): exotic Higgs boson decays involving hadronic final states. We find that if the signal cross section is high enough, information about the new physics is visible in the unfolded data. We will show that in some cases, quantifiably all of the information about the new physics is encoded in the unfolded data. Finally, we show that there are still many cases when the unfolding does not work fully or precisely, such as when the signal cross section is small. This study will serve as an important benchmark for enhancing unfolding methods for the LHC and beyond.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<yes>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<yes>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Electron Energy Regression in the CMS High-Granularity Calorimeter Prototype. We present a new publicly available dataset that contains simulated data of a novel calorimeter to be installed at the CERN Large Hadron Collider. This detector will have more than six-million channels with each channel capable of position, ionisation and precision time measurement. Reconstructing these events in an efficient way poses an immense challenge which is being addressed with the latest machine learning techniques. As part of this development a large prototype with 12,000 channels was built and a beam of high-energy electrons incident on it. Using machine learning methods we have reconstructed the energy of incident electrons from the energies of three-dimensional hits, which is known to some precision. By releasing this data publicly we hope to encourage experts in the application of machine learning to develop efficient and accurate image reconstruction of these electrons.",
    "output": "{Reviews}<yes>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<yes>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "ML-based Calibration and Control of the GlueX Central Drift Chamber. The GlueX Central Drift Chamber (CDC) in Hall D at Jefferson Lab, used for detecting and tracking charged particles, is calibrated and controlled during data taking using a Gaussian process. The system dynamically adjusts the high voltage applied to the anode wires inside the chamber in response to changing environmental and experimental conditions such that the gain is stabilized. Control policies have been established to manage the CDC's behavior. These policies are activated when the model's uncertainty exceeds a configurable threshold or during human-initiated tests during normal production running. We demonstrate the system reduces the time detector experts dedicate to calibration of the data offline, leading to a marked decrease in computing resource usage without compromising detector performance.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<yes>{Pileup}<no>{Calibration}<yes>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Hadronic Top Quark Polarimetry with ParticleNet. Precision studies for top quark physics are a cornerstone of the Large Hadron Collider program. Polarization, probed through decay kinematics, provides a unique tool to scrutinize the top quark across its various production modes and to explore potential new physics effects. However, the top quark most often decays hadronically, for which unambiguous identification of its decay products sensitive to top quark polarization is not possible. In this Letter, we introduce a jet flavor tagging method to significantly improve spin analyzing power in hadronic decays, going beyond exclusive kinematic information employed in previous studies. We provide parametric estimates of the improvement from flavor tagging with any set of measured observables and demonstrate this in practice on simulated data using a Graph Neural Network (GNN). We find that the spin analyzing power in hadronic decays can improve by approximately 20% (40%) compared to the kinematic approach, assuming an efficiency of 0.5 (0.2) for the network.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<yes>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Probing stop pair production at the LHC with graph neural networks. Top-squarks (stops) play a crucial role for the naturalness of supersymmetry (SUSY). However, searching for the stops is a tough task at the LHC. To dig the stops out of the huge LHC data, various expert-constructed kinematic variables or cutting-edge analysis techniques have been invented. In this paper, we propose to represent collision events as event graphs and use the message passing neutral network (MPNN) to analyze the events. As a proof-of-concept, we use our method in the search of the stop pair production at the LHC, and find that our MPNN can efficiently discriminate the signal and background events. In comparison with other machine learning methods (e.g. DNN), MPNN can enhance the mass reach of stop mass by several tens of GeV to over a hundred GeV.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<yes>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<yes>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Resonant anomaly detection without background sculpting. We introduce a new technique named Latent CATHODE (LaCATHODE) for performing \"enhanced bump hunts\", a type of resonant anomaly search that combines conventional one-dimensional bump hunts with a model-agnostic anomaly score in an auxiliary feature space where potential signals could also be localized. The main advantage of LaCATHODE over existing methods is that it provides an anomaly score that is well behaved when evaluating it beyond the signal region, which is essential to prevent the sculpting of background distributions in the bump hunt. LaCATHODE accomplishes this by constructing the anomaly score directly in the latent space learned by a conditional normalizing flow trained on sideband regions. We demonstrate the superior stability and comparable performance of LaCATHODE for enhanced bump hunting in an illustrative toy example as well as on the LHC Olympics R&D dataset.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<yes>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "High energy nuclear physics meets Machine Learning. Though being seemingly disparate and with relatively new intersection, high energy nuclear physics and machine learning have already begun to merge and yield interesting results during the last few years. It's worthy to raise the profile of utilizing this novel mindset from machine learning in high energy nuclear physics, to help more interested readers see the breadth of activities around this intersection. The aim of this mini-review is to introduce to the community the current status and report an overview of applying machine learning for high energy nuclear physics, to present from different aspects and examples how scientific questions involved in high energy nuclear physics can be tackled using machine learning.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<yes>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Pulse shape discrimination using a convolutional neural network for organic liquid scintillator signals. A convolutional neural network (CNN) architecture is developed to improve the pulse shape discrimination (PSD) power of the gadolinium-loaded organic liquid scintillation detector to reduce the fast neutron background in the inverse beta decay candidate events of the NEOS-II data. A power spectrum of an event is constructed using a fast Fourier transform of the time domain raw waveforms and put into CNN. An early data set is evaluated by CNN after it is trained using low energy $\\beta$ and $\\alpha$ events. The signal-to-background ratio averaged over 1-10 MeV visible energy range is enhanced by more than 20% in the result of the CNN method compared to that of an existing conventional PSD method, and the improvement is even higher in the low energy region.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<yes>{Performance studies}<yes>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Machine Learning for Particle Flow Reconstruction at CMS. We provide details on the implementation of a machine-learning based particle flow algorithm for CMS. The standard particle flow algorithm reconstructs stable particles based on calorimeter clusters and tracks to provide a global event reconstruction that exploits the combined information of multiple detector subsystems, leading to strong improvements for quantities such as jets and missing transverse energy. We have studied a possible evolution of particle flow towards heterogeneous computing platforms such as GPUs using a graph neural network. The machine-learned PF model reconstructs particle candidates based on the full list of tracks and calorimeter clusters in the event. For validation, we determine the physics performance directly in the CMS software framework when the proposed algorithm is interfaced with the offline reconstruction of jets and missing transverse energy. We also report the computational performance of the algorithm, which scales approximately linearly in runtime and memory usage with the input size.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<yes>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<yes>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<yes>{Pileup}<no>{Calibration}<yes>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Flow Annealed Importance Sampling Bootstrap meets Differentiable Particle Physics. High-energy physics requires the generation of large numbers of simulated data samples from complex but analytically tractable distributions called matrix elements. Surrogate models, such as normalizing flows, are gaining popularity for this task due to their computational efficiency. We adopt an approach based on Flow Annealed importance sampling Bootstrap (FAB) that evaluates the differentiable target density during training and helps avoid the costly generation of training data in advance. We show that FAB reaches higher sampling efficiency with fewer target evaluations in high dimensions in comparison to other methods.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<yes>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<yes>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "New approaches for boosting to uniformity. The use of multivariate classifiers has become commonplace in particle physics. To enhance the performance, a series of classifiers is typically trained; this is a technique known as boosting. This paper explores several novel boosting methods that have been designed to produce a uniform selection efficiency in a chosen multivariate space. Such algorithms have a wide range of applications in particle physics, from producing uniform signal selection efficiency across a Dalitz-plot to avoiding the creation of false signal peaks in an invariant mass distribution when searching for new particles.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<yes>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Unsupervised Particle Tracking with Neuromorphic Computing. We study the application of a neural network architecture for identifying charged particle trajectories via unsupervised learning of delays and synaptic weights using a spike-time-dependent plasticity rule. In the considered model, the neurons receive time-encoded information on the position of particle hits in a tracking detector for a particle collider, modeled according to the geometry of the Compact Muon Solenoid Phase II detector. We show how a spiking neural network is capable of successfully identifying in a completely unsupervised way the signal left by charged particles in the presence of conspicuous noise from accidental or combinatorial hits. These results open the way to applications of neuromorphic computing to particle tracking, motivating further studies into its potential for real-time, low-power particle tracking in future high-energy physics experiments.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<yes>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<yes>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<yes>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Presenting Unbinned Differential Cross Section Results. Machine learning tools have empowered a qualitatively new way to perform differential cross section measurements whereby the data are unbinned, possibly in many dimensions. Unbinned measurements can enable, improve, or at least simplify comparisons between experiments and with theoretical predictions. Furthermore, many-dimensional measurements can be used to define observables after the measurement instead of before. There is currently no community standard for publishing unbinned data. While there are also essentially no measurements of this type public, unbinned measurements are expected in the near future given recent methodological advances. The purpose of this paper is to propose a scheme for presenting and using unbinned results, which can hopefully form the basis for a community standard to allow for integration into analysis workflows. This is foreseen to be the start of an evolving community dialogue, in order to accommodate future developments in this field that is rapidly evolving.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<yes>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<yes>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Controlling Physical Attributes in GAN-Accelerated Simulation of Electromagnetic Calorimeters. High-precision modeling of subatomic particle interactions is critical for many fields within the physical sciences, such as nuclear physics and high energy particle physics. Most simulation pipelines in the sciences are computationally intensive -- in a variety of scientific fields, Generative Adversarial Networks have been suggested as a solution to speed up the forward component of simulation, with promising results. An important component of any simulation system for the sciences is the ability to condition on any number of physically meaningful latent characteristics that can effect the forward generation procedure. We introduce an auxiliary task to the training of a Generative Adversarial Network on particle showers in a multi-layer electromagnetic calorimeter, which allows our model to learn an attribute-aware conditioning mechanism.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<yes>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Learning BPS Spectra and the Gap Conjecture. We explore statistical properties of BPS q-series for 3d N=2 strongly coupled supersymmetric theories that correspond to a particular family of 3-manifolds Y. We discover that gaps between exponents in the q-series are statistically more significant at the beginning of the q-series compared to gaps that appear in higher powers of q. Our observations are obtained by calculating saliencies of q-series features used as input data for principal component analysis, which is a standard example of an explainable machine learning technique that allows for a direct calculation and a better analysis of feature saliencies.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<yes>{Theory and physics for ML}<no>{ML for theory}<yes>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Event Generation and Density Estimation with Surjective Normalizing Flows. Normalizing flows are a class of generative models that enable exact likelihood evaluation. While these models have already found various applications in particle physics, normalizing flows are not flexible enough to model many of the peripheral features of collision events. Using the framework of Nielsen et al. (2020), we introduce several surjective and stochastic transform layers to a baseline normalizing flow to improve modelling of permutation symmetry, varying dimensionality and discrete features, which are all commonly encountered in particle physics events. We assess their efficacy in the context of the generation of a matrix element-level process, and in the context of anomaly detection in detector-level LHC events.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<yes>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<yes>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Heavy Quark Potential in QGP: DNN meets LQCD. Bottomonium states are key probes for experimental studies of the quark-gluon plasma (QGP) created in high-energy nuclear collisions. Theoretical models of bottomonium productions in high-energy nuclear collisions rely on the in-medium interactions between the bottom and antibottom quarks. The latter can be characterized by the temperature ($T$) dependent potential, with real ($V_R(T,r)$) and imaginary ($V_I(T,r)$) parts, as a function of the spatial separation ($r$). Recently, the masses and thermal widths of up to $3S$ and $2P$ bottomonium states in QGP were calculated using lattice quantum chromodynamics (LQCD). Starting from these LQCD results and through a novel application of deep neural network, here, we obtain $V_R(T,r)$ and $V_I(T,r)$ in a model-independent fashion. The temperature dependence of $V_R(T,r)$ was found to be very mild between $T\\approx0-334$~MeV. For $T=151-334$~MeV, $V_I(T,r)$ shows a rapid increase with $T$ and $r$, which is much larger than the perturbation-theory-based expectations.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<yes>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<yes>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  }
]