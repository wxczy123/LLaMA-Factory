[
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Accelerating Resonance Searches via Signature-Oriented Pre-training. The search for heavy resonances beyond the Standard Model (BSM) is a key objective at the LHC. While the recent use of advanced deep neural networks for boosted-jet tagging significantly enhances the sensitivity of dedicated searches, it is limited to specific final states, leaving vast potential BSM phase space underexplored. We introduce a novel experimental method, Signature-Oriented Pre-training for Heavy-resonance ObservatioN (Sophon), which leverages deep learning to cover an extensive number of boosted final states. Pre-trained on the comprehensive JetClass-II dataset, the Sophon model learns intricate jet signatures, ensuring the optimal constructions of various jet tagging discriminates and enabling high-performance transfer learning capabilities. We show that the method can not only push widespread model-specific searches to their sensitivity frontier, but also greatly improve model-agnostic approaches, accelerating LHC resonance searches in a broad sense.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<yes>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Using Machine Learning techniques in phenomenological studies in flavour physics. An updated analysis of New Physics violating Lepton Flavour Universality, by using the Standard Model Effective Field Lagrangian with semileptonic dimension six operators at $\\Lambda = 1\\,\\mathrm{TeV}$ is presented. We perform a global fit, by discussing the relevance of the mixing in the first generation. We use for the first time in this context a Montecarlo analysis to extract the confidence intervals and correlations between observables. Our results show that machine learning, made jointly with the SHAP values, constitute a suitable strategy to use in this kind of analysis.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<yes>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<yes>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Graph theory inspired anomaly detection at the LHC. Designing model-independent anomaly detection algorithms for analyzing LHC data remains a central challenge in the search for new physics, due to the high dimensionality of collider events. In this work, we develop a graph autoencoder as an unsupervised, model-agnostic tool for anomaly detection, using the LHC Olympics dataset as a benchmark. By representing jet constituents as a graph, we introduce a method to systematically control the information available to the model through sparse graph constructions that serve as physically motivated inductive biases. Specifically, (1) we construct graph autoencoders based on locally rigid Laman graphs and globally rigid unique graphs, and (2) we explore the clustering of jet constituents into subjets to interpolate between high- and low-level input representations. We obtain the best performance, measured in terms of the Significance Improvement Characteristic curve for an intermediate level of subjet clustering and certain sparse unique graph constructions. We further investigate the role of graph connectivity in jet classification tasks. Our results demonstrate the potential of leveraging graph-theoretic insights to refine and increase the interpretability of machine learning tools for collider experiments.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<yes>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<yes>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<no>{(Variational) Autoencoders}<yes>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<yes>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Deep Learning Jet Image as a Probe of Light Higgsino Dark Matter at the LHC. Higgsino in supersymmetric standard models can play the role of dark matter particle. In conjunction with the naturalness criterion, the higgsino mass parameter is expected to be around the electroweak scale. In this work, we explore the potential of probing the nearly degenerate light higgsinos with machine learning at the LHC. By analyzing jet images and other jet substructure information, we use the Convolutional Neural Network(CNN) to enhance the signal significance. We find that our deep learning jet image method can improve the previous result based on the conventional cut-flow by about a factor of two at the High-Luminosity LHC.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<yes>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Deep learning for track recognition in pixel and strip-based particle detectors. The reconstruction of charged particle trajectories in tracking detectors is a key problem in the analysis of experimental data for high-energy and nuclear physics. The amount of data in modern experiments is so large that classical tracking methods, such as the Kalman filter, cannot process them fast enough. To solve this problem, we developed two neural network algorithms based on deep learning architectures for track recognition in pixel and strip-based particle detectors. These are TrackNETv3 for local (track by track) and RDGraphNet for global (all tracks in an event) tracking. These algorithms were tested using the GEM tracker of the BM@N experiment at JINR (Dubna) and the cylindrical GEM inner tracker of the BESIII experiment at IHEP CAS (Beijing). The RDGraphNet model, based on a reverse directed graph, showed encouraging results: 95% recall and 74% precision for track finding. The TrackNETv3 model demonstrated a recall value of 95% and 76% precision. This result can be improved after further model optimization.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<yes>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Gravitational Duals from Equations of State. Holography relates gravitational theories in five dimensions to four-dimensional quantum field theories in flat space. Under this map, the equation of state of the field theory is encoded in the black hole solutions of the gravitational theory. Solving the five-dimensional Einstein's equations to determine the equation of state is an algorithmic, direct problem. Determining the gravitational theory that gives rise to a prescribed equation of state is a much more challenging, inverse problem. We present a novel approach to solve this problem based on physics-informed neural networks. The resulting algorithm is not only data-driven but also informed by the physics of the Einstein's equations. We successfully apply it to theories with crossovers, first- and second-order phase transitions.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<yes>{Theory and physics for ML}<no>{ML for theory}<yes>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "A Lorentz-Equivariant Transformer for All of the LHC. We show that the Lorentz-Equivariant Geometric Algebra Transformer (L-GATr) yields state-of-the-art performance for a wide range of machine learning tasks at the Large Hadron Collider. L-GATr represents data in a geometric algebra over space-time and is equivariant under Lorentz transformations. The underlying architecture is a versatile and scalable transformer, which is able to break symmetries if needed. We demonstrate the power of L-GATr for amplitude regression and jet classification, and then benchmark it as the first Lorentz-equivariant generative network. For all three LHC tasks, we find significant improvements over previous architectures.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<yes>{top quark tagging}<yes>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<yes>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<yes>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Mind the Gap: Navigating Inference with Optimal Transport Maps. Machine learning (ML) techniques have recently enabled enormous gains in sensitivity across the sciences. In particle physics, much of this progress has relied on excellent simulations of a wide range of physical processes. However, due to the sophistication of modern machine learning (ML) algorithms and their reliance on high-quality training samples, discrepancies between simulation and experimental data can significantly limit the effectiveness of ML techniques. In this work, we present a solution to this ``mis-specification'' problem: a calibration approach based on optimal transport, which we apply to high-dimensional simulations for the first time. We demonstrate the performance of our approach through jet tagging, using a CMS-inspired dataset. A 128-dimensional internal jet representation from a powerful general-purpose classifier is studied; after calibrating this internal ``latent'' representation, we find that a wide variety of quantities derived from it for downstream tasks are also properly calibrated: using this calibrated high-dimensional representation, powerful new applications of jet flavor information can be utilized in LHC analyses. This is a key step toward allowing properly-calibrated ``foundation models'' in particle physics. More broadly, this calibration framework has broad applications for correcting high-dimensional simulations across the sciences.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<yes>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<yes>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<yes>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<yes>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<yes>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Comparing Weak- and Unsupervised Methods for Resonant Anomaly Detection. Anomaly detection techniques are growing in importance at the Large Hadron Collider (LHC), motivated by the increasing need to search for new physics in a model-agnostic way. In this work, we provide a detailed comparative study between a well-studied unsupervised method called the autoencoder (AE) and a weakly-supervised approach based on the Classification Without Labels (CWoLa) technique. We examine the ability of the two methods to identify a new physics signal at different cross sections in a fully hadronic resonance search. By construction, the AE classification performance is independent of the amount of injected signal. In contrast, the CWoLa performance improves with increasing signal abundance. When integrating these approaches with a complete background estimate, we find that the two methods have complementary sensitivity. In particular, CWoLa is effective at finding diverse and moderately rare signals while the AE can provide sensitivity to very rare signals, but only with certain topologies. We therefore demonstrate that both techniques are complementary and can be used together for anomaly detection at the LHC.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<yes>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "The NFLikelihood: an unsupervised DNNLikelihood from Normalizing Flows. We propose the NFLikelihood, an unsupervised version, based on Normalizing Flows, of the DNNLikelihood proposed in Ref.[1]. We show, through realistic examples, how Autoregressive Flows, based on affine and rational quadratic spline bijectors, are able to learn complicated high-dimensional Likelihoods arising in High Energy Physics (HEP) analyses. We focus on a toy LHC analysis example already considered in the literature and on two Effective Field Theory fits of flavor and electroweak observables, whose samples have been obtained throught the HEPFit code. We discuss advantages and disadvantages of the unsupervised approach with respect to the supervised one and discuss possible interplays of the two.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<yes>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<yes>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<yes>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Estimating the warm dark matter mass from strong lensing images with truncated marginal neural ratio estimation. Precision analysis of galaxy-galaxy strong gravitational lensing images provides a unique way of characterizing small-scale dark matter halos, and could allow us to uncover the fundamental properties of dark matter's constituents. Recently, gravitational imaging techniques made it possible to detect a few heavy subhalos. However, gravitational lenses contain numerous subhalos and line-of-sight halos, whose subtle imprint is extremely difficult to detect individually. Existing methods for marginalizing over this large population of sub-threshold perturbers to infer population-level parameters are typically computationally expensive, or require compressing observations into hand-crafted summary statistics, such as a power spectrum of residuals. Here, we present the first analysis pipeline to combine parametric lensing models and a recently-developed neural simulation-based inference technique called truncated marginal neural ratio estimation (TMNRE) to constrain the warm dark matter halo mass function cutoff scale directly from multiple lensing images. Through a proof-of-concept application to simulated data, we show that our approach enables empirically testable inference of the dark matter cutoff mass through marginalization over a large population of realistic perturbers that would be undetectable on their own, and over lens and source parameters uncertainties. To obtain our results, we combine the signal contained in a set of images with Hubble Space Telescope resolution. Our results suggest that TMNRE can be a powerful approach to put tight constraints on the mass of warm dark matter in the multi-keV regime, which will be relevant both for existing lensing data and in the large sample of lenses that will be delivered by near-future telescopes.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<yes>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Performance studies of jet flavor tagging and measurement of $R_b$ using ParticleNet at CEPC. Jet flavor tagging plays a crucial role in the measurement of relative partial decay widths of $Z$ boson, denoted as $R_b$($R_c$), which is considered as a fundamental test of the Standard Model and sensitive probe to new physics. In this study, a Deep Learning algorithm, ParticleNet, is employed to enhance the performance of jet flavor tagging. The combined efficiency and purity of $c$-tagging is improved by more than 50\\% compared to the Circular Electron Positron Collider (CEPC) baseline software. In order to measure $R_b$($R_c$) with this new flavor tagging approach, we have adopted the double-tagging method. The precision of $R_b$($R_c$) is improved significantly, in particular to $R_c$, which has seen a reduction in statistical uncertainty by 40\\%.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<yes>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Particle-flow based tau identification at future $\\textrm{e}^{+}\\textrm{e}^{-}$ colliders. Identifying and reconstructing hadronic $\\tau$ decays ($\\tau_{\\textrm{h}}$) is an important task at current and future high-energy physics experiments, as $\\tau_{\\textrm{h}}$ represent an important tool to analyze the production of Higgs and electroweak bosons as well as to search for physics beyond the Standard Model. The identification of $\\tau_{\\textrm{h}}$ can be viewed as a generalization and extension of jet-flavour tagging, which has in the recent years undergone significant progress due to the use of deep learning. Based on a granular simulation with realistic detector effects and a particle flow-based event reconstruction, we show in this paper that deep learning-based jet-flavour-tagging algorithms are powerful $\\tau_{\\textrm{h}}$ identifiers. Specifically, we show that jet-flavour-tagging algorithms such as LorentzNet and ParticleTransformer can be adapted in an end-to-end fashion for discriminating $\\tau_{\\textrm{h}}$ from quark and gluon jets. We find that the end-to-end transformer-based approach significantly outperforms contemporary state-of-the-art $\\tau_{\\textrm{h}}$ reconstruction and identification algorithms currently in use at the Large Hadron Collider.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<yes>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Implicit Neural Representation as a Differentiable Surrogate for Photon Propagation in a Monolithic Neutrino Detector. Optical photons are used as signal in a wide variety of particle detectors. Modern neutrino experiments employ hundreds to tens of thousands of photon detectors to observe signal from millions to billions of scintillation photons produced from energy deposition of charged particles. These neutrino detectors are typically large, containing kilotons of target volume, with different optical properties. Modeling individual photon propagation in form of look-up table requires huge computational resources. As the size of a table increases with detector volume for a fixed resolution, this method scales poorly for future larger detectors. Alternative approaches such as fitting a polynomial to the model could address the memory issue, but results in poorer performance. Both look-up table and fitting approaches are prone to discrepancies between the detector simulation and the data collected. We propose a new approach using SIREN, an implicit neural representation with periodic activation functions, to model the look-up table as a 3D scene and reproduces the acceptance map with high accuracy. The number of parameters in our SIREN model is orders of magnitude smaller than the number of voxels in the look-up table. As it models an underlying functional shape, SIREN is scalable to a larger detector. Furthermore, SIREN can successfully learn the spatial gradients of the photon library, providing additional information for downstream applications. Finally, as SIREN is a neural network representation, it is differentiable with respect to its parameters, and therefore tunable via gradient descent. We demonstrate the potential of optimizing SIREN directly on real data, which mitigates the concern of data vs. simulation discrepancies. We further present an application for data reconstruction where SIREN is used to form a likelihood function for photon statistics.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<yes>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<yes>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<yes>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<yes>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Probes of Anomalous Events at LHC with Self-Organizing Maps. We propose an Unsupervised Learning Algorithm, Self-Organizing Maps (SOM), built on a neural network architecture, for the probe of a rare top decay, mediated by Flavor Changing Neutral Current (FCNC), to charm and the Higgs boson, with the Higgs boson further decaying to a pair of b-quarks or a pair of gauge bosons ($W^{\\pm}/Z$) in a boosted regime. Ideally, the particles originating from the decay of the boosted top lead to the reconstruction of a large-R jet, comprising three-prong substructures, with b- and c-tagged subjets. We have illustrated that SOM can be effectively implemented as an anomaly-finder for model-agnostic probes of such decay at the LHC. Relatively simple in formalism, this algorithm achieves commendable performance over other existing formalisms, tailored to probe this rare decay. We discuss the robustness of this algorithm, which can be successfully implemented for other BSM probes, both for model-agnostic and model-dependent searches.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<yes>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<yes>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<yes>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Generative Machine Learning for Detector Response Modeling with a Conditional Normalizing Flow. In this paper, we explore the potential of generative machine learning models as an alternative to the computationally expensive Monte Carlo (MC) simulations commonly used by the Large Hadron Collider (LHC) experiments. Our objective is to develop a generative model capable of efficiently simulating detector responses for specific particle observables, focusing on the correlations between detector responses of different particles in the same event and accommodating asymmetric detector responses. We present a conditional normalizing flow model (CNF) based on a chain of Masked Autoregressive Flows, which effectively incorporates conditional variables and models high-dimensional density distributions. We assess the performance of the \\cnf model using a simulated sample of Higgs boson decaying to diphoton events at the LHC. We create reconstruction-level observables using a smearing technique. We show that conditional normalizing flows can accurately model complex detector responses and their correlation. This method can potentially reduce the computational burden associated with generating large numbers of simulated events while ensuring that the generated events meet the requirements for data analyses.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<yes>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "A deep-learning based raw waveform region-of-interest finder for the liquid argon time projection chamber. The liquid argon time projection chamber (LArTPC) detector technology has an excellent capability to measure properties of low-energy neutrinos produced by the sun and supernovae and to look for exotic physics at very low energies. In order to achieve those physics goals, it is crucial to identify and reconstruct signals in the waveforms recorded on each TPC wire. In this paper, we report on a novel algorithm based on a one-dimensional convolutional neural network (CNN) to look for the region-of-interest (ROI) in raw waveforms. We test this algorithm using data from the ArgoNeuT experiment in conjunction with an improved noise mitigation procedure and a more realistic data-driven noise model for simulated events. This deep-learning ROI finder shows promising performance in extracting small signals and gives an efficiency approximately twice that of the traditional algorithm in the low energy region of $\\sim$0.03-0.1 MeV. This method offers great potential to explore low-energy physics using LArTPCs.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<yes>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "JetFlow: Generating Jets with Conditioned and Mass Constrained Normalising Flows. Fast data generation based on Machine Learning has become a major research topic in particle physics. This is mainly because the Monte Carlo simulation approach is computationally challenging for future colliders, which will have a significantly higher luminosity. The generation of collider data is similar to point cloud generation with complex correlations between the points. In this study, the generation of jets with up to 30 constituents with Normalising Flows using Rational Quadratic Spline coupling layers is investigated. Without conditioning on the jet mass, our Normalising Flows are unable to model all correlations in data correctly, which is evident when comparing the invariant jet mass distributions between ground truth and generated data. Using the invariant mass as a condition for the coupling transformation enhances the performance on all tracked metrics. In addition, we demonstrate how to sample the original mass distribution by interpolating the empirical cumulative distribution function. Similarly, the variable number of constituents is taken care of by introducing an additional condition on the number of constituents in the jet. Furthermore, we study the usefulness of including an additional mass constraint in the loss term. On the \\texttt{JetNet} dataset, our model shows state-of-the-art performance combined with fast and stable training.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<yes>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Machine Learning improved fits of the sound horizon at the baryon drag epoch. The baryon acoustic oscillations (BAO) have proven to be an invaluable tool in constraining the expansion history of the Universe at late times and are characterized by the comoving sound horizon at the baryon drag epoch $r_\\mathrm{s}(z_\\mathrm{d})$. The latter quantity can be calculated either numerically using recombination codes or via fitting functions, such as the one by Eisenstein and Hu (EH), made via grids of parameters of the recombination history. Here we quantify the accuracy of these expressions and show that they can strongly bias the derived constraints on the cosmological parameters using BAO data. Then, using a machine learning approach, called the genetic algorithms, we proceed to derive new analytic expressions for $r_\\mathrm{s}(z_\\mathrm{d})$ which are accurate at the $\\sim0.003\\%$ level in a range of $10\\sigma$ around the Planck 2018 best-fit or $\\sim0.018\\%$ in a much broader range, compared to $\\sim 2-4\\%$ for the EH expression, thus obtaining an improvement of two to three orders of magnitude. Moreover, we also provide fits that include the effects of massive neutrinos and an extension to the concordance cosmological model assuming variations of the fine structure constant. Finally, we note that our expressions can be used to ease the computational cost required to compute $r_\\mathrm{s}(z_\\mathrm{d})$ with a Boltzmann code when deriving cosmological constraints using BAO data from current and upcoming surveys.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<yes>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Breaking Free with AI: The Deconfinement Transition. Employing supervised machine learning techniques, we investigate the deconfinement phase transition within $4$-dimensional $SU(2)$ Yang-Mills (YM) theory, compactified on a small circle and endowed with center-stabilizing potential. This exploration encompasses scenarios both without and with matter in either the fundamental or adjoint representations. Central to our study is a profound duality relationship, intricately mapping the YM theory onto an XY-spin model with $\\mathbb Z_p$-preserving perturbations. The parameter $p$ embodies the essence of the matter representation, with values of $p=1$ and $p=4$ for fundamental and adjoint representations, respectively, while $p=2$ corresponds to pure YM theory. The logistic regression method struggles to produce satisfactory results, particularly in predicting the transition temperature. Contrarily, convolutional neural networks (CNNs) exhibit remarkable prowess, effectively foreseeing critical temperatures in cases where $p=2$ and $p=4$. Furthermore, by harnessing CNNs, we compute critical exponents at the transition, aligning favorably with computations grounded in conventional order parameters. Taking our investigation a step further, we use CNNs to lend meaning to phases within YM theory with fundamental matter. Notably, this theory lacks conventional order parameters. Interestingly, CNNs manage to predict a transition temperature in this context. However, the fragility of this prediction under variations in the boundaries of the training window undermines its utility as a robust order parameter. This outcome underscores the constraints inherent in employing supervised machine learning techniques as innovative substitutes for traditional order parameters.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<yes>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<yes>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Classification without labels: Learning from mixed samples in high energy physics. Modern machine learning techniques can be used to construct powerful models for difficult collider physics problems. In many applications, however, these models are trained on imperfect simulations due to a lack of truth-level information in the data, which risks the model learning artifacts of the simulation. In this paper, we introduce the paradigm of classification without labels (CWoLa) in which a classifier is trained to distinguish statistical mixtures of classes, which are common in collider physics. Crucially, neither individual labels nor class proportions are required, yet we prove that the optimal classifier in the CWoLa paradigm is also the optimal classifier in the traditional fully-supervised case where all label information is available. After demonstrating the power of this method in an analytical toy example, we consider a realistic benchmark for collider physics: distinguishing quark- versus gluon-initiated jets using mixed quark/gluon training samples. More generally, CWoLa can be applied to any classification problem where labels or class proportions are unknown or simulations are unreliable, but statistical mixtures of the classes are available.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<yes>{Hyperparameters}<no>{Weak/Semi supervision}<yes>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Jet Reconstruction with Mamba Networks in Collider Events. We introduce a novel end-to-end framework for jet reconstruction in high-energy collider events, leveraging the efficiency and long-range modeling capabilities of the Mamba architecture. Our model unifies instance segmentation, classification, and kinematic regression into a single multi-task learning system, enabling a sophisticated multi-level reconstruction that simultaneously identifies primary heavy jets ($t$, $H$, $W/Z$) and their constituent sub-jets. To facilitate supervised learning for this complex task, we develop a novel method for assigning final-state hadrons to their ancestor colored partons using a Mixed-Integer Linear Programming solver, which generates high-fidelity ground-truth labels. The model achieves high classification accuracy, with an Average Precision score of 0.569 for $W/Z$-jets and 0.568 for $b$-jets, and shows exceptional precision in kinematic reconstruction. Furthermore, we show that the model not only maintains stable performance in high-pileup environments but also successfully reconstructs the mass peaks of beyond the standard model particles. This work presents a powerful and versatile new tool for comprehensive event reconstruction at the LHC.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<yes>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<yes>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<yes>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Branches of a Tree: Taking Derivatives of Programs with Discrete and Branching Randomness in High Energy Physics. We propose to apply several gradient estimation techniques to enable the differentiation of programs with discrete randomness in High Energy Physics. Such programs are common in High Energy Physics due to the presence of branching processes and clustering-based analysis. Thus differentiating such programs can open the way for gradient based optimization in the context of detector design optimization, simulator tuning, or data analysis and reconstruction optimization. We discuss several possible gradient estimation strategies, including the recent Stochastic AD method, and compare them in simplified detector design experiments. In doing so we develop, to the best of our knowledge, the first fully differentiable branching program.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<yes>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<yes>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "JEDI-net: a jet identification algorithm based on interaction networks. We investigate the performance of a jet identification algorithm based on interaction networks (JEDI-net) to identify all-hadronic decays of high-momentum heavy particles produced at the LHC and distinguish them from ordinary jets originating from the hadronization of quarks and gluons. The jet dynamics are described as a set of one-to-one interactions between the jet constituents. Based on a representation learned from these interactions, the jet is associated to one of the considered categories. Unlike other architectures, the JEDI-net models achieve their performance without special handling of the sparse input jet representation, extensive pre-processing, particle ordering, or specific assumptions regarding the underlying detector geometry. The presented models give better results with less model parameters, offering interesting prospects for LHC applications.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<yes>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<yes>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<yes>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "A feasibility study of multi-electrode high-purity germanium detector for $^{76}$Ge neutrinoless double beta decay searching. Experiments to search for neutrinoless double-beta (0{\\nu}\\b{eta}\\b{eta}) decay of 76Ge using a high-purity germanium (HPGe) detector rely heavily on background suppression technologies to enhance their sensitivities. In this work, we proposed a pulse-shape analysis method based on a neural network (NN) and a light gradient boosting machine (lightGBM; LGB) to discriminate single-electron (background) and double-electrons (0{\\nu}\\b{eta}\\b{eta} signal) events in a multi-electrode HPGe detector. In this paper, we describe a multi-electrode HPGe detector system, a data-processing system, and pulse-shape simulation procedures. We built a fully connected (FC) neural network and an LGB model to classify the single- and double-electron events. The FC network is trained with simulated single- and double-electron-induced pulses and tested in an independent dataset generated by the pulse-shape simulation. The discrimination efficiency of the FC neural network in the test set for the 0{\\nu}\\b{eta}\\b{eta} double-electron events signal was 77.4%, the precision was 57.7%, and the training time was 430 min. The discrimination efficiency of LGB model was 73.1%, the precision was 64.0%, and the training time was 1.5 min. This study demonstrated that it is feasible to realize single- and double-electron discrimination on multi-electrode HPGe detectors using an FC neural network and LGB model. These results can be used as a reference for future 76Ge 0{\\nu}\\b{eta}\\b{eta} experiments.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<yes>{Performance studies}<yes>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Maximum performance of strange-jet tagging at hadron colliders. The maximum achievable performance of strange-jet tagging at hadron colliders and the loss in performance in different detector designs is estimated based on simulated truth jets from strange-quark and down-quark hadronisation. Both jet types are classified with a recurrent neural network using long short-term memory units, at first using all available truth particles and then applying selections to study the impacts of ideal tracking detectors, Cherenkov detectors, and calorimeters. Additionally, a manual reconstruction of strange hadron decays such as $K_S\\rightarrow \\pi^+ \\pi^-$ from charged tracks is considered.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<yes>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Systematic Interpretability and the Likelihood for Boosted Top Quark Identification. Identification of boosted, hadronically-decaying top quarks is a problem of central importance for physics goals of the Large Hadron Collider. We present a theoretical analysis of top quark tagging, establishing zeroth-order, minimal assumptions that should be satisfied by any purported top-tagged jet, like existence of three hard subjets, a bottom-tagged subjet, total mass consistent with the top quark, and a pairwise subjet mass consistent with the W boson. From these minimal assumptions, we construct the optimal discrimination observable, the likelihood ratio, for the binary discrimination problem of top quark-initiated versus bottom quark-initiated jets through next-to-leading order in the strong coupling. We compare and compute corresponding signal and background efficiencies both analytically and from simulated data, validating an understanding of the relevant physics identified and exploited by the likelihood. In the process, we construct a method for systematic interpretability of the likelihood ratio for this problem, and explicitly establish a hard floor on possible discrimination power. These results can correspondingly be applied to understanding and interpreting machine learning studies of this problem.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<yes>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Shower Identification in Calorimeter using Deep Learning. Pions constitute nearly $70\\%$ of final state particles in ultra high energy collisions. They act as a probe to understand the statistical properties of Quantum Chromodynamics (QCD) matter i.e. Quark Gluon Plasma (QGP) created in such relativistic heavy ion collisions (HIC). Apart from this, direct photons are the most versatile tools to study relativistic HIC. They are produced, by various mechanisms, during the entire space-time history of the strongly interacting system. Direct photons provide measure of jet-quenching when compared with other quark or gluon jets. The $\\pi^{0}$ decay into two photons make the identification of non-correlated gamma coming from another process cumbersome in the Electromagnetic Calorimeter. We investigate the use of deep learning architecture for reconstruction and identification of single as well as multi particles showers produced in calorimeter by particles created in high energy collisions. We utilize the data of electromagnetic shower at calorimeter cell-level to train the network and show improvements for identification and characterization. These networks are fast and computationally inexpensive for particle shower identification and reconstruction for current and future experiments at particle colliders.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<yes>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Equivariance and generalization in neural networks. The crucial role played by the underlying symmetries of high energy physics and lattice field theories calls for the implementation of such symmetries in the neural network architectures that are applied to the physical system under consideration. In these proceedings, we focus on the consequences of incorporating translational equivariance among the network properties, particularly in terms of performance and generalization. The benefits of equivariant networks are exemplified by studying a complex scalar field theory, on which various regression and classification tasks are examined. For a meaningful comparison, promising equivariant and non-equivariant architectures are identified by means of a systematic search. The results indicate that in most of the tasks our best equivariant architectures can perform and generalize significantly better than their non-equivariant counterparts, which applies not only to physical parameters beyond those represented in the training set, but also to different lattice sizes.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<yes>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<yes>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<yes>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Transport away your problems: Calibrating stochastic simulations with optimal transport. Stochastic simulators are an indispensable tool in many branches of science. Often based on first principles, they deliver a series of samples whose distribution implicitly defines a probability measure to describe the phenomena of interest. However, the fidelity of these simulators is not always sufficient for all scientific purposes, necessitating the construction of ad-hoc corrections to \"calibrate\" the simulation and ensure that its output is a faithful representation of reality. In this paper, we leverage methods from transportation theory to construct such corrections in a systematic way. We use a neural network to compute minimal modifications to the individual samples produced by the simulator such that the resulting distribution becomes properly calibrated. We illustrate the method and its benefits in the context of experimental particle physics, where the need for calibrated stochastic simulators is particularly pronounced.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<yes>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<yes>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<yes>{Pileup}<no>{Calibration}<yes>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "A Neural Network Approach for Orienting Heavy-Ion Collision Events. A convolutional neural network-based classifier is elaborated to retrace the initial orientation of deformed nucleus-nucleus collisions by integrating multiple typical experimental observables. The isospin-dependent Boltzmann-Uehling-Uhlenbeck transport model is employed to generate data for random orientations of ultra-central uranium-uranium collisions at $E_\\text{beam} = 1\\, \\text{GeV/nucleon}$. Statistically, the data-driven polarization scheme is essentially accomplished via the classifier, whose distinct categories filter out specific orientation-biased collision events. This will advance the deformed nucleus-based studies on nuclear symmetry energy, neutron skin, etc.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<yes>{Jet images}<no>{Event images}<yes>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "CaloFlow for CaloChallenge Dataset 1. CaloFlow is a new and promising approach to fast calorimeter simulation based on normalizing flows. Applying CaloFlow to the photon and charged pion Geant4 showers of Dataset 1 of the Fast Calorimeter Simulation Challenge 2022, we show how it can produce high-fidelity samples with a sampling time that is several orders of magnitude faster than Geant4. We demonstrate the fidelity of the samples using calorimeter shower images, histograms of high-level features, and aggregate metrics such as a classifier trained to distinguish CaloFlow from Geant4 samples.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<yes>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "An Extended Closure Relation by LightGBM for Neutrino Radiation Transport in Core-collapse Supernovae. We developed a machine learning model using LightGBM, one of the most popular gradient-boosting decision tree methods these days, to predict the Eddington tensor, or the second-order angular moment, for neutrino radiation transport in core-collapse supernova simulations. We use not only the zeroth and first moments of the neutrino distribution function in momentum space as in ordinary closure relations but also information on the background matter configuration extensively. For training the model, we utilize some post-bounce snapshots from one of our previous Boltzmann radiation-hydrodynamics simulations; the Eddington tensor as well as the zeroth and first angular moments are calculated from the neutrino distribution function obtained in the simulation. LightGBM is light indeed, and its high efficiency in training enables us to feed a large number of features and figure out which features are more important than others. In this paper, we report the results of this feature engineering in addition to those of the training, validation, and generalization of our model. We find that the flux factor and non-local features are among the most relevant features; our LightGBM model can reproduce the Eddington factor better in general than the M1 closure relation, one of the most commonly employed algebraic closure relations at present; the generalization performance is also much improved from our previous model based on the deep neural network.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<yes>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Symmetry-preserving neural networks in lattice field theories. This thesis deals with neural networks that respect symmetries and presents the advantages in applying them to lattice field theory problems. The concept of equivariance is explained, together with the reason why such a property is crucial for the network to preserve the desired symmetry. The benefits of choosing equivariant networks are first illustrated for translational symmetry on a complex scalar field toy model. The discussion is then extended to gauge theories, for which Lattice Gauge Equivariant Convolutional Neural Networks (L-CNNs) are specifically designed ad hoc. Regressions of physical observables such as Wilson loops are successfully solved by L-CNNs, whereas traditional architectures which are not gauge symmetric perform significantly worse. Finally, we introduce the technique of neural gradient flow, which is an ordinary differential equation solved by neural networks, and propose it as a method to generate lattice gauge configurations.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<yes>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<yes>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Refereeing the referees: evaluating two-sample tests for validating generators in precision sciences. We propose a robust methodology to evaluate the performance and computational efficiency of non-parametric two-sample tests, specifically designed for high-dimensional generative models in scientific applications such as in particle physics. The study focuses on tests built from univariate integral probability measures: the sliced Wasserstein distance and the mean of the Kolmogorov-Smirnov statistics, already discussed in the literature, and the novel sliced Kolmogorov-Smirnov statistic. These metrics can be evaluated in parallel, allowing for fast and reliable estimates of their distribution under the null hypothesis. We also compare these metrics with the recently proposed unbiased Fr\\'echet Gaussian Distance and the unbiased quadratic Maximum Mean Discrepancy, computed with a quartic polynomial kernel. We evaluate the proposed tests on various distributions, focusing on their sensitivity to deformations parameterized by a single parameter $\\epsilon$. Our experiments include correlated Gaussians and mixtures of Gaussians in 5, 20, and 100 dimensions, and a particle physics dataset of gluon jets from the JetNet dataset, considering both jet- and particle-level features. Our results demonstrate that one-dimensional-based tests provide a level of sensitivity comparable to other multivariate metrics, but with significantly lower computational cost, making them ideal for evaluating generative models in high-dimensional settings. This methodology offers an efficient, standardized tool for model comparison and can serve as a benchmark for more advanced tests, including machine-learning-based approaches.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<yes>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Convolutional Neural Networks for Direct Detection of Dark Matter. The XENON1T experiment uses a time projection chamber (TPC) with liquid Xenon to search for Weakly Interacting Massive Particles (WIMPs), a proposed Dark Matter particle, via direct detection. As this experiment relies on capturing rare events, the focus is on achieving a high recall of WIMP events. Hence the ability to distinguish between WIMP and the background is extremely important. To accomplish this, we suggest using Convolutional Neural Networks (CNNs); a Machine Learning procedure mainly used in image recognition tasks. To explore this technique we use XENON collaboration open-source software to simulate the TPC graphical output of Dark Matter signals and main backgrounds. A CNN turns out to be a suitable tool for this purpose, as it can identify features in the images that differentiate the two types of events without the need to manipulate or remove data in order to focus on a particular region of the detector. We find that the CNN can distinguish between the dominant background events (ER) and 500 GeV WIMP events with a recall of 93.4\\%, precision of 81.2\\% and an accuracy of 87.2\\%.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<yes>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Extracting Transport Properties of Quark-Gluon Plasma from the Heavy-Quark Potential With Neural Networks in a Holographic Model. Using Kolmogorov-Arnold Networks (KANs), we construct a holographic model informed by lattice QCD data. This neural network approach enables the derivation of an analytical solution for the deformation factor $w(r)$ and the determination of a constant $g$ related to the string tension. Within the KANs-based holographic framework, we further analyze heavy quark potentials under finite temperature and chemical potential conditions. Additionally, we calculate the drag force, jet quenching parameter, and diffusion coefficient of heavy quarks in this paper. Our findings demonstrate qualitative consistency with both experimental measurements and established phenomenological model.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<yes>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<yes>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Stressed GANs snag desserts, a.k.a Spotting Symmetry Violation with Symmetric Functions. Testing whether data breaks symmetries of interest can be important to many fields. This paper describes a simple way that machine learning algorithms (whose outputs have been appropriately symmetrised) can be used to detect symmetry breaking. The original motivation for the paper was an important question in Particle Physics: \"Is parity violated at the LHC in some way that no-one has anticipated?\" and so we illustrate the main idea with an example strongly related to that question. However, in order that the key ideas be accessible to readers who are not particle physicists but who are interesting in symmetry breaking, we choose to illustrate the method/approach with a 'toy' example which places a simple discrete source of symmetry breaking (the handedness of human handwriting) within a idealised particle-physics-like context. Readers interested in seeing extensions to continuous symmetries, non-ideal environments or more realistic particle-physics contexts are provided with links to separate papers which delve into such details.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<yes>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "RICH 2018. We propose a way to simulate Cherenkov detector response using a generative adversarial neural network to bypass low-level details. This network is trained to reproduce high level features of the simulated detector events based on input observables of incident particles. This allows the dramatic increase of simulation speed. We demonstrate that this approach provides simulation precision which is consistent with the baseline and discuss possible implications of these results.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<yes>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Sibyll. In the last decade, an increasing number of datasets have revealed a consistent discrepancy between the number of muons measured in ultra-high-energy extensive air showers (EAS) and the numbers predicted by simulations. This gap persists despite incorporating Large Hadron Collider (LHC) data into the tuning of current hadronic interaction models, leading to the phenomenon often termed the ''muon puzzle''. To gain a deeper understanding of the potential origins of this muon puzzle, we have developed Sibyll$^{\\bigstar}$, a series of phenomenologically modified versions of Sibyll 2.3d. In these models, we have increased muon production by altering $\\rho^0$, baryon-antibaryon pair, or kaon production in hadronic multiparticle production processes. These variants remain within bounds from provided by accelerator measurements, including those from the LHC and fixed-target experiments, notably NA49 and NA61, showing a level of consistency comparable to Sibyll 2.3d. Our findings show that these modifications can increase the muon count in EAS by up to 35%, while minimally affecting the depth of shower maximum ($X_{\\rm max}$) and other shower variables. Additionally, we assess the impact of these modifications on various observables, including inclusive muon and neutrino fluxes and the multiplicities of muon bundles in deep underground and water/ice Cherenkov detectors. We aim for at least one of these model variants to offer a more accurate representation of EAS data at the highest energies, thereby enhancing the quality of Monte Carlo predictions used in training neural networks. This improvement is crucial for achieving more reliable data analyses and interpretations.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<yes>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "NNPDF4.0 aN$^3$LO PDFs with QED corrections. We review recent progress in the global determination of the collinear parton distributions (PDFs) of the proton within the NNPDF framework. This progress includes NNPDF4.0 variants with QED effects and with missing higher order uncertainties (MHOUs) via the theory covariance matrix formalism, as well as PDFs based on QCD calculations at approximate N$^3$LO (aN$^3$LO) accuracy. We present the combination of these theoretical developments resulting into NNPDF4.0 aN$^3$LO variants accounting for QED corrections and with a photon PDF. We compare these aN$^3$LO QED PDFs with analogous results obtained by the MSHT group, and briefly quantify their implications at the level of representative LHC cross-sections.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<yes>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<yes>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Universal anomaly detection at the LHC: transforming optimal classifiers and the DDD method. In this work, we present a novel approach to transform supervised classifiers into effective unsupervised anomaly detectors. The method we have developed, termed Discriminatory Detection of Distortions (DDD), enhances anomaly detection by training a discriminator model on both original and artificially modified datasets. We conducted a comprehensive evaluation of our models on the Dark Machines Anomaly Score Challenge channels and a search for 4-top quark events, demonstrating the effectiveness of our approach across various final states and beyond the Standard Model scenarios. We compare the performance of the DDD method with the Deep Robust One-Class Classification method (DROCC), which incorporates signals in the training process, and the Deep Support Vector Data Description (DeepSVDD) method, a well-established and well-performing method for anomaly detection. Results show that the effectiveness of each model varies by signal and channel, with DDD proving to be a very effective anomaly detector. We recommend the combined use of DeepSVDD and DDD for purely unsupervised applications, with the addition of flow models for improved performance when resources allow. Findings suggest that network architectures that excel in supervised contexts, such as the particle transformer with standard model interactions, also perform well as unsupervised anomaly detectors. We also show that with these methods, it is likely possible to recognize 4-top quark production as an anomaly without prior knowledge of the process. We argue that the Large Hadron Collider community can transform supervised classifiers into anomaly detectors to uncover potential new physical phenomena in each search.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<yes>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Deep learning techniques for Imaging Air Cherenkov Telescopes. Very High Energy (VHE) gamma rays and charged cosmic rays (CCRs) provide an observational window into the acceleration mechanisms of extreme astrophysical environments. One of the major challenges at Imaging Air Cherenkov Telescopes (IACTs) designed to look for VHE gamma rays, is the separation of air showers initiated by CCRs which form a background to gamma ray searches. Two other less well studied problems at IACTs are a) the classification of different primary nuclei among the CCR events and b) identification of anomalous events initiated by Beyond Standard Model particles that could give rise to shower signatures which differ from the standard images of either gamma rays or CCR showers. The problems of categorizing the primary particle that initiates a shower image, or the problem of tagging anomalous shower events in a model independent way, are problems that are well suited to a machine learning (ML) approach. Traditional studies that have explored gamma ray/CCR separation have used a multivariate analysis based on derived shower properties, which contains significantly reduced information about the shower. In our work, we address the problems outlined above by using ML architectures trained on full simulated shower images, as opposed to training on just a few derived shower properties. We illustrate the techniques of binary and multi-category classification using convolutional neural networks, and we also pioneer the use of autoencoders for anomaly detection at VHE gamma ray experiments. As a case study, we apply our techniques to the H.E.S.S. experiment. However, the real strength of the techniques that we broach here in the context of VHE gamma ray observatories, is that these methods can be applied broadly to any other IACT, such as the upcoming Cherenkov Telescope Array (CTA), or can even be suitably adapted to CCR experiments.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<yes>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Towards a foundation model for heavy-ion collision experiments through point cloud diffusion. A novel point cloud diffusion model for relativistic heavy-ion collisions, capable of ultra-fast generation of event-by-event collision output, is introduced. When trained on UrQMD cascade simulations, the model generates realistic collision event output containing 26 distinct hadron species, as a list of particle momentum vectors along with their particle ID. From solving inverse problems to accelerating model calculations or detector simulations, the model can be a promising general purpose tool for heavy-ion collisions beneficial to both theoretical studies and experimental applications.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<yes>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Explainable AI classification for parton density theory. Quantitatively connecting properties of parton distribution functions (PDFs, or parton densities) to the theoretical assumptions made within the QCD analyses which produce them has been a longstanding problem in HEP phenomenology. To confront this challenge, we introduce an ML-based explainability framework, $\\texttt{XAI4PDF}$, to classify PDFs by parton flavor or underlying theoretical model using ResNet-like neural networks (NNs). By leveraging the differentiable nature of ResNet models, this approach deploys guided backpropagation to dissect relevant features of fitted PDFs, identifying x-dependent signatures of PDFs important to the ML model classifications. By applying our framework, we are able to sort PDFs according to the analysis which produced them while constructing quantitative, human-readable maps locating the x regions most affected by the internal theory assumptions going into each analysis. This technique expands the toolkit available to PDF analysis and adjacent particle phenomenology while pointing to promising generalizations.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<yes>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<yes>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<yes>{Interpretability}<yes>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Full Detector Simulation of a Projective Dual-Readout Segmented Crystal Electromagnetic Calorimeter with Precision Timing. A differentiable full detector simulation has been implemented in the key4hep software stack for future colliders. A fully automated and configurable geometry enabling differentiation of all detector dimensions, including crystal widths and thicknesses, is presented. The software architecture, development environment, and necessary components to implement a new detector concept from scratch are described. General AI/ML reconstruction strategies for future collider detectors are discussed, based around the idea of picking the right neural network for each detector.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<yes>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<yes>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Exploring Exotic Decays of the Higgs Boson to Multi-Photons at the LHC via Multimodal Learning Approaches. The Standard Model (SM) Higgs boson, the most recently discovered elementary particle, may still serve as a mediator between the SM sector and a new physics sector related to dark matter (DM). The Large Hadron Collider (LHC) has not yet fully constrained the physics associated with the Higgs boson, leaving room for such possibilities. Among the various potential mass scales of the dark sector, the sub-GeV mass range is particularly intriguing. This parameter space presents significant challenges for DM direct detection experiments that rely on nuclear recoils. Various innovative experimental methods are currently under investigation to explore this sub-GeV dark sector. The LHC, functioning as a Higgs factory, could explore this sector once the challenge of identifying DM signals is resolved. Due to the significantly lower mass of particles in the dark sector compared to the Higgs boson, these particles are expected to be highly boosted following the Higgs boson's decay. However, detecting and identifying these highly boosted particles remains a considerable challenge at the LHC, despite their eventual decay into SM particles. We employ a well-motivated leptophobic $Z^{\\prime}_B$ model as a prototype to analyze the distinctive signatures from Higgs boson exotic decays into multi-photons. These signatures consist of collimated photons that fail to meet the photon isolation criteria, forming jet-like objects. Conventional analyses relying solely on the purity of energy deposits in the electromagnetic calorimeter would fail to detect these signatures, as they would be overwhelmed by background events from Quantum Chromodynamics. To effectively distinguish between such novel signal signatures and SM background events, we leverage advanced machine learning techniques, specifically the transformer encoder in a multimodal network structure.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<yes>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<yes>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Model-Independent Detection of New Physics Signals Using Interpretable Semi-Supervised Classifier Tests. A central goal in experimental high energy physics is to detect new physics signals that are not explained by known physics. In this paper, we aim to search for new signals that appear as deviations from known Standard Model physics in high-dimensional particle physics data. To do this, we determine whether there is any statistically significant difference between the distribution of Standard Model background samples and the distribution of the experimental observations, which are a mixture of the background and a potential new signal. Traditionally, one also assumes access to a sample from a model for the hypothesized signal distribution. Here we instead investigate a model-independent method that does not make any assumptions about the signal and uses a semi-supervised classifier to detect the presence of the signal in the experimental data. We construct three test statistics using the classifier: an estimated likelihood ratio test (LRT) statistic, a test based on the area under the ROC curve (AUC), and a test based on the misclassification error (MCE). Additionally, we propose a method for estimating the signal strength parameter and explore active subspace methods to interpret the proposed semi-supervised classifier in order to understand the properties of the detected signal. We also propose a Score test statistic that can be used in the model-dependent setting. We investigate the performance of the methods on a simulated data set related to the search for the Higgs boson at the Large Hadron Collider at CERN. We demonstrate that the semi-supervised tests have power competitive with the classical supervised methods for a well-specified signal, but much higher power for an unexpected signal which might be entirely missed by the supervised tests.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<yes>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Higgs analysis with quantum classifiers. We have developed two quantum classifier models for the $t\\bar{t}H(b\\bar{b})$ classification problem, both of which fall into the category of hybrid quantum-classical algorithms for Noisy Intermediate Scale Quantum devices (NISQ). Our results, along with other studies, serve as a proof of concept that Quantum Machine Learning (QML) methods can have similar or better performance, in specific cases of low number of training samples, with respect to conventional ML methods even with a limited number of qubits available in current hardware. To utilise algorithms with a low number of qubits -- to accommodate for limitations in both simulation hardware and real quantum hardware -- we investigated different feature reduction methods. Their impact on the performance of both the classical and quantum models was assessed. We addressed different implementations of two QML models, representative of the two main approaches to supervised quantum machine learning today: a Quantum Support Vector Machine (QSVM), a kernel-based method, and a Variational Quantum Circuit (VQC), a variational approach.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<yes>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<yes>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Towards a realistic track reconstruction algorithm based on graph neural networks for the HL-LHC. The physics reach of the HL-LHC will be limited by how efficiently the experiments can use the available computing resources, i.e. affordable software and computing are essential. The development of novel methods for charged particle reconstruction at the HL-LHC incorporating machine learning techniques or based entirely on machine learning is a vibrant area of research. In the past two years, algorithms for track pattern recognition based on graph neural networks (GNNs) have emerged as a particularly promising approach. Previous work mainly aimed at establishing proof of principle. In the present document we describe new algorithms that can handle complex realistic detectors. The new algorithms are implemented in ACTS, a common framework for tracking software. This work aims at implementing a realistic GNN-based algorithm that can be deployed in an HL-LHC experiment.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<yes>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<yes>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<yes>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Jet Single Shot Detection. We apply object detection techniques based on Convolutional Neural Networks to jet reconstruction and identification at the CERN Large Hadron Collider. In particular, we focus on CaloJet reconstruction, representing each event as an image composed of calorimeter cells and using a Single Shot Detection network, called Jet-SSD. The model performs simultaneous localization and classification and additional regression tasks to measure jet features. We investigate Ternary Weight Networks with weights constrained to {-1, 0, 1} times a layer- and channel-dependent scaling factors. We show that the quantized version of the network closely matches the performance of its full-precision equivalent.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<yes>{Jet images}<no>{Event images}<yes>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<yes>{Software}<yes>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Ranking-based neural network for ambiguity resolution in ACTS. The reconstruction of particle trajectories is a key challenge of particle physics experiments, as it directly impacts particle identification and physics performances while also representing one of the main CPU consumers of many high-energy physics experiments. As the luminosity of particle colliders increases, this reconstruction will become more challenging and resource-intensive. New algorithms are thus needed to address these challenges efficiently. One potential step of track reconstruction is ambiguity resolution. In this step, performed at the end of the tracking chain, we select which tracks candidates should be kept and which must be discarded. The speed of this algorithm is directly driven by the number of track candidates, which can be reduced at the cost of some physics performance. Since this problem is fundamentally an issue of comparison and classification, we propose to use a machine learning-based approach to the Ambiguity Resolution. Using a shared-hits-based clustering algorithm, we can efficiently determine which candidates belong to the same truth particle. Afterwards, we can apply a Neural Network (NN) to compare those tracks and decide which ones are duplicates and which ones should be kept. This approach is implemented within A Common Tracking Software (ACTS) framework and tested on the Open Data Detector (ODD), a realistic virtual detector similar to a future ATLAS one. This new approach was shown to be 15 times faster than the default ACTS algorithm while removing 32 times more duplicates down to less than one duplicated track per event.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<yes>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Machine Learning for the LHCb Simulation. Most of the computing resources pledged to the LHCb experiment at CERN are necessary to the production of simulated samples used to predict resolution functions on the reconstructed quantities and the reconstruction and selection efficiency. Projecting the Simulation requests to the years following the upcoming LHCb Upgrade, the relative computing resources would exceed the pledges by more than a factor of 2. In this contribution, I discuss how Machine Learning can help to speed up the Detector Simulation for the upcoming Runs of the LHCb experiment.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<yes>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Exploring the Universality of Hadronic Jet Classification. The modeling of jet substructure significantly differs between Parton Shower Monte Carlo (PSMC) programs. Despite this, we observe that machine learning classifiers trained on different PSMCs learn nearly the same function. This means that when these classifiers are applied to the same PSMC for testing, they result in nearly the same performance. This classifier universality indicates that a machine learning model trained on one simulation and tested on another simulation (or data) will likely be optimal. Our observations are based on detailed studies of shallow and deep neural networks applied to simulated Lorentz boosted Higgs jet tagging at the LHC.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<yes>{Interpretability}<no>{Estimation}<yes>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "CaloGraph: Graph-based diffusion model for fast shower generation in calorimeters with irregular geometry. Denoising diffusion models have gained prominence in various generative tasks, prompting their exploration for the generation of calorimeter responses. Given the computational challenges posed by detector simulations in high-energy physics experiments, the necessity to explore new machine-learning-based approaches is evident. This study introduces a novel graph-based diffusion model designed specifically for rapid calorimeter simulations. The methodology is particularly well-suited for low-granularity detectors featuring irregular geometries. We apply this model to the ATLAS dataset published in the context of the Fast Calorimeter Simulation Challenge 2022, marking the first application of a graph diffusion model in the field of particle physics.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<yes>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "The SMARTHEP European Training Network. Synergies between MAchine learning, Real-Time analysis and Hybrid architectures for efficient Event Processing and decision-making (SMARTHEP) is a European Training Network, training a new generation of Early Stage Researchers (ESRs) to advance real-time decision-making, driving data-collection and analysis towards synonymity. SMARTHEP brings together scientists from major LHC collaborations at the frontiers of real-time analysis (RTA) and key specialists from computer science and industry. By solving concrete problems as a community, SMARTHEP will further the adoption of RTA techniques, enabling future High Energy Physics (HEP) discoveries and generating impact in industry. ESRs will contribute to European growth, leveraging their hands-on experience in machine learning and accelerators towards commercial deliverables in fields that can profit most from RTA, e.g., transport, manufacturing, and finance. This contribution presents the training and outreach plan for the network, and is intended as an opportunity for further collaboration and feedback from the CHEP community.",
    "output": "{Reviews}<yes>{Modern reviews}<no>{Specialized reviews}<yes>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "The DL Advocate: Playing the devil's advocate with hidden systematic uncertainties. We propose a new method based on machine learning to \\emph{play the devil's advocate} and investigate the impact of unknown systematic effects in a quantitative way. This method proceeds by reversing the measurement process and using the physics results to interpret systematic effects under the Standard Model hypothesis. We explore this idea with two alternative approaches: the first one relies on a combination of gradient descent and optimisation techniques, its application and potentiality is illustrated with an example that studies the branching fraction measurement of a heavy-flavour decay. The second method employs reinforcement learning and it is applied to the determination of the $P_{5}^{'}$ angular observable in $B^0 \\to K^{*0} \\mu^+\\mu^-$ decays. We find that for the former, the size of a hypothetical hidden systematic uncertainty strongly depends on the kinematic overlap between the signal and normalisation channel, while the latter is very robust against possible mismodellings of the efficiency.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<yes>{Interpretability}<no>{Estimation}<yes>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "cDVGAN: One Flexible Model for Multi-class Gravitational Wave Signal and Glitch Generation. Simulating realistic time-domain observations of gravitational waves (GWs) and GW detector glitches can help in advancing GW data analysis. Simulated data can be used in downstream tasks by augmenting datasets for signal searches, balancing data sets for machine learning, and validating detection schemes. In this work, we present Conditional Derivative GAN (cDVGAN), a novel conditional model in the Generative Adversarial Network framework for simulating multiple classes of time-domain observations that represent gravitational waves (GWs) and detector glitches. cDVGAN can also generate generalized hybrid samples that span the variation between classes through interpolation in the conditioned class vector. cDVGAN introduces an additional player into the typical 2-player adversarial game of GANs, where an auxiliary discriminator analyzes the first-order derivative time-series. Our results show that this provides synthetic data that better captures the features of the original data. cDVGAN conditions on three classes, two denoised from LIGO blip and tomte glitch events from its 3rd observing run (O3), and the third representing binary black hole (BBH) mergers. Our proposed cDVGAN outperforms 4 different baseline GAN models in replicating the features of the three classes. Specifically, our experiments show that training convolutional neural networks (CNNs) with our cDVGAN-generated data improves the detection of samples embedded in detector noise beyond the synthetic data from other state-of-the-art GAN models. Our best synthetic dataset yields as much as a 4.2% increase in area-under-the-curve (AUC) performance compared to synthetic datasets from baseline GANs. Moreover, training the CNN with hybrid samples from our cDVGAN outperforms CNNs trained only on the standard classes, when identifying real samples embedded in LIGO detector background (4% AUC improvement for cDVGAN).",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<yes>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Dealing with Nuisance Parameters using Machine Learning in High Energy Physics: a Review. In this work we discuss the impact of nuisance parameters on the effectiveness of machine learning in high-energy physics problems, and provide a review of techniques that allow to include their effect and reduce their impact in the search for optimal selection criteria and variable transformations. The introduction of nuisance parameters complicates the supervised learning task and its correspondence with the data analysis goal, due to their contribution degrading the model performances in real data, and the necessary addition of uncertainties in the resulting statistical inference. The approaches discussed include nuisance-parameterized models, modified or adversary losses, semi-supervised learning approaches, and inference-aware techniques.",
    "output": "{Reviews}<yes>{Modern reviews}<no>{Specialized reviews}<yes>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Estimation of Impact Parameter and Transverse Spherocity in heavy-ion collisions at the LHC energies using Machine Learning. Recently, machine learning (ML) techniques have led to a range of numerous developments in the field of nuclear and high-energy physics. In heavy-ion collisions, the impact parameter of a collision is one of the crucial observables which has a significant impact on the final state particle production. However, calculation of such a quantity is nearly impossible in experiments as the length scale ranges in the level of a few fermi. In this work, we implement the ML-based regression technique via Boosted Decision Tree (BDT) to obtain a prediction of impact parameter in Pb-Pb collisions at $\\sqrt{s_{\\rm NN}}$ = 5.02 TeV using A Multi-Phase Transport (AMPT) model. In addition, we predict an event shape observable, transverse spherocity in Pb-Pb collisions at $\\sqrt{s_{\\rm NN}}$ = 2.76 and 5.02 TeV using AMPT and PYTHIA8 based on Angantyr model. After a successful implementation in small collision systems, the use of transverse spherocity in heavy-ion collisions has potential to reveal new results from heavy-ion collisions where the production of a QGP medium is already established. We predict the centrality dependent spherocity distributions from the training of minimum bias simulated data and it was found that the predictions from BDT based ML technique match with true simulated data. In the absence of experimental measurements, we propose to implement Machine learning based regression technique to obtain transverse spherocity from the known final state observables in heavy-ion collisions.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<yes>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Accelerating HEP simulations with Neural Importance Sampling. Many high-energy-physics (HEP) simulations for the LHC rely on Monte Carlo using importance sampling by means of the VEGAS algorithm. However, complex high-precision calculations have become a challenge for the standard toolbox, as this approach suffers from poor performance in complex cases. As a result, there has been keen interest in HEP for modern machine learning to power adaptive sampling. While previous studies have shown the potential of normalizing-flow-powered neural importance sampling (NIS) over VEGAS, there remains a gap in accessible tools tailored for non-experts. In response, we introduce Z\\\"uNIS, a fully automated NIS library designed to bridge this divide, while at the same time providing the infrastructure to customise the algorithm for dealing with challenging tasks. After a general introduction on NIS, we first show how to extend the original formulation of NIS to reuse samples over multiple gradient steps while guaranteeing a stable training, yielding a significant improvement for slow functions. Next, we introduce the structure of the library, which can be used by non-experts with minimal effort and is extensivly documented, which is crucial to become a mature tool for the wider HEP public. We present systematic benchmark results on both toy and physics examples, and stress the benefit of providing different survey strategies, which allows higher performance in challenging cases. We show that Z\\\"uNIS shows high performance on a range of problems with limited fine-tuning.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<yes>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<yes>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<yes>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Deep-learning quasi-particle masses from QCD equation of state. The interactions of quarks and gluons are strong at non-perturbative region. The equation of state (EoS) of a strongly-interacting quantum chromodynamics (QCD) medium can only be studied using the first-principle lattice QCD calculations. However, the complicated QCD EoS can be reproduced using simple statistical formula by treating the medium as a free parton gas whose fundamental degree of freedoms are dressed quarks and gluons called quasi-particles, with temperature-dependent masses. We use deep neural network and auto differentiation to solve this variational problem in which the masses of quasi gluons, up/down and strange quarks are three unknown functions, whose forms are represented by deep neural network. We reproduce the QCD EoS using these machine learned quasi-particle masses, and calculate the shear viscosity over entropy density ($\\eta/s$) as a function of temperature of the hot QCD matter.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<yes>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<yes>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Accelerated Discovery of Machine-Learned Symmetries: Deriving the Exceptional Lie Groups G2, F4 and E6. Recent work has applied supervised deep learning to derive continuous symmetry transformations that preserve the data labels and to obtain the corresponding algebras of symmetry generators. This letter introduces two improved algorithms that significantly speed up the discovery of these symmetry transformations. The new methods are demonstrated by deriving the complete set of generators for the unitary groups U(n) and the exceptional Lie groups $G_2$, $F_4$, and $E_6$. A third post-processing algorithm renders the found generators in sparse form. We benchmark the performance improvement of the new algorithms relative to the standard approach. Given the significant complexity of the exceptional Lie groups, our results demonstrate that this machine-learning method for discovering symmetries is completely general and can be applied to a wide variety of labeled datasets.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<yes>{Theory and physics for ML}<no>{ML for theory}<yes>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Search for non-resonant Higgs boson pair production in the $bb\\ell\\nu\\ell\\nu$ final state with the ATLAS detector in $pp$ collisions at $\\sqrt{s}. A search for non-resonant Higgs boson pair production, as predicted by the Standard Model, is presented, where one of the Higgs bosons decays via the $H\\rightarrow bb$ channel and the other via one of the $H \\rightarrow WW^*/ZZ^*/\\tau\\tau$ channels. The analysis selection requires events to have at least two $b$-tagged jets and exactly two leptons (electrons or muons) with opposite electric charge in the final state. Candidate events consistent with Higgs boson pair production are selected using a multi-class neural network discriminant. The analysis uses 139 fb$^{-1}$ of $pp$ collision data recorded at a centre-of-mass energy of 13 TeV by the ATLAS detector at the Large Hadron Collider. An observed (expected) upper limit of 1.2 ($0.9^{+0.4}_{-0.3}$) pb is set on the non-resonant Higgs boson pair production cross-section at 95% confidence level, which is equivalent to 40 ($29^{+14}_{-9}$) times the value predicted in the Standard Model.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<yes>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<yes>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "End-to-end multi-particle reconstruction in high occupancy imaging calorimeters with graph neural networks. We present an end-to-end reconstruction algorithm to build particle candidates from detector hits in next-generation granular calorimeters similar to that foreseen for the high-luminosity upgrade of the CMS detector. The algorithm exploits a distance-weighted graph neural network, trained with object condensation, a graph segmentation technique. Through a single-shot approach, the reconstruction task is paired with energy regression. We describe the reconstruction performance in terms of efficiency as well as in terms of energy resolution. In addition, we show the jet reconstruction performance of our method and discuss its inference computational cost. To our knowledge, this work is the first-ever example of single-shot calorimetric reconstruction of ${\\cal O}(1000)$ particles in high-luminosity conditions with 200 pileup.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<yes>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<yes>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Exploring the standard model EFT in VH production with machine learning. In this paper we study the use of Machine Learning techniques to exploit kinematic information in VH, the production of a Higgs in association with a massive vector boson. We parametrise the effect of new physics in terms of the SMEFT framework. We find that the use of a shallow neural network allows us to dramatically increase the sensitivity to deviations in VH respect to previous estimates. We also discuss the relation between the usual measures of performance in Machine Learning, such as AUC or accuracy, with the more adept measure of Asimov significance. This relation is particularly relevant when parametrising systematic uncertainties. Our results show the potential of incorporating Machine Learning techniques to the SMEFT studies using the current datasets.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<yes>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Deep Learning Calabi-Yau four folds with hybrid and recurrent neural network architectures. In this work, we report the results of applying deep learning based on hybrid convolutional-recurrent and purely recurrent neural network architectures to the dataset of almost one million complete intersection Calabi-Yau four-folds (CICY4) to machine-learn their four Hodge numbers $h^{1,1}, h^{2,1}, h^{3,1}, h^{2,2}$. In particular, we explored and experimented with twelve different neural network models, nine of which are convolutional-recurrent (CNN-RNN) hybrids with the RNN unit being either GRU (Gated Recurrent Unit) or Long Short Term Memory (LSTM). The remaining four models are purely recurrent neural networks based on LSTM. In terms of the $h^{1,1}, h^{2,1}, h^{3,1}, h^{2,2}$ prediction accuracies, at 72% training ratio, our best performing individual model is CNN-LSTM-400, a hybrid CNN-LSTM with the LSTM hidden size of 400, which obtained 99.74%, 98.07%, 95.19%, 81.01%, our second best performing individual model is LSTM-448, an LSTM-based model with the hidden size of 448, which obtained 99.74%, 97.51%, 94.24%, and 78.63%. These results were improved by forming ensembles of the top two, three or even four models. Our best ensemble, consisting of the top four models, achieved the accuracies of 99.84%, 98.71%, 96.26%, 85.03%. At 80% training ratio, the top two performing models LSTM-448 and LSTM-424 are both LSTM-based with the hidden sizes of 448 and 424. Compared with the 72% training ratio, there is a significant improvement of accuracies, which reached 99.85%, 98.66%, 96.26%, 84.77% for the best individual model and 99.90%, 99.03%, 97.97%, 87.34% for the best ensemble. By nature a proof of concept, the results of this work conclusively established the utility of RNN-based architectures and demonstrated their effective performances compared to the well-explored purely CNN-based architectures in the problem of deep learning Calabi Yau manifolds.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<yes>{Theory and physics for ML}<no>{ML for theory}<yes>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Quarks and gluons in the Lund plane. Discriminating quark and gluon jets is a long-standing topic in collider phenomenology. In this paper, we address this question using the Lund jet plane substructure technique introduced in recent years. We present two complementary approaches: one where the quark/gluon likelihood ratio is computed analytically, to single-logarithmic accuracy, in perturbative QCD, and one where the Lund declusterings are used to train a neural network. For both approaches, we either consider only the primary Lund plane or the full clustering tree. The analytic and machine-learning discriminants are shown to be equivalent on a toy event sample resumming exactly leading collinear single logarithms, where the analytic calculation corresponds to the exact likelihood ratio. On a full Monte Carlo event sample, both approaches show a good discriminating power, with the machine-learning models usually being superior. We carry on a study in the asymptotic limit of large logarithm, allowing us to gain confidence that this superior performance comes from effects that are subleading in our analytic approach. We then compare our approach to other quark-gluon discriminants in the literature. Finally, we study the resilience of our quark-gluon discriminants against the details of the event sample and observe that the analytic and machine-learning approaches show similar behaviour.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<yes>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Improving sensitivity of trilinear RPV SUSY searches using machine learning at the LHC. In this work, we have explored the sensitivity of multilepton final states in probing the gaugino sector of R-parity violating supersymmetric scenario with specific lepton number violating trilinear couplings ($\\lambda_{ijk}$) being non-zero. The gaugino spectrum is such that the charged leptons in the final state can arise from the R-parity violating decays of the lightest supersymmetric particle (LSP) as well as R-parity conserving decays of the next-to-LSP (NLSP). Apart from a detailed cut-based analysis, we have also performed a machine learning-based analysis using boosted decision tree algorithm which provides much better sensitivity. In the scenarios with non-zero $\\lambda_{121}$ and/or $\\lambda_{122}$ couplings, the LSP pair in the final states decays to $4l~(l = e, \\mu) + \\rm E{\\!\\!\\!/}_T$ final states with $100\\%$ branching ratio. We have shown that under this circumstance, a final state with $\\ge 4l$ has the highest sensitivity in probing the gaugino masses. We also discuss how the sensitivity can change in the presence of $\\tau$ lepton(s) in the final state due to other choices of trilinear couplings. We present our results through the estimation of the discovery and exclusion contours in the gaugino mass plane for both the HL-LHC and the HE-LHC. For $\\lambda_{121}$ and/or $\\lambda_{122}$ nonzero scenario, the projected 2$\\sigma$ exclusion limit on NLSP masses reaches upto 2.37 TeV and 4 TeV for the HL-LHC and the HE-LHC respectively by using a machine learning based algorithm. We obtain an enhancement of $\\sim$ 380 (190) GeV in the projected 2$\\sigma$ exclusion limit on the NLSP masses at the 27 (14) TeV LHC. Considering the same final state ($N_l \\geq 4$) for $\\lambda_{133}$ and/or $\\lambda_{233}$ non-zero scenario, we find that the corresponding 2$\\sigma$ projected limits are $\\sim$ 1.97 TeV and $\\sim$ 3.25 TeV for the HL-LHC and HE-LHC respectively.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<yes>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<yes>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<yes>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<no>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<no>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  },
  {
    "instruction": "You are an expert in machine learning for high-energy physics. Your task is to read the title and abstract of a paper and perform STRICT hierarchical multi-label classification over the 91 predefined labels below. Assign <yes> ONLY when there is strong, explicit evidence in the title/abstract; never guess or infer. If uncertain, prefer <no>. A paper may belong to multiple labels simultaneously. HIERARCHICAL CONSISTENCY RULES: (1) If ANY sub-label is <yes>, ALL parents must be <yes>. (2) If a parent is <no>, ALL sub-labels must be <no>. (3) For overlapping labels, choose the MOST SPECIFIC explicitly supported ones; assign multiple only if independently evidenced. OUTPUT FORMAT: For EACH label in the EXACT order of the predefined list, output the FULL label name immediately followed by <yes> or <no> with NO spaces. No explanations, no extra text, no missing or reordered labels. Example: {Reviews}<yes>{Modern reviews}<yes>{Specialized reviews}<no>{Datasets}<no>. PREDEFINED LABEL SPACE (exact order, 91 labels): [{'Reviews': ['Modern reviews', 'Specialized reviews', 'Datasets']}, {'Classification': ['Parameterized classifiers', {'Representations': ['Jet images', 'Event images', 'Sequences', 'Trees', 'Graphs', 'Sets (point clouds)', 'Physics-inspired basis']}, {'Targets': ['W/Z tagging', 'H -> b anti-b', 'quarks and gluons', 'top quark tagging', 'strange jets', '$b$-tagging', 'Flavor physics', 'BSM particles and models', 'Particle identification', 'Neutrino Detectors', 'Direct Dark Matter Detectors', 'Cosmology, Astro Particle, and Cosmic Ray physics', 'Tracking', 'Heavy Ions / Nuclear Physics']}, {'Learning strategies': ['Hyperparameters', 'Weak/Semi supervision', 'Unsupervised', 'Reinforcement Learning', 'Quantum Machine Learning', 'Feature ranking', 'Attention', 'Regularization', 'Optimal Transport']}, {'Fast inference / deployment': ['Software', 'Hardware/firmware', 'Deployment']}]}, {'Regression': ['Pileup', 'Calibration', 'Recasting', 'Matrix elements', 'Parameter estimation FROM Regression', 'Parton Distribution Functions (and related)', 'Lattice Gauge Theory', 'Function Approximation', 'Symbolic Regression', 'Monitoring']}, {'Equivariant networks.'}, {'Physics-informed neural networks (PINNs) / Neural Operators.'}, {'Decorrelation methods.'}, {'Generative models / density estimation': ['GANs', '(Variational) Autoencoders', '(Continuous) Normalizing flows', 'Diffusion Models', 'Transformer Models', 'Physics-inspired', 'Mixture Models', 'Phase space generation', 'Gaussian processes', 'Evaluation of Generative Models', 'Other/hybrid']}, {'Anomaly detection.'}, {'Foundation Models, LLMs.'}, {'Kolmogorov-Arnold Networks (KANs).'}, {'Simulation-based (likelihood-free) Inference': ['Parameter estimation FROM Simulation-based (likelihood-free) Inference', 'Unfolding', 'Domain adaptation', 'BSM', 'Differentiable Simulation']}, {'Uncertainty Quantification': ['Interpretability', 'Estimation', 'Mitigation', 'Uncertainty- and inference-aware learning']}, {'Formal Theory and ML': ['Theory and physics for ML', 'ML for theory']}, {'Experimental results.': ['Performance studies', 'Searches and measurements where ML reconstruction is a core component', 'Final analysis discriminate for searches', 'Measurements using deep learning directly (not through object reconstruction)']}] Now classify the following paper based solely on its title and abstract:",
    "input": "Symbolic regression for precision LHC physics. We study the potential of symbolic regression (SR) to derive compact and precise analytic expressions that can improve the accuracy and simplicity of phenomenological analyses at the Large Hadron Collider (LHC). As a benchmark, we apply SR to equation recovery in quantum electrodynamics (QED), where established analytical results from quantum field theory provide a reliable framework for evaluation. This benchmark serves to validate the performance and reliability of SR before extending its application to structure functions in the Drell-Yan process mediated by virtual photons, which lack analytic representations from first principles. By combining the simplicity of analytic expressions with the predictive power of machine learning techniques, SR offers a useful tool for facilitating phenomenological analyses in high energy physics.",
    "output": "{Reviews}<no>{Modern reviews}<no>{Specialized reviews}<no>{Datasets}<no>{Classification}<no>{Parameterized classifiers}<no>{Representations}<no>{Jet images}<no>{Event images}<no>{Sequences}<no>{Trees}<no>{Graphs}<no>{Sets (point clouds)}<no>{Physics-inspired basis}<no>{Targets}<no>{W/Z tagging}<no>{H -> b anti-b}<no>{quarks and gluons}<no>{top quark tagging}<no>{strange jets}<no>{$b$-tagging}<no>{Flavor physics}<no>{BSM particles and models}<no>{Particle identification}<no>{Neutrino Detectors}<no>{Direct Dark Matter Detectors}<no>{Cosmology, Astro Particle, and Cosmic Ray physics}<no>{Tracking}<no>{Heavy Ions / Nuclear Physics}<no>{Learning strategies}<no>{Hyperparameters}<no>{Weak/Semi supervision}<no>{Unsupervised}<no>{Reinforcement Learning}<no>{Quantum Machine Learning}<no>{Feature ranking}<no>{Attention}<no>{Regularization}<no>{Optimal Transport}<no>{Fast inference / deployment}<no>{Software}<no>{Hardware/firmware}<no>{Deployment}<no>{Regression}<yes>{Pileup}<no>{Calibration}<no>{Recasting}<no>{Matrix elements}<no>{Parameter estimation FROM Regression}<no>{Parton Distribution Functions (and related)}<no>{Lattice Gauge Theory}<no>{Function Approximation}<no>{Symbolic Regression}<yes>{Monitoring}<no>{Equivariant networks.}<no>{Physics-informed neural networks (PINNs) / Neural Operators.}<no>{Decorrelation methods.}<no>{Generative models / density estimation}<no>{GANs}<no>{(Variational) Autoencoders}<no>{(Continuous) Normalizing flows}<no>{Diffusion Models}<no>{Transformer Models}<no>{Physics-inspired}<no>{Mixture Models}<no>{Phase space generation}<no>{Gaussian processes}<no>{Evaluation of Generative Models}<no>{Other/hybrid}<no>{Anomaly detection.}<no>{Foundation Models, LLMs.}<no>{Kolmogorov-Arnold Networks (KANs).}<no>{Simulation-based (likelihood-free) Inference}<no>{Parameter estimation FROM Simulation-based (likelihood-free) Inference}<no>{Unfolding}<no>{Domain adaptation}<no>{BSM}<no>{Differentiable Simulation}<no>{Uncertainty Quantification}<no>{Interpretability}<no>{Estimation}<no>{Mitigation}<no>{Uncertainty- and inference-aware learning}<no>{Formal Theory and ML}<no>{Theory and physics for ML}<no>{ML for theory}<no>{Experimental results.}<no>{Performance studies}<no>{Searches and measurements where ML reconstruction is a core component}<no>{Final analysis discriminate for searches}<no>{Measurements using deep learning directly (not through object reconstruction)}<no>"
  }
]